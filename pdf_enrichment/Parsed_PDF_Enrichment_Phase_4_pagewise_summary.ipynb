{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "start_page_idx=99\n",
    "end_page_index=480\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename\n",
    "pdf_enrichment_output_dir='./pdf_enriched_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name)\n",
    "\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs = model_kwargs)\n",
    "\n",
    "##Define Vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_kubernetes_in_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialized data)\n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_stage2_phase1.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44433479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_dict_deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dict_deserialized_stage2=document_dict_deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20db8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_dict_deserialized_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f9e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_summary_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract a small summary within 150 words that will\\n\n",
    "            be enough to represent all information for that page.\n",
    "            There is no need to mention any header statement before the summary.\n",
    "            Wrap the summary in a json with key named summary.\n",
    "            Output the json and nothing else no headers no footers.\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    json_output = output\n",
    "    json_output=json_output.split('{')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='{'+json_output\n",
    "    \n",
    "    json_output=json_output.split('}')[0]\n",
    "    \n",
    "    json_output=json_output+'}'\n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "    \n",
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_highlights_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker,\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page and extract upto maximum of 10 highlights from the page \\n\n",
    "            content.\\n\n",
    "            Extract the highlights following the mentioned rules below:\\n\n",
    "            1.For each highlight wrap it up in json with the key named highlight.\\n\n",
    "            2.After all the highlights have been extracted collate them into a list of json.\\n\n",
    "            Out should only contain the list of json and no other words or character or sentences.\\n\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    json_output=output.split('[')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='['+json_output\n",
    "    \n",
    "    \n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    print('Reversed JSON OUTPUT:')\n",
    "    print(json_output)\n",
    "    \n",
    "    json_output=json_output.split(']')[1]\n",
    "    \n",
    "    json_output=reverse(json_output)\n",
    "    #json_output=json_output.rsplit(']')[-1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output= json_output + ']'\n",
    "    \n",
    "    print('JSON OUTPUT:')\n",
    "    print(json_output)\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "def entity_collector_per_page(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity)\n",
    "        entity_name=entity['entity']\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def enrich_page(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    page_text=document_dict_deserialized_stage2[page_idx]['text']\n",
    "    \n",
    "    ##Entity Extraction Enrichment\n",
    "    page_summary_txt=extract_summary_per_page(page_text)\n",
    "    \n",
    "    #print(\"Page Summary Text\")\n",
    "    print(page_summary_txt)\n",
    "    \n",
    "    #print(\"Page Summary Json\")\n",
    "    #print(page_entity_lst_dict)\n",
    "    \n",
    "    #page_highlights_lst_dict_json={}\n",
    "    #page_highlights_lst_dict_json['summary']=page_highlights_lst_dict\n",
    "    \n",
    "    #page_highlights_lst_dict_json=json.loads(page_highlights_lst_dict)\n",
    "    document_dict_deserialized_stage2[page_idx]['summary']=json.loads(page_summary_txt.strip())[\"summary\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5465a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Headless services allow clients to discover and connect directly to pods using DNS names. They provide load balancing through DNS round-robin, but unlike regular services, they don't use a service proxy. To include unready pods in a service's DNS lookup, add the 'service.alpha.kubernetes.io/tolerate-unready-endpoints: true' annotation. When troubleshooting services, check if the pods are running and the service is properly configured, as issues can arise from incorrect labels or annotations.\"}\n",
      "Done for page number:99\n",
      "{\"summary\": \"To troubleshoot Kubernetes service-related problems, ensure you're connecting to the cluster IP from within the cluster. Verify that readiness probes are succeeding and check the Endpoints object with kubectl get endpoints. Ensure you're accessing the correct port and try connecting directly to the pod IP. If still issues, check if your app is only binding to localhost.\"}\n",
      "Done for page number:100\n",
      "{\"summary\": \"Services enable clients to discover and talk to pods, using a pod's readiness probe and enabling discovery of pod IPs through DNS for headless services. Additionally, knowledge has been gained on troubleshooting services, modifying firewall rules, executing commands in pod containers, running bash shells, and modifying Kubernetes resources.\"}\n",
      "Done for page number:101\n",
      "{\"summary\": \"Kubernetes volumes are used to attach disk storage to containers, enabling them to access external storage and share it between them. This chapter covers creating multi-container pods, shared volume creation, using Git repositories, attaching persistent storage, and dynamic provisioning.\"}\n",
      "Done for page number:102\n",
      "{\"summary\": \"Kubernetes provides storage volumes to preserve data across container restarts. Volumes are defined in a pod's specification and can be mounted in each container that needs to access it, allowing multiple containers to share disk storage.\"}\n",
      "Done for page number:103\n",
      "{\"summary\": \"Volumes are used to persist data between container restarts, and can be shared among multiple containers within a pod. A pod can have one or more volumes attached, which can be accessed by its containers. In the example shown, three containers share two volumes, publicHtml and logVol, mounted at different paths /var/htdocs/logs and /var/html respectively.\"}\n",
      "Done for page number:104\n",
      "{\"summary\": \"A Kubernetes pod can have multiple volumes that are shared among its containers. These volumes can be mounted at different locations in each container, allowing them to operate on the same files. The available volume types include emptyDir, hostPath, gitRepo, nfs, and cloud provider-specific storage like gcePersistentDisk, awsElasticBlockStore, and azureDisk.\"}\n",
      "Done for page number:105\n",
      "{\"summary\": \"Volumes can share data between containers in a pod, with types like emptyDir being useful for sharing files. An emptyDir volume is especially useful for sharing files between containers running in the same pod and can be used by a single container for temporary storage. A pod can use multiple volumes of different types at the same time, and each container can have the volume mounted or not.\"}\n",
      "Done for page number:106\n",
      "{\"summary\": \"This chapter explains how to attach disk storage to containers in Docker, specifically building and pushing a custom image for a fortune container that writes fortunes to an HTML file every 10 seconds. It also covers creating a pod manifest with two containers sharing the same volume using Kubernetes.\"}\n",
      "Done for page number:107\n",
      "{\"summary\": \"A pod contains two containers, html-generator and web-server, sharing a single volume mounted at different paths. The html-generator writes output to /var/htdocs/index.html every 10 seconds, while the web-server serves HTML files from /usr/share/nginx/html. A client requesting HTTP on port 80 receives the current fortune message as response. Access is enabled by forwarding a port or exposing through service.\"}\n",
      "Done for page number:108\n",
      "{\"summary\": \"An emptyDir volume can be created on a tmpfs filesystem for in-memory storage, and a gitRepo volume is an emptyDir that gets populated by cloning a Git repository and checking out a specific revision. The files in a gitRepo volume are not kept in sync with the referenced repository, but will be updated when a new pod is created by a ReplicationController.\"}\n",
      "Done for page number:109\n",
      "{\"summary\": \"To share data between containers, use volumes. Create a pod with a single Nginx container and a gitRepo volume pointing to your own fork of a Git repository. The volume will clone the repository and check out the master branch revision. Delete the pod every time you push changes to the Git repo to serve new versions of the website.\"}\n",
      "Done for page number:110\n",
      "{\"summary\": \"To see new website changes on Nginx web server after editing index.html file in GitHub repository, delete pod and recreate it. Alternatively, run a process to keep gitRepo volume in sync with Git repository using a sidecar container. Find an existing Docker image that does this, mount the volume, and configure the sync container. This allows files served by web server to be kept in sync with GitHub repo.\"}\n",
      "Done for page number:111\n",
      "{\"summary\": \"A hostPath volume allows pods to access files on the worker node's filesystem, making it possible for system-level pods to read node files or use devices through the filesystem. The contents of a hostPath volume persist even after a pod is deleted and can be seen by subsequent pods scheduled to the same node.\"}\n",
      "Done for page number:112\n",
      "{\"summary\": \"HostPath volumes are not recommended for regular pods as they make the pod sensitive to node scheduling. However, system-wide pods like fluentd-kubia use hostPath volumes to access node's log files and other directories. These pods do not store their own data using hostPath volumes, but rather use them to gain access to the node's data.\"}\n",
      "Done for page number:113\n",
      "{\"summary\": \"To persist data across pods, you need to use network-attached storage (NAS) like a GCE Persistent Disk. Create the disk in the same zone as your Kubernetes cluster using `gcloud compute disks create` command. Then, add an appropriate type of volume to the pod and mount it in the MongoDB container.\"}\n",
      "Done for page number:114\n",
      "{\"summary\": \"This chapter explains how to attach disk storage to containers using Kubernetes volumes. It creates a 1 GiB GCE persistent disk called mongodb and uses it in a volume inside a MongoDB pod. The YAML for the pod is provided, specifying the volume and container details. A note is also made about using Minikube, which cannot use a GCE Persistent Disk, but can deploy a hostPath volume instead.\"}\n",
      "Done for page number:115\n",
      "{\"summary\": \"To store data persistently, write documents to the MongoDB database using the MongoDB shell within the container. Insert a JSON document and use find() to verify it's stored. Then, delete and re-create the pod, ensuring the new pod can read persisted data from the same GCE persistent disk.\"}\n",
      "Done for page number:116\n",
      "{\"summary\": \"You can persist data across multiple pod instances by attaching disk storage to containers using volumes, such as GCE Persistent Disk or other types like awsElasticBlockStore (for AWS EC2) or azureFile/azureDisk (for Microsoft Azure). Create the underlying storage and set properties in the volume definition to use these alternatives. For example, changing from GCE Persistent Disk to awsElasticBlockStore requires modifying the volume definition as shown, where you'd replace `gcePersistentDisk` with `awsElasticBlockStore`.\"}\n",
      "Done for page number:117\n",
      "{\"summary\": \"Kubernetes supports various storage technologies such as NFS, ISCSI, GlusterFS, and others. However, having a pod's volumes tied to specific infrastructure details is not ideal for developers. It makes the pod definition cluster-specific and less portable. Instead, using persistent storage with an ID like \\\"my-volume\\\" is recommended.\"}\n",
      "Done for page number:118\n",
      "{\"summary\": \"Kubernetes aims to hide infrastructure from developers, allowing them to request storage without knowing specifics. Two new resources, Persistent Volumes (PVs) and Persistent Volume Claims (PVCs), were introduced to enable apps to request storage without dealing with infrastructure details. Cluster admins set up network storage, create PVs, and users create PVCs which are bound to available PVs, allowing pods to use the requested storage without knowing underlying tech.\"}\n",
      "Done for page number:119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Kubernetes PersistentVolumes are managed by cluster administrators who set up and register underlying storage. Users create a PersistentVolumeClaim manifest, specifying size and access mode requirements. Kubernetes finds an available PersistentVolume and binds it to the claim. The volume can be used within a pod, but other users cannot use the same PersistentVolume until it is released.\"}\n",
      "Done for page number:120\n",
      "{\"summary\": \"To attach disk storage to containers, create a PersistentVolume (PV) that specifies its capacity, read/write permissions, and storage type. Create the PV with kubectl create command. Afterward, list all PVs using kubectl get pv to check if it's available. A PV is cluster-level resource like nodes and doesn't belong to any namespace.\"}\n",
      "Done for page number:121\n",
      "{\"summary\": \"To use persistent storage in a Kubernetes pod, you must claim a PersistentVolume by creating a PersistentVolumeClaim. This is a separate process from creating a pod to ensure the claim remains available even if the pod is rescheduled. A PersistentVolumeClaim can be created using a YAML manifest and posted to the Kubernetes API through kubectl create.\"}\n",
      "Done for page number:122\n",
      "{\"summary\": \"A Kubernetes PersistentVolumeClaim is bound to a PersistentVolume with 1Gi capacity and ReadWriteOnce access mode. The claim is shown as Bound, and the PersistentVolume is also marked as Bound when inspected with kubectl get. Note that RWO means only one node can read and write, while ROX and RWX allow multiple nodes to read or read/write respectively. The PersistentVolumeClaim and Volume are now tied together for disk storage.\"}\n",
      "Done for page number:123\n",
      "{\"summary\": \"To use a PersistentVolumeClaim in a pod, reference it by name inside the pod's volume. This decouples pods from underlying storage technology. A pod can use a PersistentVolumeClaim created in the same namespace, and the claim is tied to the PersistentVolume until released. Example shows a pod using a PVC and PV for MongoDB data, with successful retrieval of stored documents.\"}\n",
      "Done for page number:124\n",
      "{\"summary\": \"Kubernetes' PersistentVolumes (PVs) and claims provide an indirect method of obtaining storage from infrastructure, simplifying application development. A pod can use a PV directly or through a claim, referencing the claim by name in one of its volumes. This approach makes it easier for developers to manage storage without worrying about specific infrastructure details, and allows the same pod and claim manifests to be used across multiple Kubernetes clusters.\"}\n",
      "Done for page number:125\n",
      "{\n",
      "  \"summary\": \"Decoupling pods from storage technology involves understanding PersistentVolume reclaim policies. The default policy is 'Delete', but setting it to 'Retain' allows the cluster admin to clean up a released PersistentVolume before re-claiming it. Two other options exist: 'Recycle', which deletes contents and makes the volume available, and 'Delete', which removes underlying storage.\"\n",
      "}\n",
      "Done for page number:126\n",
      "{\"summary\": \"A PersistentVolume only supports Retain or Delete policies. Kubernetes can perform dynamic provisioning of PersistentVolumes through provisioners and StorageClass objects. Users can choose the type of PersistentVolume they want by referencing the StorageClass in their PersistentVolumeClaim. The cluster admin can deploy a provisioner and define one or more StorageClass objects to let users choose what type of PersistentVolume they want, making it easy to obtain persistent storage without the developer having to deal with the actual storage technology used underneath.\"}\n",
      "Done for page number:127\n",
      "{\"summary\": \"Dynamic Persistent Volumes are provisioned through StorageClasses, which define available storage types and provisioners. Users request a specific StorageClass in their PersistentVolumeClaim, and the system creates a new PersistentVolume. This approach ensures that storage space is not depleted.\"}\n",
      "Done for page number:128\n",
      "{\"summary\": \"When creating a PersistentVolumeClaim, you can specify the storage class using the `storageClassName` field. This allows the provisioner to dynamically create a PersistentVolume matching the claim's specifications. The `accessModes` and `resources.requests.storage` fields also define the access modes and size of the requested storage. If an existing manually provisioned PersistentVolume matches the claim, it will be used instead. The created PVC and dynamically provisioned PV can be examined using `kubectl get`, showing the bound volume and its capacity and access modes.\"}\n",
      "Done for page number:129\n",
      "{\"summary\": \"The document explains how to use dynamic provisioning of PersistentVolumes with Kubernetes, specifically with StorageClasses. It shows how to create multiple storage classes with different characteristics and how claims refer to them by name, making PVC definitions portable across clusters. The portability is demonstrated by running the same example on Minikube with a custom storage class. The document also explains how to list available storage classes in GKE and Minikube.\"}\n",
      "Done for page number:130\n",
      "{\"summary\": \"A Kubernetes default storage class can be seen using `kubectl get sc standard -o yaml`, which displays the `standard` storage class with a default annotation, indicating it's used for dynamic provisioning of Persistent Volumes without specifying a storage class. A PVC can be created without specifying a storage class, and on GKE, it will provision a GCE Persistent Disk of type pd-standard.\"}\n",
      "Done for page number:131\n",
      "{\"summary\": \"Dynamic provisioning of PersistentVolumes: A PVC definition can use the default storage class when creating, and specifying an empty string for storageClassName ensures binding to a pre-provisioned PV instead of dynamic provisioning. The complete picture involves only creating the PVC and pod, with the dynamic provisioner handling everything else.\"}\n",
      "Done for page number:132\n",
      "{\"summary\": \"This chapter explains how volumes provide storage to containers in a pod, including creating multi-container pods with shared files, using various volume types (emptyDir, gitRepo, hostPath), and dynamically provisioned PersistentVolumes through PersistentVolumeClaims and StorageClasses. A complete picture of dynamic provisioning is also provided.\"}\n",
      "Done for page number:133\n",
      "{\"summary\": \"This chapter explains how to pass configuration data to applications running in Kubernetes, covering changing the main process of a container, passing command-line options, setting environment variables, configuring apps through ConfigMaps, and passing sensitive information through Secrets.\"}\n",
      "Done for page number:134\n",
      "{\"summary\": \"ConfigMaps and Secrets are used to configure applications in Kubernetes. ConfigMaps store configuration data and can be mounted into containers using volumes or environment variables. Secrets, on the other hand, handle sensitive information like credentials and encryption keys with special care. Command-line arguments can also be passed to containers, but using environment variables is popular due to its simplicity and security.\"}\n",
      "Done for page number:135\n",
      "{\"summary\": \"In Docker, command-line arguments can be passed to containers instead of the ones specified in the image by using ENTRYPOINT and CMD instructions. ENTRYPOINT defines the executable invoked when the container is started, while CMD specifies default arguments that get passed to it. The correct way to specify a command is through ENTRYPOINT, and CMD should only define default arguments. This allows running an image with or without additional arguments.\"}\n",
      "Done for page number:136\n",
      "{\"summary\": \"ConfigMaps and Secrets are used to configure applications in Kubernetes. The example uses a Dockerfile to create an image that runs a script with a configurable interval, which is set to 10 seconds by default. The image can be built and pushed to Docker Hub, and run locally with the exec form of the ENTRYPOINT instruction.\"}\n",
      "Done for page number:137\n",
      "{\"summary\": \"You can pass command-line arguments to containers using Docker and Kubernetes by specifying the 'command' and 'args' fields in the container specification. This allows you to override the default behavior of an image and customize its execution. In Kubernetes, this is done by setting the 'command' and 'args' properties in the pod specification. Note that these fields cannot be updated after the pod is created.\"}\n",
      "Done for page number:138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"ConfigMaps and Secrets in Kubernetes allow passing configuration options to containers through command-line arguments or environment variables, where values are passed as arrays or lists of key-value pairs, with options for specifying custom variables for each container within a pod, but not at the pod level.\"}\n",
      "Done for page number:139\n",
      "{\"summary\": \"This section explains how to make environment variables configurable within a container, and how to specify these variables when defining a container in Kubernetes. It provides examples of modifying a bash script to use an environment variable, and defining a pod with an environment variable passed to the container.\"}\n",
      "Done for page number:140\n",
      "{\"summary\": \"ConfigMaps and Secrets allow decoupling application configuration from the pod descriptor, enabling reusability across environments. Environment variables can reference previously defined variables using $(VAR) syntax. Hardcoding environment variables leads to separate pod definitions for production and development pods, but ConfigMaps enable reuse of the same pod definition in multiple environments by keeping configuration options separate from source code.\"}\n",
      "Done for page number:141\n",
      "{\"summary\": \"ConfigMaps allow decoupling configuration from pods, enabling multiple manifests for the same ConfigMap to be used across different environments (dev, test, prod, etc.) by referencing them by name. Pods can consume ConfigMaps through environment variables or config volumes, keeping the app Kubernetes-agnostic.\"}\n",
      "Done for page number:142\n",
      "{\"summary\": \"A ConfigMap is created using the kubectl create configmap command, defining entries with literals or files. A single-entry ConfigMap is created with $ kubectl create configmap fortune-config --from-literal=sleep-interval=25. Multiple entries can be added with multiple --from-literal arguments, as shown in $ kubectl create configmap myconfigmap --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two. The ConfigMap's YAML descriptor is inspected using the kubectl get command, displaying its name, namespace, and data entries.\"}\n",
      "Done for page number:143\n",
      "{\"summary\": \"ConfigMaps in Kubernetes can store configuration data, including files and literals. You can create a ConfigMap from a file using `kubectl create configmap` with the `--from-file` option, specifying the file path or a custom key. You can also combine different options, such as importing multiple files from a directory or adding literal values. This allows for flexible configuration management within Kubernetes.\"}\n",
      "Done for page number:144\n",
      "{\"summary\": \"A ConfigMap can be passed to a container as an environment variable using the valueFrom field in the pod descriptor. Three options are available: setting an environment variable with the simplest method, passing multiple values as separate entries, or creating a ConfigMap from individual files, a directory, and literal values.\"}\n",
      "Done for page number:145\n",
      "{\"summary\": \"A ConfigMap named fortune-config is referenced to set the value of an environment variable INTERVAL in a pod, allowing for decoupling configuration from the pod specification and keeping options together rather than scattered across multiple pods.\"}\n",
      "Done for page number:146\n",
      "{\"summary\": \"You can expose all entries of a ConfigMap as environment variables using Kubernetes version 1.6's envFrom attribute, which is more efficient than creating individual env variables. A prefix (e.g. CONFIG_) can be specified to customize the variable names. However, if a ConfigMap key contains an invalid character (like dashes), it will be skipped and an event recorded. Additionally, ConfigMap entries can be passed as command-line arguments by initializing an environment variable from the ConfigMap entry and referencing it in the pod's args field.\"}\n",
      "Done for page number:147\n",
      "{\"summary\": \"A ConfigMap can be used to decouple configuration with a Pod by exposing each entry as a file through a configMap volume. This allows the process running in the container to obtain the entry's value by reading the contents of the file, making it suitable for whole config files rather than short variable values.\"}\n",
      "Done for page number:148\n",
      "{\"summary\": \"A ConfigMap is created to configure an application, in this case an Nginx web server running inside a fortune pod's web-server container. A config file for Nginx is created and stored in a local directory along with a plain text file containing the sleep-interval entry. The ConfigMap is then created from these files using kubectl create configmap command.\"}\n",
      "Done for page number:149\n",
      "{\"summary\": \"A ConfigMap is used to decouple configuration from a pod's containers by storing it in a separate entity. The YAML definition of a ConfigMap shows two entries, my-nginx-config.conf and sleep-interval, which contain multi-line values. These entries can be used to populate a volume in a container, allowing the configuration to be used without overriding the default config file.\"}\n",
      "Done for page number:150\n",
      "{\n",
      "  \"summary\": \"This chapter explains how to configure applications using ConfigMaps and Secrets. A Pod definition is provided that mounts a ConfigMap volume into the /etc/nginx/conf.d directory, allowing Nginx to use it. The example demonstrates verifying that the web server is configured to compress responses by enabling port-forwarding and checking with curl.\"\n",
      "}\n",
      "Done for page number:151\n",
      "{\n",
      "  \"summary\": \"ConfigMap can be used to decouple configuration from a pod, allowing for separate configuration of containers within the same pod. To expose certain ConfigMap entries as files in a volume, use the volume's items attribute, specifying the key and path for each entry.\"\n",
      "}\n",
      "Done for page number:152\n",
      "{\"summary\": \"When mounting ConfigMaps or Secrets as volumes, be aware that it can hide existing files in the target directory if mounted as a directory. To add individual files without hiding others, use the subPath property to mount either a single file or directory from the volume.\"}\n",
      "Done for page number:153\n",
      "{\"summary\": \"A ConfigMap in Kubernetes can be used to update an application's configuration without restarting it. By exposing a ConfigMap through a volume, the files are updated automatically when the ConfigMap is modified. This allows for dynamic updates to configuration values without requiring a pod or container restart. File permissions can also be set using the defaultMode property, with a warning that file updates may take a long time (up to 1 minute).\"}\n",
      "Done for page number:154\n",
      "{\"summary\": \"A ConfigMap can be edited using kubectl edit to change its contents, and the changes will propagate to the actual file in the volume. However, the application may not automatically reload the updated files. To force Nginx to reload its config files, use the command `nginx -s reload`. Kubernetes achieves atomic updates by using symbolic links.\"}\n",
      "Done for page number:155\n",
      "{\"summary\": \"When updating a ConfigMap, Kubernetes creates a new directory with updated files, but mounted single files won't be updated. This can lead to different running instances being configured differently. Modifying an existing ConfigMap may not be ideal if the app doesn't reload its config automatically, and even if it does, files in individual pods may be out of sync for up to a minute.\"}\n",
      "Done for page number:156\n",
      "{\"summary\": \"Kubernetes provides Secrets for storing sensitive data, similar to ConfigMaps but more secure. Secrets are used to store non-sensitive configuration data in a ConfigMap and sensitive data in a Secret. The default token Secret is mounted into every container automatically, and can be listed with kubectl get secrets. Choosing between ConfigMaps and Secrets is simple: use a ConfigMap for plain configuration data and a Secret for sensitive data that needs to be kept secure.\"}\n",
      "Done for page number:157\n",
      "{\"summary\": \"A Kubernetes Secret allows passing sensitive data to containers. It contains entries like ca.crt, namespace, and token that represent necessary information for secure communication with the Kubernetes API server from within pods. The default-token Secret is mounted into every container by default but can be disabled in each pod or service account.\"}\n",
      "Done for page number:158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"You'll create a Secret called fortune-https to configure Nginx container to serve HTTPS traffic by generating a certificate, private key, and dummy file. The files are added to the Secret using kubectl create secret generic command, similar to creating ConfigMaps. This creates a volume in each pod with a default-token Secret.\"}\n",
      "Done for page number:159\n",
      "{\"summary\": \"Secrets in Kubernetes can contain sensitive or binary data and are encoded in Base64 for storage in plain-text formats like YAML or JSON. Unlike ConfigMaps, which display clear text, Secrets entries show as encoded strings. This allows for storing binary values within YAML or JSON manifests.\"}\n",
      "Done for page number:160\n",
      "{\"summary\": \"Kubernetes allows setting a Secret's values through the stringData field for sensitive non-binary data. The field is write-only and values are displayed under 'data' in YAML format. When exposed to a container or environment variable, values are decoded and written in their actual form.\"}\n",
      "Done for page number:161\n",
      "{\"summary\": \"This page explains how to use secrets in Kubernetes to pass sensitive data to containers. It uses an example of a web server configured with Nginx, which reads certificate and key files from /etc/nginx/certs. A pod is created to mount the secret volume into the correct location in the web-server container.\"}\n",
      "Done for page number:162\n",
      "{\"summary\": \"A Kubernetes pod is configured with a ConfigMap and a Secret to run an https server. The ConfigMap, named fortune-config, exposes a configuration file my-nginx-config.conf. The Secret, named fortune-https, provides SSL certificates, referenced in the volumes as certs. The default-token Secret and volume are automatically added but not shown in the figure. Environment variables INTERVAL=25 are set for the pod.\"}\n",
      "Done for page number:163\n",
      "{\"summary\": \"Using Kubernetes Secrets to pass sensitive data to containers, such as certificates and keys, can be done through volume mounts or environment variables. A secret volume uses an in-memory filesystem (tmpfs) for storing Secret files, keeping them from being written to disk. Individual entries from a secret can also be exposed as environment variables, allowing for secure access to sensitive data within a container.\"}\n",
      "Done for page number:164\n",
      "{\"summary\": \"ConfigMaps and Secrets are used to configure applications in Kubernetes. However, exposing secrets through environment variables is not recommended due to potential security risks. Instead, use secret volumes or image pull secrets when deploying a pod with images from private registries.\"}\n",
      "Done for page number:165\n",
      "{\"summary\": \"Creating a Secret for authenticating with a Docker registry involves specifying credentials such as username, password, and email using the kubectl create secret command. This Secret can then be used in a pod definition by listing it under imagePullSecrets, allowing Kubernetes to pull images from private repositories without needing to specify credentials on every pod.\"}\n",
      "Done for page number:166\n",
      "{\"summary\": \"This chapter concludes by summarizing key takeaways on passing configuration data to containers, including overriding commands, setting environment variables, using ConfigMaps and Secrets for sensitive data, and delivering Docker-registry Secrets to pull private images.\"}\n",
      "Done for page number:167\n",
      "{\"summary\": \"This chapter focuses on enabling applications to access pod and other resources metadata, such as environment details, through the Downward API or by talking directly to the Kubernetes API server. Topics include passing information into containers, exploring the REST API, accessing the API from within a container, understanding the ambassador pattern, and using client libraries.\"}\n",
      "Done for page number:168\n",
      "{\"summary\": \"The Kubernetes Downward API solves two problems: passing metadata about the pod and its environment without repeating information, and exposing the pod's own metadata to processes running inside the pod. It allows passing of metadata such as pod name, IP address, labels, and annotations through environment variables or files in a downwardAPI volume.\"}\n",
      "Done for page number:169\n",
      "{\"summary\": \"The Downward API allows passing metadata to containers through environment variables or a volume, but labels and annotations can only be exposed through the volume. An example is given using the Downward API to pass pod and container metadata to a containerized process through environment variables, including namespace, node name, service account, CPU and memory requests and limits, pod's labels and annotations.\"}\n",
      "Done for page number:170\n",
      "{\"summary\": \"This chapter explains how to access pod metadata and other resources from applications by using environment variables. You can look up environment variables defined in the pod spec, such as POD_NAME, POD_IP, NODE_NAME, SERVICE_ACCOUNT, CONTAINER_CPU_REQUEST_MILLICORES, and CONTAINER_MEMORY_LIMIT_KIBIBYTES. These variables expose information like the pod's name, IP, namespace, node name, service account, CPU requests, and memory limits. You can specify divisors to get values in specific units.\"}\n",
      "Done for page number:171\n",
      "{\"summary\": \"The Downward API allows passing metadata to a pod as environment variables, using divisors for CPU (1 or 1m) and memory (1, 1k, 1Ki, etc.) limits and requests. Environment variables can be viewed using kubectl exec, showing attributes like POD_NAME, CONTAINER_CPU_REQUEST_MILLICORES, and CONTAINER_MEMORY_LIMIT_KIBIBYTES. The pod manifest specifies resources, serviceAccountName, and nodeName.\"}\n",
      "Done for page number:172\n",
      "{\"summary\": \"This chapter discusses accessing pod metadata and other resources from applications. It covers two methods: passing metadata through environment variables and using a downwardAPI volume. The latter method is used when exposing pod labels or annotations, which cannot be exposed through environment variables. A YAML example is provided for creating a pod with a downwardAPI volume, where the pod's name and namespace are written to files in the container.\"}\n",
      "Done for page number:173\n",
      "{\"summary\": \"The Downward API allows passing metadata from a pod to a container through a volume called downward. The volume contains files with metadata such as labels, annotations, and resource usage written by the pod. The metadata is referenced in the pod manifest under the downwardAPI.items attribute. For example, the pod's labels are written to /etc/downward/labels and annotations to /etc/downward/annotations.\"}\n",
      "Done for page number:174\n",
      "{\"summary\": \"You can access pod metadata and other resources from applications by mounting a downwardAPI volume, which contains files corresponding to metadata fields like labels and annotations. Each file has contents in key=value format on separate lines for multi-line values. Labels and annotations can be updated while a pod is running, and Kubernetes updates the files holding them.\"}\n",
      "Done for page number:175\n",
      "{\n",
      "  \"summary\": \"The Downward API allows passing pod and container metadata to applications, but its metadata is limited. To access more information, you need to talk directly to the Kubernetes API server.\"\n",
      "}\n",
      "Done for page number:176\n",
      "{\"summary\": \"Accessing Kubernetes API server directly from an application running in a pod can be achieved by using kubectl proxy command, which runs a proxy server that accepts HTTP connections on local machine and proxies them to the API server while taking care of authentication. This allows applications to talk to the API server without dealing with authentication themselves.\"}\n",
      "Done for page number:177\n",
      "{\"summary\": \"Running kubectl proxy is trivial, serving on local port 8001. Using curl or a web browser to access the API server's base URL returns a list of paths corresponding to API groups and versions, which match resource definitions in Kubernetes.\"}\n",
      "Done for page number:178\n",
      "{\"summary\": \"The batch API group contains two versions. Clients should use the v1 version instead of v2alpha1. The Job resource is namespaced and described in detail.\"}\n",
      "Done for page number:179\n",
      "{\"summary\": \"The Kubernetes API server returns a list of resource types and REST endpoints in the batch/v1 API group. The Job resource allows retrieval, update, and deletion through the /apis/batch/v1/jobs endpoint, with additional endpoints for modifying status.\"}\n",
      "Done for page number:180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"This chapter focuses on accessing pod metadata and other resources from applications. The Kubernetes REST API server can be accessed using curl, allowing applications to retrieve resources such as Jobs. To access a specific Job instance, the name and namespace must be specified in the URL. From within a pod, one must find the location of the API server, verify its authenticity, and authenticate with the server before making requests.\"}\n",
      "Done for page number:181\n",
      "{\"summary\": \"To talk to the Kubernetes API server, create a pod using the tutum/curl image and run a bash shell inside its container. Then, use curl to access the API server from within that shell. To find the API server's address, look up the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables, which are automatically configured for each service.\"}\n",
      "Done for page number:182\n",
      "{\"summary\": \"To access pod metadata and resources, applications can use the environment variables provided by Kubernetes. However, it's recommended to verify the server's identity by checking its certificate, rather than using curl's --insecure option. The default-token-xyz Secret provides a CA.crt file that holds the certificate of the CA used to sign the API server's certificate, which can be used to verify the API server's identity with curl's --cacert option.\"}\n",
      "Done for page number:183\n",
      "{\"summary\": \"To authenticate with the Kubernetes API server, set the CURL_CA_BUNDLE environment variable to trust its certificate. Then, obtain an authentication token by loading it into an environment variable (TOKEN) and use it in curl requests with -H \\\"Authorization: Bearer $TOKEN\\\" to access API objects deployed in the cluster.\"}\n",
      "Done for page number:184\n",
      "{\"summary\": \"You can access pod metadata and other resources from applications by passing a token inside the Authorization HTTP header in the request. You can also retrieve the namespace the pod is running in from the secret volume file called namespace. By using this information, you can list all pods running in the same namespace as your pod or even update them by sending PUT or PATCH requests.\"}\n",
      "Done for page number:185\n",
      "{\"summary\": \"An app running inside a pod can access the Kubernetes API properly by verifying the API server's certificate, authenticating itself with a bearer token, and using a namespace file to pass the namespace to the API server. This process can be simplified using Ambassador containers.\"}\n",
      "Done for page number:186\n",
      "{\"summary\": \"The chapter explains how to access pod metadata and resources from applications using the kubectl proxy command within a pod. It introduces the ambassador container pattern, where an application communicates with the API server through a proxy in a separate container, handling security and authentication transparently. A curl pod is created with an additional ambassador container to demonstrate this concept.\"}\n",
      "Done for page number:187\n",
      "{\"summary\": \"To connect to the Kubernetes API server without dealing with authentication tokens and server certificates, an ambassador container can be used. The ambassador container runs a proxy (kubectl-proxy) that sends an HTTPS request to the API server on behalf of the main container, handling client authentication and server certificate validation. This simplifies the process for the app running in the main container, but requires an additional process consuming resources.\"}\n",
      "Done for page number:188\n",
      "{\"summary\": \"Kubernetes API client libraries are available for various programming languages, including Golang, Python, Java, Node.js, PHP, and others. The two officially supported libraries are the Golang client and the Python client. Additionally, user-contributed client libraries exist for other languages, such as Fabric8's Java client, Amdatu's Java client, and tenxcloud's Node.js client.\"}\n",
      "Done for page number:189\n",
      "{\"summary\": \"Client libraries for Kubernetes allow interaction with the API server without needing an ambassador container, as they handle HTTPS and authentication. An example is shown using the Fabric8 Java client to list services, create a pod, and edit its metadata.\"}\n",
      "Done for page number:190\n",
      "{\"summary\": \"The chapter discusses accessing pod metadata and other resources from applications in Kubernetes. It shows an example code using the Fabric8 client to create and delete a pod. If no client is available, it suggests using the Swagger API framework to generate a client library and documentation. The Swagger UI can be used to explore the REST API, and can be enabled on the API server with the --enable-swagger-ui=true option.\"}\n",
      "Done for page number:191\n",
      "{\n",
      "  \"summary\": \"This chapter explains how a pod can get data about itself and its environment through environment variables or files in a downwardAPI volume, as well as how to interact with the Kubernetes REST API using kubectl proxy. It also covers how pods can find the API server's location and authenticate themselves.\"\n",
      "}\n",
      "Done for page number:192\n",
      "{\"summary\": \"Kubernetes provides a Deployment resource that enables updating applications declaratively, allowing for true zero-downtime updates. This includes replacing pods with newer versions, performing rolling updates, controlling the rollout rate, and reverting to previous versions.\"}\n",
      "Done for page number:193\n",
      "{\"summary\": \"Updating applications running in pods involves replacing old pods with new ones, either by deleting and re-creating or adding and then removing them gradually. This process requires careful consideration to avoid application downtime or compatibility issues between versions.\"}\n",
      "Done for page number:194\n",
      "{\"summary\": \"Kubernetes allows for two update methods in deployments: manually updating pods by modifying the replication controller's pod template and replacing old pods with new ones, or spinning up new pods while keeping old ones running to avoid downtime. The process involves changing the pod template to refer to a newer version of the image and letting the ReplicationController spin up new instances.\"}\n",
      "Done for page number:195\n",
      "{\"summary\": \"Updating applications running in pods involves two methods: switching from the old to the new version at once, or performing a rolling update. The first method requires double the number of pods for a short while, but is simpler. The second method replaces pods step by step, scaling down the previous ReplicationController and scaling up the new one. A single Kubernetes command can perform this rolling update, making it less laborious and error-prone.\"}\n",
      "Done for page number:196\n",
      "{\"summary\": \"This chapter discusses deploying applications declaratively using Deployments and performing automatic rolling updates with a ReplicationController, which is now an outdated way of updating apps. The process involves running the initial version of the app, creating a new version (v2) that returns its version number in the response, and then using kubectl to perform the update, but this method will be replaced by more efficient methods later on.\"}\n",
      "Done for page number:197\n",
      "{\"summary\": \"Creating a ReplicationController and Service using a single YAML file, which defines multiple resources separated by a line of three dashes. The YAML contains a ReplicationController called kubia-v1 and a Service called kubia, enabling access to the app externally via a LoadBalancer. Posting this YAML to Kubernetes creates three v1 pods and a load balancer, allowing you to start hitting the service with curl.\"}\n",
      "Done for page number:198\n",
      "{\"summary\": \"To update an application declaratively using Kubernetes deployments, create version 2 of the app by changing the response in the code and tagging it with 'v2'. Pushing updates to the same image tag isn't recommended as changes won't be picked up if the imagePullPolicy is set to 'IfNotPresent'. Instead, use a new tag or set imagePullPolicy to 'Always' to ensure all nodes run the updated version.\"}\n",
      "Done for page number:199\n",
      "{\"summary\": \"Performing an automatic rolling update with a ReplicationController involves running the kubectl rolling-update command, specifying the old ReplicationController to replace, a new name for the replacement ReplicationController, and the new image. This process creates a new ReplicationController with the desired replicas set to 0, while scaling down the old one.\"}\n",
      "Done for page number:200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"kubectl creates a ReplicationController by copying an existing one, modifying its image and label selector. Before commencing a rolling update, kubectl modifies the original controller's selector to avoid conflicts with the new controller's pod selector, which includes a deployment label. The labels of live pods are also modified to include this deployment label.\"}\n",
      "Done for page number:201\n",
      "{\"summary\": \"kubectl performs an automatic rolling update with a ReplicationController, replacing old pods with new ones by scaling up the new controller while scaling down the old one, allowing for a smooth transition with minimal downtime.\"}\n",
      "Done for page number:202\n",
      "{\"summary\": \"Kubernetes deployments can be updated declaratively using a RollingUpdate strategy, which involves scaling down the old version to zero and then deleting it, leaving only the new version in place with zero downtime. However, the `kubectl rolling-update` command is now obsolete due to its inability to update objects without modifying them, which can cause unexpected changes to user resources.\"}\n",
      "Done for page number:203\n",
      "{\"summary\": \"Deployments in Kubernetes provide a declarative way to update applications, replacing imperative methods like ReplicationControllers and ReplicaSets. A Deployment creates a ReplicaSet that manages pods, ensuring smooth updates even with network connectivity issues. This is the preferred method for deploying applications in Kubernetes, allowing for rolling updates and desired state management.\"}\n",
      "Done for page number:204\n",
      "{\"summary\": \"A Deployment resource makes updating an application easier by defining the desired state through a single resource. It contains a label selector, replica count, and pod template, with an additional deployment strategy field. Creating a Deployment is similar to creating a ReplicationController, requiring only three trivial changes to the YAML manifest. Deployments are in the apps API group, version v1beta1.\"}\n",
      "Done for page number:205\n",
      "{\"summary\": \"A deployment is created using kubectl create -f kubia-deployment-v1.yaml --record and records the command in revision history. The rollout status can be checked with kubectl rollout status deployment kubia, which shows a successful roll out with three pod replicas up and running. A deployment creates ReplicaSets and pods, with names containing a hash value of the pod template, allowing multiple ReplicaSets for each version of the pod.\"}\n",
      "Done for page number:206\n",
      "{\"summary\": \"Deployments are used for updating applications declaratively, allowing Kubernetes to automatically transform the system state to match the desired state defined in the Deployment resource. The default deployment strategy is RollingUpdate, which performs a rolling update by replacing old pods with new ones, while Recreate deletes all old pods at once and creates new ones.\"}\n",
      "Done for page number:207\n",
      "{\"summary\": \"The RollingUpdate strategy in Kubernetes updates apps by removing old pods one by one while adding new ones, keeping the application available throughout the process. To slow down the update process, set the minReadySeconds attribute on the Deployment using kubectl patch command. To trigger a rollout, change the image used in the pod container using kubectl set image command.\"}\n",
      "Done for page number:208\n",
      "{\"summary\": \"Deployments can be updated declaratively using various methods such as kubectl edit, patch, apply, replace and set image. These methods change the Deployment's specification, triggering a rollout process. The methods are equivalent for Deployments and include updating the image registry, pod template, or container, among others.\"}\n",
      "Done for page number:209\n",
      "{\"summary\": \"Using Deployments for updating apps declaratively allows you to change the pod template in your Deployment resource, updating your app to a newer version by changing a single field. This process is simpler than running a special command with kubectl and lets the controllers perform the update, creating an additional ReplicaSet and scaling it up while scaling down the previous one.\"}\n",
      "Done for page number:210\n",
      "{\"summary\": \"The chapter explains Deployments in Kubernetes, which allows for declarative updates to applications. A Deployment resource is created and managed, with underlying ReplicaSets being an implementation detail. The difference between managing a single Deployment object versus multiple ReplicationControllers becomes apparent when issues arise during the rollout process. The example introduces a bug into version 3 of the app, causing it to handle only the first four requests properly before returning a 500 internal server error on subsequent requests.\"}\n",
      "Done for page number:211\n",
      "{\"summary\": \"You've updated the app to v3 by changing the Deployment specification, but it's causing internal server errors. To fix this, you can use `kubectl rollout undo deployment kubia` to roll back to the previous revision and remove the bad pods.\"}\n",
      "Done for page number:212\n",
      "{\"summary\": \"Deployments in Kubernetes keep a revision history, allowing for rolling back to previous versions using the kubectl rollout undo command. The revision history is stored in ReplicaSets, and deleting an old ReplicaSet will lose that specific revision from the Deployment's history. The length of the revision history is limited by the revisionHistoryLimit property on the Deployment resource, which defaults to two, preserving only the current and previous revisions.\"}\n",
      "Done for page number:213\n",
      "{\n",
      "  \"summary\": \"You can control the rate of a deployment's rollout using two properties of the rolling update strategy: maxSurge and maxUnavailable. MaxSurge determines how many pod instances can exist above the desired replica count, defaulting to 25% or an absolute value like one or two additional pods. MaxUnavailable determines how many pod instances can be unavailable during the update, also defaulting to 25% or an absolute value.\"\n",
      "}\n",
      "Done for page number:214\n",
      "{\"summary\": \"Deployments can be updated declaratively using `maxSurge` and `maxUnavailable` properties to control the rollout process. The default values for `maxSurge` are 25% of total replicas, but in v1beta1 Deployments, it's set to 1, while `maxUnavailable` defaults to 0 or 1, depending on the version. This affects how the rollout unwinds, with the example showing a rollout process where pods are deleted and new ones created until both versions are available.\"}\n",
      "Done for page number:215\n",
      "{\"summary\": \"Deployments in Kubernetes allow for declarative updates of applications, ensuring that at least a specified number of replicas remain available during rollout. maxUnavailable is relative to the desired replica count, meaning that the update process must keep at least the minimum number of pods available, while the number of unavailable pods can exceed the maximum. Deployments can also be paused during rollout, allowing for verification before proceeding with the rest of the rollout.\"}\n",
      "Done for page number:216\n",
      "{\n",
      "  \"summary\": \"The chapter discusses Deployments in Kubernetes, specifically rolling out updates declaratively using the rollout feature. A deployment can be paused to test a new version before resuming the rollout or rolled back if issues arise. The minReadySeconds property prevents deploying malfunctioning versions by waiting for pods to report readiness before continuing with the rollout.\"\n",
      "}\n",
      "Done for page number:217\n",
      "{\"summary\": \"A small summary of the document page is: Using Deployments for updating apps declaratively involves defining a readiness probe and minReadySeconds to prevent buggy versions from being rolled out fully. This can be achieved by configuring a readiness probe and setting minReadySeconds, which prevents Kubernetes from deploying buggy versions. An example YAML file is provided to update the deployment with a readiness probe.\"}\n",
      "Done for page number:218\n",
      "{\"summary\": \"To update a deployment with kubectl apply, use the command `kubectl apply -f <yaml_file>` and ensure the YAML file doesn't include the replicas field to maintain the desired replica count. The rollout status command can be used to check the progress of the update. A readiness probe can also be defined in the YAML file to execute an HTTP GET request against the container, checking its readiness before serving traffic.\"}\n",
      "Done for page number:219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A deployment is updated declaratively, with a readiness probe that prevents bad versions from being rolled out. The probe hits every second and fails when returning HTTP status code 500, marking the pod as not ready. The rollout process stops, waiting for the new pod to be available for at least 10 seconds, ensuring clients don't hit an unhealthy pod.\"}\n",
      "Done for page number:220\n",
      "{\"summary\": \"Deployments allow updating applications declaratively, but can get stuck if there's an issue with the rollout. A readiness probe and minReadySeconds setting help prevent this. If the rollout gets stuck, it can be aborted using kubectl rollout undo deployment. The progress deadline is configurable via progressDeadlineSeconds property in the Deployment spec.\"}\n",
      "Done for page number:221\n",
      "{\"summary\": \"This chapter shows how to use a declarative approach in Kubernetes to deploy and update applications, covering topics such as rolling updates, Deployments, and updating pods. You learned how to perform tasks like rolling back a Deployment, pausing it mid-way, controlling the rate of rollout, and using readiness probes to block faulty versions.\"}\n",
      "Done for page number:222\n",
      "{\"summary\": \"This chapter covers deploying stateful clustered applications, providing separate storage for each instance of a replicated pod, guaranteeing a stable name and hostname for pod replicas, starting and stopping pod replicas in a predictable order, and discovering peers through DNS SRV records.\"}\n",
      "Done for page number:223\n",
      "{\"summary\": \"Replicating stateful pods is challenging due to their reliance on separate storage volumes, which can't be achieved with a single ReplicaSet. To overcome this, multiple ReplicaSets or manual pod creation with dedicated PersistentVolumeClaims can be used, but the latter requires manual management and recreation upon node failures.\"}\n",
      "Done for page number:224\n",
      "{\n",
      "  \"summary\": \"In Kubernetes, StatefulSets allow deploying replicated stateful applications. However, if multiple ReplicaSets are used, changing the desired replica count is not feasible. A workaround is to use a single ReplicaSet with persistent storage and have each pod instance use its own directory within the shared volume, or provide a stable identity for each pod.\"\n",
      "}\n",
      "Done for page number:225\n",
      "{\"summary\": \"A stable network identity is required for certain distributed stateful applications in Kubernetes. To achieve this, a dedicated service can be created for each pod instance, providing a stable IP address that doesn't change when the pod is rescheduled. However, this solution has limitations, as individual pods cannot know which service they are exposed through and therefore cannot self-register using their stable IP.\"}\n",
      "Done for page number:226\n",
      "{\n",
      "  \"summary\": \"StatefulSets in Kubernetes allow deploying replicated stateful applications with stable names and states. Unlike ReplicaSets or ReplicationControllers, which treat instances as replaceable 'cattle', StatefulSets manage instances as non-fungible 'pets' that require the same name, network identity, and state to be resurrected on another node if they fail.\"\n",
      "}\n",
      "Done for page number:227\n",
      "{\"summary\": \"A StatefulSet ensures pods retain their identity and state, allowing for easy scaling and predictable network identities through ordinal indexing, hostname derivation, and stable storage. It requires a governing headless Service to provide actual network identity, enabling peer-to-peer communication and client access by hostname.\"}\n",
      "Done for page number:228\n",
      "{\"summary\": \"StatefulSets are used to deploy replicated stateful applications, allowing access to pods through their fully qualified domain name and using DNS to look up all pods' names. If a pod instance disappears, the StatefulSet replaces it with a new instance that gets the same name and hostname as the lost pod, unlike ReplicaSets which replace with completely new unrelated pods.\"}\n",
      "Done for page number:229\n",
      "{\"summary\": \"The new pod isnt necessarily scheduled to the same node, but as you learned early on, what node a pod runs on shouldnt matter. Scaling a StatefulSet creates a new pod instance with the next unused ordinal index. It scales down only one pod instance at a time and never permit scale-down operations if any of the instances are unhealthy. Each stateful pod instance needs to use its own storage, plus if a state-ful pod is rescheduled, the new instance must have the same storage attached to it.\"}\n",
      "Done for page number:230\n",
      "{\"summary\": \"A StatefulSet in Kubernetes can create PersistentVolumeClaims along with pod instances, allowing each pod to have its own separate storage. However, these claims are not deleted when the scale-down of a StatefulSet is triggered, as deleting them would result in losing the data stored in the volume. Instead, they must be manually deleted to release the underlying PersistentVolume.\"}\n",
      "Done for page number:231\n",
      "{\"summary\": \"StatefulSets in Kubernetes have stable identity and storage, meaning pods are replaced with identical ones if the old pod is deleted or scaled down. This reattachment of persistent volume claims (PVCs) to new pod instances prevents data loss, making StatefulSets a reliable choice for stateful applications.\"}\n",
      "Done for page number:232\n",
      "{\"summary\": \"StatefulSets in Kubernetes must ensure two stateful pod instances never run with the same identity and are bound to the same PersistentVolumeClaim. A StatefulSet must guarantee at-most-one semantics for stateful pod instances, ensuring a pod is no longer running before creating a replacement. This has implications for node failures.\"}\n",
      "Done for page number:233\n",
      "{\"summary\": \"The document describes deploying an application using a StatefulSet, which requires creating PersistentVolumes for storing data files, a governing Service, and the StatefulSet itself. A Dockerfile is provided to build the container image. The process involves creating three PersistentVolumes if scaling up more than three replicas is planned. Instructions are given for Minikube and Google Kubernetes Engine.\"}\n",
      "Done for page number:234\n",
      "{\"summary\": \"This chapter is about deploying replicated stateful applications using StatefulSets. It involves creating PersistentVolumes from the persistent-volumes-gcepd.yaml file, which defines three volumes named pv-a, pv-b, and pv-c with 1Mi capacity each. The volumes use GCE Persistent Disks as storage mechanism and are recycled when released by a claim. Additionally, a headless Service called kubia is created to provide network identity for the stateful pods.\"}\n",
      "Done for page number:235\n",
      "{\"summary\": \"A headless Service is created with a selector and port 80, then a StatefulSet manifest is created with a serviceName, replicas of 2, and a container image. The StatefulSet has a volumeClaimTemplates list defining one volume claim template called data, which will be used to create a Persistent-VolumeClaim for each pod.\"}\n",
      "Done for page number:236\n",
      "{\"summary\": \"StatefulSets create replicated applications, adding a volume to each pod's specification automatically and configuring it to be bound to a claim. When creating multiple replicas, pods are brought up one at a time due to sensitivity to race conditions in clustered stateful apps.\"}\n",
      "Done for page number:237\n",
      "{\"summary\": \"A Kubernetes StatefulSet is used to create a PersistentVolumeClaim and its corresponding volume. The PersistentVolumeClaims are listed using kubectl get pvc, confirming they were created. To communicate with pods directly, the API server can be used as a proxy, allowing requests to be sent to individual pods. This method is cumbersome due to security requirements, but can be simplified using kubectl proxy.\"}\n",
      "Done for page number:238\n",
      "{\"summary\": \"You can communicate with a Kubernetes pod by using the `kubectl proxy` command, which allows you to send requests to the pod without dealing with authentication and SSL certificates. You can use tools like `curl` to send GET or POST requests to the pod, and the request will be proxied through the API server. This is done by sending requests to a URL in the form of `localhost:8001/api/v1/namespaces/default/pods/<pod_name>/proxy/`, where `<pod_name>` is the name of the pod you want to communicate with.\"}\n",
      "Done for page number:239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"The data stored on each StatefulSet pod is persisted and can be accessed via GET requests. When a pod is deleted and rescheduled, its state and identity are preserved, but it may be scheduled to a different node in the cluster.\"}\n",
      "Done for page number:240\n",
      "{\"summary\": \"StatefulSets replace deleted pods with identical ones, preserving hostname and persistent data. Scaling down a StatefulSet deletes pods but leaves PersistentVolumeClaims intact, scaling up recreates pods gradually, deleting highest ordinal numbers first. A non-headless Service can be used to expose stateful pods, retaining their name, hostname, and storage even when rescheduled.\"}\n",
      "Done for page number:241\n",
      "{\"summary\": \"You can access a ClusterIP Service from inside the cluster using the API server's proxy feature, or by running a pod that uses the service. Peer discovery in a StatefulSet is also necessary, where each member needs to find all other members, without relying on the API server. This allows applications to be Kubernetes-agnostic.\"}\n",
      "Done for page number:242\n",
      "{\"summary\": \"Kubernetes uses SRV records to point to the hostnames of pods backing a headless service, allowing a pod to discover its peers through an SRV DNS lookup. The SRV record is used to list all other pods of a StatefulSet, and can be performed using the dig DNS lookup tool or in Node.js.\"}\n",
      "Done for page number:243\n",
      "{\"summary\": \"This page describes discovering peers in a StatefulSet by performing a DNS lookup using SRV records. The goal is to have each node in the cluster respond with data from all other nodes. The example code shows how to use Node.js and the dns module to perform this lookup, resolving the SRV records for the kubia.default.svc.cluster.local service and retrieving the addresses of peer nodes. If peers are found, it returns the stored data from each node in the cluster.\"}\n",
      "Done for page number:244\n",
      "{\"summary\": \"A StatefulSet is used to deploy replicated stateful applications, such as a distributed data store that stores data on each pod pointed to by an SRV record. The pods are contacted to get their data and return a collated list. To update a StatefulSet, the pod template can be updated to use a new image, and the replica count can also be set to 3.\"}\n",
      "Done for page number:245\n",
      "{\"summary\": \"To discover peers in a StatefulSet, update the set using `kubectl edit` and modify the spec.replicas to 3 and image attribute. New replicas will be created, but existing ones won't be updated automatically. Delete them manually for the StatefulSet to bring them up again based on the new template.\"}\n",
      "Done for page number:246\n",
      "{\"summary\": \"StatefulSets deploy replicated stateful applications, maintaining data across cluster nodes. When a node fails, Kubernetes can't know its state, and StatefulSet creates replacement pods only when told by the admin that the pod is no longer running. Simulating a node's disconnection from the network shows how StatefulSets handle this scenario.\"}\n",
      "Done for page number:247\n",
      "{\"summary\": \"When a Kubernetes node's network interface is shut down, the control plane marks it as NotReady. The status of all pods on that node becomes Unknown. If the status remains unknown for more than a few minutes, the pod is automatically evicted and deleted by the master.\"}\n",
      "Done for page number:248\n",
      "{\"summary\": \"A StatefulSet's pod is shown as Terminating due to a node being unresponsive, but in reality, the pod's container is still running fine. Deleting the pod manually doesn't immediately create a replacement, and the same pod remains even after deletion.\"}\n",
      "Done for page number:249\n",
      "{\"summary\": \"When a pod is marked for deletion, it may not be immediately deleted if the node's network is down. To forcibly delete the pod, use `kubectl delete po --force --grace-period 0`. This will delete the pod without waiting for confirmation from the Kubelet. Note that this should only be done when the node is no longer running or unreachable.\"}\n",
      "Done for page number:250\n",
      "{\"summary\": \"Kubernetes StatefulSets allow replicated stateful applications to connect with other members through host names, and can forcibly delete stateful pods, enabling deployment management for apps by controlling cluster components.\"}\n",
      "Done for page number:251\n",
      "{\"summary\": \"This chapter delves into the internal workings of Kubernetes, explaining how a cluster is composed of various components and how they function. It covers topics such as pod scheduling, controller managers, Deployment objects, network communication between pods, Services, and achieving high-availability.\"}\n",
      "Done for page number:252\n",
      "{\"summary\": \"A Kubernetes cluster consists of a Control Plane, which includes etcd, API server, Scheduler, and Controller Manager, responsible for storing and managing the state of the cluster. Worker nodes run Kubelet, Kubernetes Service Proxy (kube-proxy), and Container Runtime (Docker, rkt, or others) to execute application containers. Add-on components like DNS server, Dashboard, Ingress controller, Heapster, and Container Network Interface network plugin are required for the cluster to function.\"}\n",
      "Done for page number:253\n",
      "{\"summary\": \"The Kubernetes system consists of several components that communicate through the API server, which is the only component to connect with etcd directly. The other components can be run individually on a single node or split across multiple servers. The Control Plane's status can be checked using kubectl get componentstatuses.\"}\n",
      "Done for page number:254\n",
      "{\"summary\": \"Kubernetes Control Plane components can have multiple instances for high availability. The Kubelet runs all components as pods, with the exception of itself which always runs directly on the system. Control Plane components like etcd, API server, and Scheduler run on the master node, while worker nodes run kube-proxy and Flannel networking pods.\"}\n",
      "Done for page number:255\n",
      "{\"summary\": \"Kubernetes stores its cluster state and metadata in a fast, distributed, and consistent key-value store called etcd. Etcd is accessed directly only by the Kubernetes API server, which brings benefits such as optimistic locking and validation. All resources are stored under /registry with keys such as configmaps, daemonsets, deployments, etc. Optimistic concurrency control is used through version numbers to prevent concurrent updates. Clients must pass back metadata.resourceVersion field when updating an object.\"}\n",
      "Done for page number:256\n",
      "{\"summary\": \"Kubernetes stores resources as JSON files in a filesystem-like structure within etcd, with each resource having a unique key. The API server saves the complete JSON representation of a resource, but prior to v1.7, Secret resources were stored unencrypted and access to etcd could reveal sensitive information. Ensuring consistency and validity across multiple Control Plane components accessing the store is crucial for maintaining cluster integrity.\"}\n",
      "Done for page number:257\n",
      "{\"summary\": \"Kubernetes ensures consistency by requiring all Control Plane components to go through the API server, implementing optimistic locking in a single place. For an etcd cluster with multiple instances, RAFT consensus algorithm is used to achieve consistency, where clients see the actual current state or one of the past states.\"}\n",
      "Done for page number:258\n",
      "{\"summary\": \"Having an odd number of etcd instances is recommended as it provides a majority in case one instance fails, reducing the chance of cluster failure. The Kubernetes API server is the central component that stores state in etcd and performs validation, optimistic locking, and handles requests from clients like kubectl.\"}\n",
      "Done for page number:259\n",
      "{\"summary\": \"The API server authenticates clients using authentication plugins, then authorizes them based on configured authorization plugins. If a request creates, modifies, or deletes a resource, Admission Control plugins validate and/or modify the resource before allowing it to proceed. Notable examples of Admission Control plugins include AlwaysPullImages, ServiceAccount, NamespaceLifecycle, and ResourceQuota, which enforce policies such as image pulling, service account assignment, namespace deletion prevention, and resource quota enforcement.\"}\n",
      "Done for page number:260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"The Kubernetes API server validates and stores resources persistently in etcd after passing through Admission Control plugins. It doesn't create or manage resources but notifies clients of changes, enabling them to perform tasks based on cluster metadata updates. Clients watch for changes by connecting to the API server, receiving a stream of modifications, and being notified of updated objects.\"}\n",
      "Done for page number:261\n",
      "{\"summary\": \"Kubernetes' default Scheduler selects a node for a pod by first filtering out unsuitable nodes and then prioritizing the remaining ones using a combination of algorithms. The process involves updating the pod definition through the API server, which notifies the Kubelet to create and run the pod's containers. While the simplest Scheduler could pick a random node or use advanced techniques like machine learning to anticipate future hardware utilization, Kubernetes' default algorithm falls somewhere in between.\"}\n",
      "Done for page number:262\n",
      "{\"summary\": \"To determine which nodes are acceptable for a pod in Kubernetes, the Scheduler checks various conditions through predicate functions, such as resource availability, specific node selection, label matching, volume allocation, taint toleration, and affinity rules. After passing these checks, the Scheduler selects the best node by prioritizing those with available resources and considering factors like pod load on other nodes.\"}\n",
      "Done for page number:263\n",
      "{\"summary\": \"Kubernetes scheduling can be complex due to various factors and dependencies. The Scheduler can be configured or replaced with a custom implementation. Multiple schedulers can be run in the cluster, specifying which scheduler should schedule each pod by setting the schedulerName property. Controllers running in the Controller Manager ensure the system converges toward its desired state, performing tasks such as reconciliation and node assignment.\"}\n",
      "Done for page number:264\n",
      "{\"summary\": \"Kubernetes controllers watch the API server for changes to resources and perform operations such as creating or updating resources. They run a reconciliation loop to reconcile actual state with desired state, using watches and periodic re-lists to ensure accuracy. Each controller connects to the API server independently and does not communicate directly with other controllers.\"}\n",
      "Done for page number:265\n",
      "{\"summary\": \"The Replication Manager is the controller that makes ReplicationController resources come to life, not doing the actual work but being notified by the watch mechanism of each change affecting the desired replica count or number of matched pods. It creates an Informer to listen for changes and calls a syncHandler function to act accordingly when too few pod instances are running.\"}\n",
      "Done for page number:266\n",
      "{\"summary\": \"Kubernetes controllers manage and run pods through API objects, without running them directly. Key controllers include ReplicaSet, DaemonSet, Job, Deployment, StatefulSet, Node, and Service. Each controller performs specific tasks such as scheduling, scaling, rolling out new versions, managing persistent volumes, monitoring node health, and requesting load balancers.\"}\n",
      "Done for page number:267\n",
      "{\"summary\": \"The Kubernetes control plane components manage various resources through a watch-create-modify-delete process. The Endpoints Controller updates endpoint lists with IPs and ports of matching pods based on service selectors. The Namespace Controller deletes all resources in a namespace when it's deleted, while the PersistentVolume Controller binds claims to available volumes based on capacity and access mode.\"}\n",
      "Done for page number:268\n",
      "{\"summary\": \"Kubernetes controllers operate on API objects through the API server without communicating with Kubelets. The Control Plane handles one part of the system's operation, while the Kubelet runs on worker nodes, responsible for everything running on a node, including monitoring Pods, starting containers, and reporting status to the API server.\"}\n",
      "Done for page number:269\n",
      "{\"summary\": \"Kubernetes Service Proxy, also known as kube-proxy, ensures clients connect to services defined through the Kubernetes API by making connections to service IP and port redirect to pods backing that service. It performs load balancing across multiple pod instances. The initial implementation was a userspace proxy that configured iptables rules to intercept connections and redirect them to the proxy server.\"}\n",
      "Done for page number:270\n",
      "{\"summary\": \"Kubernetes' kube-proxy uses iptables rules to redirect packets to a randomly selected backend pod without passing them through an actual proxy server. This is called the iptables proxy mode, which has a major impact on performance and balances connections across pods in a random fashion rather than true round-robin.\"}\n",
      "Done for page number:271\n",
      "{\"summary\": \"This page describes how DNS and Ingress controllers work in a Kubernetes cluster. The DNS server is a pod that uses the API server's watch mechanism to observe changes to Services and Endpoints, updating its DNS records accordingly. Most Ingress controllers run a reverse proxy server (like Nginx) and keep it configured according to the Ingress resources defined in the cluster. They use the watch mechanism to observe changes to those resources and update the proxy server's config every time one of them changes.\"}\n",
      "Done for page number:272\n",
      "{\n",
      "\"summary\": \"Kubernetes system is composed of small, loosely coupled components with good separation of concerns. The API server, Scheduler, controllers, Kubelet, and kube-proxy work together to synchronize actual state with desired state. When a Pod resource is created, controllers, Scheduler, and Kubelet watch the API server for changes, involving the Controller Manager, Deployment controller, ReplicaSet controller, and etcd in the process.\"\n",
      "}\n",
      "Done for page number:273\n",
      "{\"summary\": \"When a Deployment YAML file is submitted to Kubernetes, the API server validates and stores it in etcd, triggering a chain of events. The Deployment controller creates a ReplicaSet, which then creates pods. Notified clients, including the Deployment controller and ReplicaSet controller, watch for changes and create resources accordingly. This process continues until the deployment is complete.\"}\n",
      "Done for page number:274\n",
      "{\n",
      "  \"summary\": \"The ReplicaSet controller creates Pod resources based on a pod template, then the Scheduler assigns a node to each Pod. The Kubelet runs the Pod's containers using Docker or another container runtime. Control Plane components and Kubelets emit events as they perform actions, which can be retrieved with kubectl get events. Using kubectl get without --watch can display events in incorrect order.\"\n",
      "}\n",
      "Done for page number:275\n",
      "{\"summary\": \"A running pod in Kubernetes is a logical host for one or more application containers and can have multiple containers. When you create a pod with a single container, the Kubelet runs the container within a Docker container, which is then run on the node. You can inspect the running containers by ssh-ing into the worker node and listing the running Docker containers.\"}\n",
      "Done for page number:276\n",
      "{\"summary\": \"The chapter explains Kubernetes internals by ssh-ing into a node and listing running containers with Docker ps, showing an additional 'pause' container that holds all containers of a pod together, sharing network and Linux namespaces.\"}\n",
      "Done for page number:277\n",
      "{\"summary\": \"Kubernetes achieves inter-pod networking by not doing it itself, instead relying on the system administrator or a Container Network Interface (CNI) plugin to set up the network. The network must be NAT-less and allow pods to communicate with each other without IP address translation, enabling applications inside pods to self-register in other pods.\"}\n",
      "Done for page number:278\n",
      "{\"summary\": \"In Kubernetes, pods communicate with each other using IP addresses and network namespaces. Each pod's containers use its own network namespace, which is set up by the infrastructure container (pause container). Virtual Ethernet interfaces (veth pair) are created to connect pods on the same node, allowing them to share a bridge and communicate with each other.\"}\n",
      "Done for page number:279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"The Kubernetes cluster's inter-pod networking is established through a network bridge that assigns IP addresses from its address range. Communication between containers running on different nodes requires connecting the bridges across nodes using overlay or underlay networks or regular layer 3 routing, ensuring non-overlapping address ranges to prevent IP conflicts.\"}\n",
      "Done for page number:280\n",
      "{\"summary\": \"Kubernetes pods communicate through the node's physical adapter, then over the wire to the other node's physical adapter. This only works if nodes are connected to the same network switch without routers in between. Software Defined Network (SDN) can be used to make nodes appear as though they're connected to the same network switch regardless of the underlying network topology. The Container Network Interface (CNI) allows Kubernetes to use any CNI plugin, such as Calico or Flannel, to connect containers into a network. Services are implemented by exposing a set of pods at a long-lived, stable IP address and port.\"}\n",
      "Done for page number:281\n",
      "{\"summary\": \"The kube-proxy process on each node handles everything related to Services by setting up iptables rules that redirect packets destined for the service IP/port pair to one of the pods backing the service. The virtual IP address is assigned immediately upon service creation and modified iptables rules ensure clients can connect to the pods through the Service, allowing a seamless communication between client and server without exposing actual pod IPs.\"}\n",
      "Done for page number:282\n",
      "{\"summary\": \"In Kubernetes, network packets sent to a Service's virtual IP/port pair are modified and redirected to a randomly selected backend pod. The packet is first handled by the node's kernel according to iptables rules, which can replace the destination IP and port with those of a randomly selected pod. This process allows client pods to communicate with a Service without knowing the actual IP addresses of the backend pods.\"}\n",
      "Done for page number:283\n",
      "{\"summary\": \"Running highly available clusters in Kubernetes involves ensuring both apps and control plane components stay up with minimal manual intervention. This can be achieved by running multiple app instances, using a leader-election mechanism to avoid downtime, and configuring the number of replicas for a Deployment resource. Kubernetes takes care of most tasks, but making control plane components highly available requires additional setup.\"}\n",
      "Done for page number:284\n",
      "{\"summary\": \"To make Kubernetes highly available, run multiple master nodes with multiple instances of etcd, API server, Controller Manager, and Scheduler components. etcd can be made highly available by running it on a suitable number of machines (three, five, or seven) and configuring each instance to know about the others. This allows data replication across instances, ensuring fault tolerance even in case of one node failure.\"}\n",
      "Done for page number:285\n",
      "{\"summary\": \"Running multiple instances of etcd, API server, Controller Manager, and Scheduler components in Kubernetes is possible but requires careful consideration to ensure high availability. Multiple etcd instances can be run without load balancers, while API servers need a load balancer in front. Running multiple instances of the Controller Manager or Scheduler is not recommended as it may cause undesired effects. Instead, only one instance is active at a time and controlled by the --leader-elect option to ensure high availability.\"}\n",
      "Done for page number:286\n",
      "{\"summary\": \"Kubernetes' Control Plane components, such as Controller Manager and Scheduler, can run collocated or on separate machines. Leader election is achieved by creating a resource in the API server using an Endpoints object, with the first instance to succeed putting its name into the leader annotation becoming the leader. The leader must periodically update the resource to maintain leadership, and optimistic concurrency ensures only one winner even if multiple instances try to write their name at the same time.\"}\n",
      "Done for page number:287\n",
      "{\"summary\": \"Kubernetes cluster components work together to bring a pod to life. This includes the API server, Scheduler, controllers in the Controller Manager, Kubelet, infrastructure container, network bridge, kube-proxy for load balancing, and highly available control plane.\"}\n",
      "Done for page number:288\n",
      "{\"summary\": \"This chapter covers securing the Kubernetes API server by understanding authentication, what ServiceAccounts are and how to configure their permissions, RBAC plugin, using Roles and RoleBindings, ClusterRoles and ClusterRoleBindings, and the default roles and bindings.\"}\n",
      "Done for page number:289\n",
      "{\"summary\": \"Kubernetes uses authentication plugins to determine who's sending a request, and stops invoking remaining plugins after finding identity info. Available plugins include client certificate, HTTP header token, basic HTTP auth, and others. Authentication plugins are enabled through command-line options. The API server doesn't store user info, but uses it to verify authorization. Users can be actual humans or pods, with users being managed by an external system like SSO, while pods use service accounts created in the cluster as ServiceAccount resources. Both human users and ServiceAccounts can belong to groups used for granting permissions.\"}\n",
      "Done for page number:290\n",
      "{\"summary\": \"ServiceAccounts are identities of apps running in pods, associated with a token used to authenticate with the API server. They are resources scoped to individual namespaces and can be listed using kubectl get sa. The API server passes ServiceAccount usernames to authorization plugins to determine allowed actions.\"}\n",
      "Done for page number:291\n",
      "{\"summary\": \"A ServiceAccount allows control of which resources each pod has access to by assigning different accounts to pods. When a request is received, the API server authenticates the client and determines whether the related ServiceAccount can perform the requested operation. Additional ServiceAccounts can be created in a namespace if necessary for cluster security.\"}\n",
      "Done for page number:292\n",
      "{\"summary\": \"A ServiceAccount can be created with kubectl create serviceaccount and customized tokens can be associated with it. The token is a JSON Web Token (JWT) that can be used for authentication. A pod using this ServiceAccount can only mount specific Secrets if mountable Secrets are enforced, allowing for more secure access to sensitive data.\"}\n",
      "Done for page number:293\n",
      "{\"summary\": \"A ServiceAccount in Kubernetes can contain mountable Secrets and image pull Secrets, which are used for authentication and pulling container images respectively. A ServiceAccount's image pull Secrets add the secret to all pods using the account, saving from having to add them individually. To use a custom ServiceAccount with a pod, set the spec.serviceAccountName field in the pod definition.\"}\n",
      "Done for page number:294\n",
      "{\"summary\": \"A custom ServiceAccount named foo is created and used in a Kubernetes Pod to confirm its token is mounted into containers. The token is verified by printing its contents and comparing it with the default token. The custom ServiceAccount's token is then used to talk to the API server through an ambassador container, which listens on localhost:8001, and returns a proper response from the server.\"}\n",
      "Done for page number:295\n",
      "{\"summary\": \"When using Kubernetes cluster authorization, role-based access control (RBAC) is enabled by default from version 1.8.0 to prevent unauthorized users from viewing or modifying the cluster state. The default ServiceAccount isn't allowed to view or modify cluster state unless additional privileges are granted. Additional authorization plugins include ABAC and custom implementations, but RBAC is the standard.\"}\n",
      "Done for page number:296\n",
      "{\"summary\": \"The Kubernetes API server can be secured using an authorization plugin such as RBAC, which determines whether a client is allowed to perform requested verbs on resources based on user roles. A subject (user or ServiceAccount) is associated with one or more roles, and each role allows certain verbs on specific resources. If a user has multiple roles, they may do anything that any of their roles allow. Authorization is managed by creating four RBAC-specific Kubernetes resources.\"}\n",
      "Done for page number:297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"RBAC authorization rules are configured through four resources: Roles and ClusterRoles specifying allowed verbs on resources, and RoleBindings and ClusterRoleBindings binding roles to users, groups or ServiceAccounts. Roles define what can be done, while bindings define who can do it. There is a distinction between namespaced (e.g. Roles) and cluster-level (e.g. ClusterRoles) resources.\"}\n",
      "Done for page number:298\n",
      "{\"summary\": \"To secure the Kubernetes API server, ensure RBAC is enabled in your cluster by using at least version 1.6 of Kubernetes and disabling legacy authorization if using GKE 1.6 or 1.7. Then, run a pod to demonstrate per-namespace security behavior by trying to access resources across namespaces, utilizing kubectl exec and curl inside the container, while relying on the proxy for authentication and HTTPS.\"}\n",
      "Done for page number:299\n",
      "{\"summary\": \"Creating two pods in different namespaces (foo and bar) using kubectl and verifying role-based access control (RBAC) by listing services from one pod while the other is prevented due to default ServiceAccount permissions, requiring creation of a Role resource for modifications.\"}\n",
      "Done for page number:300\n",
      "{\"summary\": \"A Kubernetes Role resource defines actions on resources, and can be namespaced. The `service-reader` Role allows users to get and list Services in the foo namespace by specifying apiGroup \\\"\\\" and resources \\\"services\\\", while excluding other namespaces.\"}\n",
      "Done for page number:301\n",
      "{\"summary\": \"A role-based access control system is used to secure a cluster by creating roles and binding them to service accounts or users. A role defines what actions can be performed, but it doesn't specify who can perform them. Role bindings are created to bind the role to a subject, such as a user, service account, or group. In this case, two roles, 'service-reader', are created in different namespaces and bound to their respective default service accounts.\"}\n",
      "Done for page number:302\n",
      "{\n",
      "  \"summary\": \"A RoleBinding is created that references a single Role and binds it to multiple subjects, such as a ServiceAccount. In this case, the default ServiceAccount in the foo namespace is bound to the service-reader Role, allowing it to get and list services from within the pod.\"\n",
      "}\n",
      "Done for page number:303\n",
      "{\"summary\": \"You can secure a Kubernetes cluster with role-based access control (RBAC) by creating a RoleBinding that references a service-reader Role and binds it to multiple ServiceAccounts from different namespaces, allowing them to list Services in a specific namespace.\"}\n",
      "Done for page number:304\n",
      "{\"summary\": \"ClusterRoles and ClusterRoleBindings are cluster-level RBAC resources that allow access to non-namespaced resources or non-resource URLs. Unlike regular Roles which are namespaced, ClusterRoles can be used to grant access to resources across different namespaces or at the cluster level. A ClusterRole can be created using `kubectl create clusterrole` command, and it can be bound to a ServiceAccount using `ClusterRoleBinding`.\"}\n",
      "Done for page number:305\n",
      "{\"summary\": \"To secure a Kubernetes cluster with role-based access control, create a ClusterRole and bind it to a ServiceAccount using a RoleBinding. Verify that the pod can list PersistentVolumes before binding the ClusterRole. A RoleBinding referencing a ClusterRole has exactly the same rules as a regular Role.\"}\n",
      "Done for page number:306\n",
      "{\"summary\": \"To grant access to cluster-level resources, use a ClusterRoleBinding instead of RoleBinding. Create a ClusterRoleBinding by deleting the RoleBinding first and then running $ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader --serviceaccount=foo:default. This will allow access to PersistentVolumes as shown in the example output of curl localhost:8001/api/v1/persistentvolumes.\"}\n",
      "Done for page number:307\n",
      "{\"summary\": \"Securing a Kubernetes cluster requires using a ClusterRole and ClusterRoleBinding to grant access to cluster-level resources. Non-resource URLs exposed by the API server also require explicit access grants, which can be achieved through predefined ClusterRoles like system:discovery and its corresponding binding.\"}\n",
      "Done for page number:308\n",
      "{\"summary\": \"The system:discovery ClusterRole allows absolute access to listed URLs, with only HTTP GET method allowed. It's bound to all users through a ClusterRoleBinding referencing the ClusterRole, making it accessible from inside a pod authenticated as ServiceAccount.\"}\n",
      "Done for page number:309\n",
      "{\"summary\": \"You've used ClusterRoles and ClusterRoleBindings to grant access to cluster-level resources. Now, you're exploring how ClusterRoles can be bound with namespaced RoleBindings to grant access to specific namespace resources. The 'view' ClusterRole allows reading, but not writing, of certain namespaced resources.\"}\n",
      "Done for page number:310\n",
      "{\"summary\": \"This chapter explains how to secure the Kubernetes API server by creating a ClusterRoleBinding or RoleBinding. A ClusterRoleBinding allows subjects listed in the binding to view resources across all namespaces, while a RoleBinding only grants access within a specific namespace. An example demonstrates how a pod can list pods in different namespaces and across all namespaces after creating a ClusterRoleBinding.\"}\n",
      "Done for page number:311\n",
      "{\"summary\": \"Securing a Kubernetes cluster with role-based access control allows a pod to access namespaced resources in any namespace by combining a ClusterRoleBinding with a ClusterRole. Alternatively, a RoleBinding can be used, but it must be created within a specific namespace and binds the default ServiceAccount with the view ClusterRole, allowing access to pods in that namespace and others.\"}\n",
      "Done for page number:312\n",
      "{\"summary\": \"Securing the Kubernetes API server involves understanding role and binding types, including ClusterRole, ClusterRoleBinding, Role, and RoleBinding combinations for accessing cluster-level resources, non-resource URLs, and namespaced resources in specific or all namespaces. The default ServiceAccount in a namespace can only view pods within that namespace, even with a ClusterRole.\"}\n",
      "Done for page number:313\n",
      "{\"summary\": \"Kubernetes has a default set of ClusterRoles and ClusterRoleBindings that are updated every time the API server starts, ensuring recreated roles and bindings if deleted or modified in newer versions. These can be viewed with commands $ kubectl get clusterrolebindings and $ kubectl get clusterroles.\"}\n",
      "Done for page number:314\n",
      "{\"summary\": \"The Kubernetes API server has several important ClusterRoles: view, edit, admin, and cluster-admin. These roles are meant to be bound to ServiceAccounts used by user-defined pods. The view role allows read-only access, the edit role allows modifying resources but not viewing or modifying Roles or RoleBindings, the admin role grants complete control of a namespace, and the cluster-admin role gives complete control of the Kubernetes cluster.\"}\n",
      "Done for page number:315\n",
      "{\"summary\": \"The Controller Manager runs as a single pod, but each controller can use a separate ClusterRole and ClusterRoleBinding. It's best to give ServiceAccounts only the permissions they need (principle of least privilege). Creating specific ServiceAccounts for each pod and associating them with tailored Roles through RoleBindings is a good idea. This helps reduce the risk of an intruder causing damage, as it constrains the ServiceAccount's capabilities even if its authentication token is compromised.\"}\n",
      "Done for page number:316\n",
      "{\"summary\": \"Kubernetes API server is secured through ServiceAccounts, which can be created manually or automatically for each namespace. ServiceAccounts can be configured to allow mounting specific Secrets in a pod, and Roles and ClusterRoles define actions on resources, bound to users, groups, and ServiceAccounts by RoleBindings and ClusterRoleBindings.\"}\n",
      "Done for page number:317\n",
      "{\"summary\": \"To prevent attackers from causing harm if they gain access to the API server, securing cluster nodes and network is crucial. This involves allowing pods to access node resources while configuring the cluster to limit user actions with their pods. Specific techniques include using Linux namespaces in pods, running containers as different users, and defining security policies to restrict pod capabilities.\"}\n",
      "Done for page number:318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"In Kubernetes, certain pods can operate in the host's default namespaces to access and manipulate node-level resources and devices by setting the hostNetwork property to true. This allows the pod to use the node's network interfaces instead of having its own, and any process binding to a port will be bound to the node's port.\"}\n",
      "Done for page number:319\n",
      "{\"summary\": \"A Kubernetes pod can use the host's network namespace by setting `hostNetwork: true` in its spec, allowing it to see all the host's network adapters. Alternatively, pods can bind to a port on the node's default namespace using `hostPort` without sharing the host's network namespace, which is different from exposing a NodePort service.\"}\n",
      "Done for page number:320\n",
      "{\"summary\": \"When using a specific host port in Kubernetes, only one pod instance can be scheduled to each node, as two processes can't bind to the same host port. The Scheduler takes this into account and doesn't schedule multiple pods to the same node. This limits the deployment of replicas, with three nodes able to handle at most three pod replicas when four are desired.\"}\n",
      "Done for page number:321\n",
      "{\"summary\": \"A Kubernetes pod can be defined to use its host node's namespaces by setting `hostPort`, `hostPID`, and `hostIPC` properties to true in the YAML definition. This allows processes running in containers to see or communicate with all other processes on the node, depending on the option used. The `hostPort` feature is primarily used for exposing system services deployed using DaemonSets.\"}\n",
      "Done for page number:322\n",
      "{\"summary\": \"You can configure cluster nodes and network security by setting properties on pods and containers, such as hostIPC=true for inter-process communication, or specifying the user ID under which processes run, preventing running as root, configuring privileged mode, fine-grained privileges, SELinux options, and preventing filesystem writes.\"}\n",
      "Done for page number:323\n",
      "{\"summary\": \"Configuring a container's security context involves specifying a user and group ID. By default, containers run as root (user ID 0) unless specified otherwise in the Dockerfile using the USER directive. To run a pod under a different user ID, set the `securityContext.runAsUser` property to the desired user ID. For example, running a pod with `runAsUser: 405` sets it to run as user guest, which has a user ID of 405 in the alpine container image. The `id` command can be used to verify the effective user and group ID within the container.\"}\n",
      "Done for page number:324\n",
      "{\"summary\": \"To prevent a container from running as root, specify the `runAsNonRoot` field in the Pod's security context. This ensures that even if an attacker pushes a different image under the same tag, the pod will not be allowed to run with root privileges, preventing potential attacks.\"}\n",
      "Done for page number:325\n",
      "{\"summary\": \"A Kubernetes pod can be configured to run in privileged mode by setting the 'privileged' property in its security-context to true, allowing full access to the node's kernel and device files.\"}\n",
      "Done for page number:326\n",
      "{\"summary\": \"A Kubernetes container can access all host node devices, granting it unlimited power. To secure this, kernel capabilities can be added or dropped on a per-container basis, limiting the impact of potential security breaches. For example, CAP_SYS_TIME can be added to allow changing the system time.\"}\n",
      "Done for page number:327\n",
      "{\"summary\": \"Configuring a container's security context by adding capabilities, such as SYS_TIME, to run system-level commands. This can be done using Kubernetes and Docker, but requires careful consideration of potential risks, including changing the system time, which may cause worker nodes to become unusable. Dropping capabilities, like CAP_CHOWN, is also possible under the securityContext property.\"}\n",
      "Done for page number:328\n",
      "{\"summary\": \"To secure cluster nodes and network, capabilities can be dropped from containers by listing them under securityContext.capabilities.drop property. Additionally, the container's readonlyRootFilesystem property can be set to true to prevent processes from writing to the filesystem, reducing vulnerability risks.\"}\n",
      "Done for page number:329\n",
      "{\"summary\": \"When setting a container's root filesystem as read-only, it prevents the container from writing files directly. However, this restriction can be bypassed by mounting a volume in the directory where the application writes to. This approach increases security and allows for sharing data between containers running as different users.\"}\n",
      "Done for page number:330\n",
      "{\"summary\": \"Kubernetes allows specifying supplemental groups for pods to share files, regardless of user IDs. This is done using fsGroup and supplementalGroups properties in a pod's security context. An example shows two containers sharing a volume with different user IDs (1111 and 2222), but belonging to the same group ID (555) and supplemental groups (666, 777).\"}\n",
      "Done for page number:331\n",
      "{\"summary\": \"The id command shows a container running with user ID 1111 and group IDs associated with it. Setting fsGroup to 555 in the pod definition mounts a volume owned by group ID 555, whereas creating files uses the user's effective group ID. This is different from how ownership is set up for newly created files. A cluster administrator can restrict users from using security-related features by creating PodSecurityPolicy resources, which define what security-related features users can or can't use in their pods.\"}\n",
      "Done for page number:332\n",
      "{\"summary\": \"The PodSecurityPolicy admission control plugin validates pod definitions against configured policies, rejecting or modifying pods that don't conform. A PodSecurityPolicy resource defines restrictions like IPC, PID, and Network namespaces access, host ports binding, user IDs, and privileged containers creation. Enabling RBAC and PodSecurityPolicy in Minikube requires a specific command with password file creation for basic authentication.\"}\n",
      "Done for page number:333\n",
      "{\"summary\": \"The document explains how to restrict the use of security-related features in pods, including kernel capabilities, SELinux labels, writable root filesystems, filesystem groups, and volume types. A sample PodSecurityPolicy is provided that prevents pods from using host IPC, PID, and network namespaces, running privileged containers, and accessing most host ports, while allowing all volume types to be used.\"}\n",
      "Done for page number:334\n",
      "{\"summary\": \"A Kubernetes cluster node and network security chapter. Privileged pods are no longer allowed after a policy is applied. RunAsUser, fsGroup, and supplementalGroups policies can be set to constrain user IDs using the MustRunAs rule with specified ID ranges.\"}\n",
      "Done for page number:335\n",
      "{\"summary\": \"The PodSecurityPolicy can restrict the use of security-related features in pods, including user IDs. Deploying a pod with a container image having an out-of-range user ID is not rejected by the API server if the user ID is hardcoded into the container image, but it will be overridden to match the allowed range specified in the PodSecurityPolicy.\"}\n",
      "Done for page number:336\n",
      "{\"summary\": \"The chapter discusses securing cluster nodes and network by using the MustRunAsNonRoot rule in the runAsUser field, preventing users from deploying containers that run as root. It also explains configuring allowed, default, and disallowed capabilities in containers by defining fields such as allowedCapabilities, defaultAddCapabilities, and requiredDropCapabilities. The chapter provides examples of PodSecurityPolicy resources and discusses specifying which capabilities can be added to a container, highlighting the importance of fine-grained permission configuration and preventing administrative operations and loading/unloading kernel modules.\"}\n",
      "Done for page number:337\n",
      "{\"summary\": \"A PodSecurityPolicy resource can restrict security-related features and capabilities, including adding or dropping specific capabilities from containers. It can also constrain the types of volumes pods can use, such as emptyDir, configMap, secret, downwardAPI, and persistentVolumeClaim. If multiple policies are in place, pods can use any volume type defined in any policy.\"}\n",
      "Done for page number:338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"PodSecurityPolicies can be assigned to different users and groups through the RBAC mechanism, allowing for flexible policy management. A specific PodSecurityPolicy is created to allow privileged users to deploy pods with privileged containers, granting access to sensitive resources.\"}\n",
      "Done for page number:339\n",
      "{\"summary\": \"Restricting security-related features in pods can be managed by assigning different PodSecurityPolicies to users using Role-Based Access Control (RBAC). This involves creating ClusterRoles for each policy, referencing specific instances of PodSecurityPolicy resources, and binding these policies to users through ClusterRoleBindings. For example, a user like Alice can be restricted to only deploy non-privileged pods while another user like Bob is allowed to deploy privileged pods by assigning the default and privileged PodSecurityPolicies respectively.\"}\n",
      "Done for page number:340\n",
      "{\"summary\": \"To secure cluster nodes and network, a ClusterRole is bound to specific users. Alice is authenticated as default user, while Bob has access to both default and privileged Pod-Security-Policies. The commands create two new kubectl users 'alice' and 'bob', and use the --user option to authenticate for creating pods, demonstrating that only Bob can create privileged pods.\"}\n",
      "Done for page number:341\n",
      "{\"summary\": \"In this section, we explore isolating the pod network by limiting which pods can talk to each other using NetworkPolicy resources. We create a default-deny NetworkPolicy to prevent all clients from connecting to any pod in a given namespace and look at ingress and egress rules with matching options for sources and destinations.\"}\n",
      "Done for page number:342\n",
      "{\"summary\": \"To secure cluster nodes and network, a NetworkPolicy resource must be created in the same namespace as the database pod. This policy allows only specific pods (with app=webserver label) to connect to the database pods on port 5432, while blocking other pods from connecting. The policy is enforced even when connecting through a Service.\"}\n",
      "Done for page number:343\n",
      "{\"summary\": \"To isolate the pod network, a NetworkPolicy resource is created with specific labels. For example, a shopping cart microservice is secured by allowing only pods from namespaces labeled as tenant:manning to access it on port 80.\"}\n",
      "Done for page number:344\n",
      "{\"summary\": \"In Kubernetes, NetworkPolicies can ensure specific pods or namespaces have access to others by using selectors or IP blocks in CIDR notation. This allows for isolation and security in multi-tenant clusters, where tenants cannot add labels or annotations to their namespaces.\"}\n",
      "Done for page number:345\n",
      "{\"summary\": \"NetworkPolicies can limit a pod's outbound traffic by specifying egress rules, restricting access to specific pods or IP addresses. This can be achieved through ClusterRoles and ClusterRoleBindings, ensuring users can't create pods that compromise nodes. Egress rules are demonstrated in the example network-policy-egress.yaml, which restricts webserver pods from accessing any pod other than those with the app=database label.\"}\n",
      "Done for page number:346\n",
      "{\"summary\": \"Managing pod computational resources is crucial in Kubernetes. This chapter covers requesting CPU, memory, and other resources for containers, setting hard limits, understanding Quality of Service guarantees, and limiting resources in a namespace to ensure pods take their fair share and are scheduled efficiently.\"}\n",
      "Done for page number:347\n",
      "{\"summary\": \"When creating a pod, resources can be specified for each container individually, not for the pod as a whole. The pod's resource requests and limits are the sum of the requests and limits of all its containers. A request is specified for CPU and memory, with a limit on what may be consumed.\"}\n",
      "Done for page number:348\n",
      "{\"summary\": \"Specifying resource requests in a pod's specification tells the Scheduler what minimum amount of resources the pod needs, which determines where the pod can be scheduled. The Scheduler doesn't consider actual resource consumption at scheduling time but rather the sum of requested resources by existing pods on the node. This ensures that already deployed pods' guarantees are upheld.\"}\n",
      "Done for page number:349\n",
      "{\"summary\": \"The Scheduler in Kubernetes prioritizes nodes based on the amount of resources requested by a pod's containers, excluding those that can't fit. The LeastRequestedPriority function prefers nodes with fewer requested resources, while MostRequestedPriority prefers nodes with more requested resources. This approach allows for resource savings on cloud infrastructure by keeping pods tightly packed and removing unused nodes.\"}\n",
      "Done for page number:350\n",
      "{\"summary\": \"You can view a node's available computational resources using kubectl describe nodes. The output shows two sets of amounts: capacity (total resources) and allocatable resources (resources available to pods). The Scheduler only bases its decisions on the allocatable resource amounts. You can create a pod with specific resource requests, but if it exceeds the available resources, it won't be scheduled. This is demonstrated by creating three pods with increasing CPU requests, showing that the third pod cannot be scheduled due to insufficient resources.\"}\n",
      "Done for page number:351\n",
      "{\"summary\": \"A Kubernetes pod is stuck in Pending status due to insufficient CPU, even though the total CPU request (2000m) matches the node's capability. The issue arises from a misunderstanding of how CPU requests are calculated and applied to pods, leading to scheduling failure on a single-node cluster.\"}\n",
      "Done for page number:352\n",
      "{\"summary\": \"The chapter discusses managing pods' computational resources in Kubernetes. It highlights how default resource requests and limits are set for various pods, including those in the kube-system namespace. The Scheduler allocates resources based on these requests, and overcommitting can occur if total limits exceed 100%. A pod's CPU requests play a role while it's running, affecting scheduling. Deleting another pod can free up resources to schedule a new pod.\"}\n",
      "Done for page number:353\n",
      "{\"summary\": \"Kubernetes allocates CPU time among pods based on requested resources, not just for scheduling but also for distributing unused CPU time. The ratio of CPU allocation is determined by the pod's resource requests, with a higher request getting more CPU time. Custom resources can also be added and requested in pods, allowing for more flexible resource management.\"}\n",
      "Done for page number:354\n",
      "{\"summary\": \"Kubernetes manages pods' computational resources by adding custom resources to Node object's capacity field. Pods specify requested quantity under resources.requests field or with --requests when using kubectl run. The Scheduler ensures the pod is only deployed to a node with available resources. Resource limits can be set for containers to prevent overuse of CPU and memory, preventing malfunctioning or malicious pods from affecting other nodes.\"}\n",
      "Done for page number:355\n",
      "{\"summary\": \"A Kubernetes pod has its container configured with CPU and memory resource limits, capping process consumption at 1 CPU core and 20 mebibytes. These limits aren't constrained by the node's allocatable resources, allowing them to exceed 100% of the node's capacity. This can lead to certain containers being killed when resources are fully utilized.\"}\n",
      "Done for page number:356\n",
      "{\"summary\": \"When a process running in a container tries to use more resources than allowed, CPU usage is throttled and for memory allocation, the process is killed. If restart policy is set to Always or OnFailure, it's restarted immediately. However, if it keeps exceeding memory limits, Kubernetes increases delays between restarts, resulting in CrashLoopBackOff status. The Kubelet waits 10 seconds, then 20, 40, 80, and finally 300 seconds before restarting the container indefinitely every five minutes until the pod stops crashing or is deleted.\"}\n",
      "Done for page number:357\n",
      "{\"summary\": \"A container's resources are not limited by its own limits but rather by the node it runs on. The top command shows memory usage of the whole node, not just the container, and CPU usage is based on the main process, which can exceed the set limit. This understanding is crucial when setting resource limits for containers.\"}\n",
      "Done for page number:358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"When running containers, they see all node's resources (memory and CPUs) regardless of configured limits. This can cause issues for apps that use system info to decide resource usage. New Java versions alleviate this by considering container limits. Containers don't magically limit CPU cores or memory, but rather get a share of overall time/resources. Certain apps may spin up too many threads on multiple cores, leading to performance/memory issues. The Downward API can be used to pass CPU limits to containers.\"}\n",
      "Done for page number:359\n",
      "{\"summary\": \"Kubernetes categorizes pods into three Quality of Service (QoS) classes: BestEffort (lowest priority), Burstable, and Guaranteed (highest). The QoS class is derived from a pod's resource requests and limits, with Guaranteed assigned to pods whose container requests match their limits for CPU and memory.\"}\n",
      "Done for page number:360\n",
      "{\"summary\": \"A pod's computational resources can be managed by assigning a Quality of Service (QoS) class, which determines the guarantee or limit on resources available to containers within the pod. The three QoS classes are BestEffort, Burstable, and Guaranteed, with Burstable being the default for all other pods. The relationship between requests and limits defines the QoS class, and can be derived from container-level QoS by considering multiple containers and their resource settings.\"}\n",
      "Done for page number:361\n",
      "{\"summary\": \"A pod's QoS class is determined by the classes of its containers, with all BestEffort for same class, and Burstable otherwise. For single-container pods, QoS class depends on requests and limits set in table 14.1. In overcommitted systems, container getting killed is based on their QoS class.\"}\n",
      "Done for page number:362\n",
      "{\"summary\": \"When a system is overcommitted, QoS classes determine which container gets killed first. BestEffort class pods are prioritized for termination followed by Burstable and then Guaranteed pods, with Guaranteed pods only being terminated if system processes need memory. The process to kill is selected based on OutOfMemory (OOM) scores, which consider the percentage of available memory consumed and a fixed OOM score adjustment based on the pod's QoS class and container's requested memory.\"}\n",
      "Done for page number:363\n",
      "{\"summary\": \"To avoid containers being at the mercy of other containers, setting resource requests and limits for each container is a good idea. This can be achieved by creating a LimitRange resource in Kubernetes, which allows specifying minimum and maximum limits, as well as default resource requests for containers that don't specify them explicitly. A LimitRange validates and defaults pods, ensuring they meet the specified requirements.\"}\n",
      "Done for page number:364\n",
      "{\"summary\": \"A LimitRange object is used to validate the computational resources of pods in a Kubernetes cluster. It prevents users from creating pods that are bigger than any node in the cluster by specifying limits for CPU and memory. The limits apply to each individual pod/container, but do not limit the total amount of resources available across all pods in the namespace. A LimitRange resource has multiple properties, including min and max limits for CPU and memory, default requests and limits for containers, and a maxLimitRequestRatio for each resource.\"}\n",
      "Done for page number:365\n",
      "{\"summary\": \"A LimitRange object allows setting default requests and limits for pods, containers, and PersistentVolumeClaims. This includes min, max, and default values for CPU and memory, as well as the maximum ratio of limits vs requests. Existing pods and PVCs are not revalidated if the limits are modified.\"}\n",
      "Done for page number:366\n",
      "{\"summary\": \"A pod's single container is requesting two CPUs which exceeds the maximum set in LimitRange, resulting in a forbidden error. However, when default resource requests and limits are set on containers without specifying them, the defaults are applied automatically when creating the pod, matching the ones specified in the LimitRange object.\"}\n",
      "Done for page number:367\n",
      "{\"summary\": \"ResourceQuotas limit total resources available in a namespace, preventing large pods from being created in certain namespaces. They enforce policies at pod creation time, rejecting excess resource usage. A ResourceQuota object can specify quotas for CPU, memory, and other resources, limiting the number of pods, claims, and API objects users can create.\"}\n",
      "Done for page number:368\n",
      "{\"summary\": \"A ResourceQuota object sets maximum CPU and memory limits for a namespace, with separate totals for requests and limits. It applies to all pod resource requests and limits in total, not individually. The quota usage can be inspected using kubectl describe command, showing the used and hard limits for CPU and memory.\"}\n",
      "Done for page number:369\n",
      "{\"summary\": \"To limit resources in a namespace, create a ResourceQuota alongside a LimitRange. This ensures pods specify resource requests or limits. A quota can also limit persistent storage, specifying amounts for different StorageClasses. Additionally, quotas can be used to limit the number of objects users can create within a namespace.\"}\n",
      "Done for page number:370\n",
      "{\"summary\": \"A ResourceQuota object limits the number of objects that can be created in a namespace, such as pods, replicationcontrollers, secrets, configmaps, persistentvolumeclaims, and services. The quota restricts the maximum number of each type of object that can be created, with some specific types like LoadBalancer Services and NodePort Services having their own quotas. This limits the computational resources available to pods.\"}\n",
      "Done for page number:371\n",
      "{\"summary\": \"Kubernetes quotas can be limited to specific pod states (BestEffort, NotBestEffort, Terminating, NotTerminating) or Quality of Service (QoS) classes. BestEffort and NotBestEffort scopes apply to pods with the BestEffort QoS class or one of the other two classes. The other scopes determine whether a quota applies based on the active deadline seconds set in the pod spec. Quotas can limit resources such as CPU/memory requests and limits, depending on their scope.\"}\n",
      "Done for page number:372\n",
      "{\"summary\": \"Properly setting resource requests and limits for Kubernetes pods is crucial to avoid underutilization or CPU-starvation. Monitoring actual resource usage through cAdvisor and Heapster helps find the sweet spot. Heapster collects metrics from all nodes and exposes it in a single location, aiding in adjusting resource requests and limits as needed.\"}\n",
      "Done for page number:373\n",
      "{\"summary\": \"Heapster is a component that collects pod resource usage data without having to talk to the processes running inside the pods' containers. It can be enabled in Google Kubernetes Engine by default or in other clusters manually. After enabling, it takes a few minutes for Heapster to collect metrics before displaying CPU and memory usage statistics for nodes and individual pods using the kubectl top command.\"}\n",
      "Done for page number:374\n",
      "{\"summary\": \"The kubectl top command shows current resource usages for pods, but not historical data. To analyze resource consumption over longer periods, use tools like InfluxDB and Grafana, which store statistics data and provide visualization to discover how resource usage behaves over time.\"}\n",
      "Done for page number:375\n",
      "{\"summary\": \"InfluxDB and Grafana can be deployed as pods in a Kubernetes cluster using manifests from the Heapster Git repository or by enabling the Heapster add-on with Minikube. Resource usage of pods can be analyzed over time by exploring predefined dashboards on Grafana's web console, accessible via kubectl cluster-info.\"}\n",
      "Done for page number:376\n",
      "{\"summary\": \"When using Minikube, Grafana's web console can be accessed through a NodePort Service, allowing users to view resource usage statistics of nodes and individual pods in real-time. Charts show actual usage, requests, and limits for CPU, memory, network, and filesystem resources. Users can zoom out to see data for longer time periods and use the information to adjust pod resource requests accordingly.\"}\n",
      "Done for page number:377\n",
      "{\"summary\": \"Specifying resource requests in Kubernetes helps schedule pods across clusters, while specifying limits prevents starving other pods of resources. Unused CPU time is allocated based on containers' CPU requests, and excess memory can be freed to prioritize more important pods.\"}\n",
      "Done for page number:378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"LimitRange objects define resource requests and limits for individual pods, while ResourceQuota objects limit resources available to all pods in a namespace. Monitoring pod resource usage over time helps set optimal requests and limits, with metrics used for automatic scaling in the next chapter.\"}\n",
      "Done for page number:379\n",
      "{\"summary\": \"This chapter covers configuring automatic horizontal scaling of pods based on CPU utilization and custom metrics, understanding why vertical scaling of pods isn't possible yet, and understanding automatic horizontal scaling of cluster nodes, making it ideal for handling sudden and unpredictable traffic increases.\"}\n",
      "Done for page number:380\n",
      "{\"summary\": \"Kubernetes can monitor pods and scale them up automatically based on CPU usage or other metrics, even spinning up additional nodes if needed. The autoscaling feature was rewritten between 1.6 and 1.7 versions, so be aware of outdated information online. Horizontal pod autoscaling involves creating a HorizontalPodAutoscaler resource to periodically check pod metrics, calculate the required number of replicas, and adjust accordingly.\"}\n",
      "Done for page number:381\n",
      "{\"summary\": \"Horizontal pod autoscaling should already be enabled in your cluster, enabling Heapster add-on if not. The Autoscaler calculates the required number of replicas by summing up pod metrics, dividing by target value, and rounding up to the next-larger integer, taking into account metric instability and considering multiple metrics when scaling.\"}\n",
      "Done for page number:382\n",
      "{\"summary\": \"The final step of an autoscaling operation is updating the desired replica count field on the scaled resource object (e.g., ReplicaSet) and letting the ReplicaSet controller manage the creation or deletion of additional pods. The Autoscaler controller modifies the replicas field through the Scale sub-resource, enabling it to operate on scalable resources like Deployments, ReplicaSets, ReplicationControllers, and StatefulSets.\"}\n",
      "Done for page number:383\n",
      "{\"summary\": \"The horizontal pod autoscaler (HPA) process involves collecting metrics from pods, cAdvisor, Heapster, and other components. Metrics data flows from pods to cAdvisor, then to Heapster, and finally to HPA. Autoscaling is based on CPU utilization, with the HPA adjusting replicas when demand reaches 100% CPU usage. The autoscaling process takes time for metrics data to propagate and rescaling actions to be performed.\"}\n",
      "Done for page number:384\n",
      "{\"summary\": \"The chapter discusses automatic scaling of pods and cluster nodes, focusing on scaling out by increasing the number of pods when average CPU usage reaches 80%. The Autoscaler considers only the pod's guaranteed CPU amount (CPU requests) to determine CPU utilization. A HorizontalPodAutoscaler is created based on CPU usage by setting a CPU resource request in the Deployment's pod template, allowing for autoscaling of pods.\"}\n",
      "Done for page number:385\n",
      "{\n",
      "    \"summary\": \"A Horizontal Pod Autoscaling (HPA) object is created to scale a Deployment's pods based on their CPU utilization. The HPA sets the target CPU utilization to 30% and specifies the minimum and maximum number of replicas (1-5). This ensures the desired replica count is preserved across application updates.\"\n",
      "}\n",
      "Done for page number:386\n",
      "{\"summary\": \"This chapter covers automatic scaling of pods and cluster nodes using Kubernetes Horizontal Pod Autoscaling (HPA). It explains how to scale down a deployment when CPU utilization is low, using the `kubectl get` command to monitor the HPA resource. The autoscaler adjusts the desired replica count on the Deployment, which then updates the ReplicaSet object and causes excess pods to be deleted.\"}\n",
      "Done for page number:387\n",
      "{\"summary\": \"The Horizontal Pod Autoscaler successfully rescaled to one replica because all metrics were below target. To trigger a scale-up, send requests to the pod, increasing its CPU usage, and expose the pods through a Service using kubectl expose. Monitor the HorizontalPodAutoscaler and Deployment with kubectl get --watch.\"}\n",
      "Done for page number:388\n",
      "{\"summary\": \"This chapter explains how to run a pod that repeatedly hits the kubia Service using kubectl run with options such as -it, --rm, and --restart=Never. The load-generator pod is used to demonstrate automatic scaling of pods and cluster nodes. As the load increases, the autoscaler increases the number of replicas, and CPU utilization on individual pods decreases and stabilizes. The chapter also discusses how the autoscaler determines the number of replicas needed based on target CPU utilization percentage.\"}\n",
      "Done for page number:389\n",
      "{\n",
      "  \"summary\": \"Horizontal pod autoscaling (HPA) has a maximum rate of scaling, which is limited by how many replicas can be added in a single operation and how soon subsequent scale-up operations can occur. The initial CPU usage can spike higher than the target value, requiring additional replicas to achieve the desired level. Modifying the target metric value on an existing HPA object involves editing the resource with kubectl edit, changing the targetAverageUtilization field, and then applying the changes, which will be detected by the autoscaler controller and acted upon.\"\n",
      "}\n",
      "Done for page number:390\n",
      "{\"summary\": \"Memory-based autoscaling in Kubernetes is problematic due to old pods needing to release memory, which can lead to infinite scaling. Memory-based autoscaling was introduced in version 1.8 and is configured like CPU-based autoscaling. Custom metrics-based autoscaling is also available but was initially complicated, prompting a redesign of the autoscaler.\"}\n",
      "Done for page number:391\n",
      "{\"summary\": \"Kubernetes Horizontal Pod Autoscaling (HPA) allows defining multiple metrics for scaling decisions. Three types of metrics are available: Resource, Pods, and Object. The Resource type scales based on resource requests, while the Pods type refers to custom pod metrics like QPS. The Object metric type scales pods based on a non-pod metric, such as an Ingress object's QPS. HPA can target specific values for each metric type.\"}\n",
      "Done for page number:392\n",
      "{\"summary\": \"Automatic scaling of pods and cluster nodes involves specifying a target object and value. The horizontal pod autoscaler (HPA) monitors metrics such as latencyMillis or Queries per Second (QPS), and scales resources based on these values. Not all metrics are suitable for autoscaling, and custom metrics should be chosen to ensure linear behavior when scaling up or down. Currently, the HPA does not allow scaling down to zero replicas.\"}\n",
      "Done for page number:393\n",
      "{\n",
      "  \"summary\": \"Kubernetes does not currently support idling and un-idling of pods, nor vertical pod autoscaling, but an experimental feature called InitialResources sets CPU and memory requests for newly created pods based on historical resource usage data, and a new proposal is being finalized to modify existing pod's resource requests while it's running.\"\n",
      "}\n",
      "Done for page number:394\n",
      "{\"summary\": \"The Cluster Autoscaler automatically requests additional nodes from the cloud provider when the need arises, ensuring that pods can be scheduled even when all existing nodes are at capacity. It de-provisions underutilized nodes over time. The Autoscaler examines available node groups to determine the best type of node to add, considering factors such as node size and features.\"}\n",
      "Done for page number:395\n",
      "{\"summary\": \"The Kubernetes Cluster Autoscaler scales the number of cluster nodes up or down based on resource utilization and node requirements, ensuring efficient use of resources and smooth service operation even in case of node shutdowns.\"}\n",
      "Done for page number:396\n",
      "{\"summary\": \"Cluster autoscaling is available on GKE, GCE, AWS, and Azure. To enable, use `gcloud container clusters update` for GKE or set environment variables for GCE. The Cluster Autoscaler publishes its status to a ConfigMap in the kube-system namespace. Limiting service disruption during scale-down can be achieved by manually cordoning and draining nodes using kubectl commands.\"}\n",
      "Done for page number:397\n",
      "{\"summary\": \"Kubernetes provides a way to specify the minimum number of pods to always keep running with a PodDisruptionBudget resource, especially for quorum-based clustered applications. This resource contains only a pod label selector and a number specifying the minimum or maximum number of pods that must be available. For example, creating a PDB resource like kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3 ensures three instances of a pod with the label app=kubia are always running. Starting from Kubernetes 1.7, the maxUnavailable field also supports blocking evictions when more than that many pods are unavailable.\"}\n",
      "Done for page number:398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Kubernetes can scale not only pods but also cluster nodes. Horizontal Pod Autoscaler (HPA) can scale based on CPU utilization or custom metrics. Cluster nodes can be scaled automatically with a supported cloud provider. Additionally, kubectl run allows running one-off processes in a pod that will stop and delete when finished.\"}\n",
      "Done for page number:399\n",
      "{\"summary\": \"Kubernetes allows for advanced scheduling by specifying a node selector, and additional mechanisms such as taints and tolerations, node affinity rules, co-locating pods with pod affinity, and keeping pods away from each other with pod anti-affinity.\"}\n",
      "Done for page number:400\n",
      "{\"summary\": \"A pod can be scheduled to a node if it tolerates the node's taints, which is different from using node selectors and node affinity. Taints allow rejecting deployment of pods to certain nodes without modifying existing pods. To use a tainted node, pods need to opt in by tolerating the taint. The best way to learn about node taints is to see an existing taint, such as the master node in a kubeadm cluster which is tainted and only Control Plane pods can be deployed on it.\"}\n",
      "Done for page number:401\n",
      "{\"summary\": \"In Kubernetes, a node's taint can prevent pods from being scheduled on it. A pod can have a toleration that matches a node's taint, allowing it to be scheduled on that node even if it has a taint. This is demonstrated by the kube-proxy pod running on the master node with a toleration matching the master node's taint.\"}\n",
      "Done for page number:402\n",
      "{\"summary\": \"In Kubernetes, taints and tolerations are used for advanced scheduling. Taints have effects like NoSchedule, PreferNoSchedule, and NoExecute. To prevent non-production pods from running on production nodes, add a taint to the node with kubectl taint command. Pods must tolerate the taint to run on tainted nodes.\"}\n",
      "Done for page number:403\n",
      "{\"summary\": \"You can use taints and tolerations in Kubernetes to repel pods from certain nodes, by applying a taint to non-production nodes and a matching toleration to non-production pods. This can be used during scheduling to prevent new pods from being scheduled on those nodes or to evict existing pods. Taints can have multiple effects, such as NoSchedule, PreferNoSchedule, or NoExecute, and tolerations can tolerate specific values or any value for a taint key using the Equal or Exists operator.\"}\n",
      "Done for page number:404\n",
      "{\"summary\": \"Kubernetes allows you to specify how long a pod is tolerated on an unready or unreachable node using tolerations. The default delay is 5 minutes but can be shortened by adding specific tolerations to the pod's spec. Node affinity is also introduced, allowing pods to be scheduled only on specific subsets of nodes, compared to node selectors which are simpler but limited in functionality.\"}\n",
      "Done for page number:405\n",
      "{\"summary\": \"Node affinity allows pods to define their own rules for which nodes they can be scheduled on, either as hard requirements or preferences. This is similar to node selectors but will eventually replace them. Node labels, such as region, zone, and hostname, are used in node affinity rules to select specific nodes. Hard node affinity rules require a pod to run on a specific set of nodes, while soft preferences allow Kubernetes to choose from multiple options if the preferred nodes are not available.\"}\n",
      "Done for page number:406\n",
      "{\"summary\": \"The document explains how to schedule a pod in Kubernetes, specifically on nodes with GPU capabilities. It compares and contrasts using nodeSelector vs nodeAffinity rules, highlighting their expressiveness and impact on scheduling. The nodeAffinity rule is shown to be more complex but more flexible, allowing for specific label matching and ignoring existing pods during execution.\"}\n",
      "Done for page number:407\n",
      "{\"summary\": \"Node affinity in Kubernetes allows pods to be scheduled on nodes with specific labels, prioritizing certain nodes over others. The nodeSelectorTerms field defines which expressions a node's labels must match for a pod to be scheduled. The preferredDuringSchedulingIgnoredDuringExecution field is used to specify preferred nodes during scheduling, useful in scenarios like deploying pods across multiple datacenters.\"}\n",
      "Done for page number:408\n",
      "{\"summary\": \"Node affinity allows scheduling to alternate zones and machines if primary ones are insufficient. Nodes must be labeled as dedicated or shared, and by availability zone. A deployment can then specify preferential rules for node selection, such as prioritizing dedicated nodes in a specific zone.\"}\n",
      "Done for page number:409\n",
      "{\"summary\": \"Node affinity is used to attract pods to certain nodes with preferred labels, rather than a hard requirement. The preference can be weighted to indicate importance, with the first rule having a weight of 80 and the second one 20, indicating that scheduling to zone1 is more important than dedicated nodes.\"}\n",
      "Done for page number:410\n",
      "{\"summary\": \"In a two-node Kubernetes cluster, deploying a Deployment will show most pods deployed to one node. This is due to the Scheduler's prioritization functions, such as Selector-SpreadPriority, which spread pods from the same ReplicaSet or Service across nodes for redundancy. Scaling up the Deployment shows majority of pods scheduled to one node. Pod affinity can be used to co-locate pods and reduce latency by scheduling them on the same node.\"}\n",
      "Done for page number:411\n",
      "{\n",
      "  \"summary\": \"A Deployment can be configured with pod affinity and anti-affinity rules, allowing pods to be co-located based on labels and topology keys. In this example, frontend pods are required to run on the same node as backend pods, ensuring they share resources and reducing communication overhead.\"\n",
      "}\n",
      "Done for page number:412\n",
      "{\"summary\": \"Advanced scheduling in Kubernetes involves using pod affinity rules to schedule pods on specific nodes based on label selectors or expressions. The Scheduler takes into account other pods' pod affinity rules, ensuring that even if a dependent pod is deleted and rescheduled, its affinity rules are not broken.\"}\n",
      "Done for page number:413\n",
      "{\"summary\": \"Pods can be co-located using pod affinity and anti-affinity with topology keys such as zone, region, or custom labels like rack. The scheduler checks the pod's config, finds matching pods, and selects nodes based on their labels, allowing for scheduling in the same availability zone, geographic region, or server rack.\"}\n",
      "Done for page number:414\n",
      "{\"summary\": \"Advanced scheduling in Kubernetes involves using label selectors and affinity rules to prefer or require pods to be scheduled on specific nodes based on labels such as rack or namespace. Pod affinity can express hard requirements or preferences, instructing the Scheduler to schedule pods accordingly.\"}\n",
      "Done for page number:415\n",
      "{\"summary\": \"Kubernetes pod affinity allows you to specify a preference for co-locating pods based on labels and topology keys. A weight and podAffinityTerm must be specified, similar to node affinity rules. The Scheduler will prefer nodes where the specified label is running, but may schedule pods to other nodes as well.\"}\n",
      "Done for page number:416\n",
      "{\"summary\": \"Pod anti-affinity is specified the same way as pod affinity, but uses the podAntiAffinity property to keep pods away from each other. This can be useful for preventing performance interference between two sets of pods running on the same node or spreading pods across different availability zones/regions.\"}\n",
      "Done for page number:417\n",
      "{\"summary\": \"Co-locating pods with pod affinity and anti-affinity using anti-affinity to spread apart pods of the same deployment. This can be achieved by configuring podAntiAffinity in a Deployment, making the labelSelector match the same pods that the Deployment creates. This will schedule the pods to different nodes, as shown in the example listings, where only two out of five frontend pods were scheduled, with the remaining three pending due to scheduling restrictions.\"}\n",
      "Done for page number:418\n",
      "{\"summary\": \"This chapter on advanced scheduling in Kubernetes explains how to ensure pods aren't scheduled to certain nodes or are only scheduled to specific nodes using taints, node affinity, pod affinity, and anti-affinity. Taints can completely prevent scheduling or specify a preference, while node affinity allows for hard requirements or preferences. Pod affinity deploys pods to the same node as another running pod, and topologyKey specifies proximity. Both can be used with hard requirements or preferences, and are useful for keeping certain pods away from each other.\"}\n",
      "Done for page number:419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"This chapter covers best practices for developing apps on Kubernetes, including understanding which resources appear in a typical application, adding lifecycle hooks, properly terminating an app without breaking client requests, making apps easy to manage, using init containers, and developing locally with Minikube.\"}\n",
      "Done for page number:420\n",
      "{\"summary\": \"A typical application manifest contains one or more Deployment and/or StatefulSet objects, including pod templates with containers, liveness and readiness probes, and services exposed through LoadBalancer, NodePort, or Ingress resources. The pod templates reference Secrets for container image pulling and internal process usage, which are usually assigned to ServiceAccounts and created by the operations team.\"}\n",
      "Done for page number:421\n",
      "{\"summary\": \"Pods are the most important Kubernetes resource, running one application each. They can be killed and relocated by Kubernetes due to scale-down requests or node relocations. Applications must expect this and be designed accordingly. Resources like ConfigMaps, PersistentVolumeClaims, Jobs, and HorizontalPodAutoscalers are also created during pod deployment.\"}\n",
      "Done for page number:422\n",
      "{\"summary\": \"When developing apps for Kubernetes, application developers need to ensure their apps can handle frequent reconfigurations and changes in local IP addresses and hostnames. Stateful apps should use a StatefulSet to maintain persistent state and hostname, while data written to disk may disappear if not mounted to persistent storage. Containers can be restarted without preserving previous files or volumes, so using at least a pod-scoped volume is recommended to preserve data across container restarts.\"}\n",
      "Done for page number:423\n",
      "{\"summary\": \"A pod's lifecycle involves containers and processes, where file writes are lost upon restart unless using a volume to persist data. Container crashes or being killed result in new container creation with preserved data in the volume.\"}\n",
      "Done for page number:424\n",
      "{\"summary\": \"Using volumes to preserve files across container restarts can lead to a continuous crash loop if data gets corrupted, and Kubernetes will not automatically remove and reschedule dead or partially dead pods even when part of a ReplicaSet.\"}\n",
      "Done for page number:425\n",
      "{\"summary\": \"The document explains Kubernetes' behavior when a pod crashes. A ReplicaSet is created with crashing pods, and the output shows that the pod's status is CrashLoopBackOff. The Kubelet delays restarting the container because it keeps crashing. There is no action taken by the controller since current replicas match desired replicas.\"}\n",
      "Done for page number:426\n",
      "{\"summary\": \"Init containers can be used to initialize a pod by writing data to volumes and delaying the start of main containers. They run sequentially and only after completion, allowing preconditions to be met before proceeding. An example uses an init container to wait for a service required by the pod's main container to be up and running, terminating and starting the main container once ready.\"}\n",
      "Done for page number:427\n",
      "{\"summary\": \"When deploying a pod, its init container is started first. The main container won't run until dependencies are met. Best practices for handling inter-pod dependencies include making apps handle internal dependency readiness and using readiness probes to signal Kubernetes. Lifecycle hooks can be used to execute commands or HTTP requests when containers start or stop, similar to liveness and readiness probes.\"}\n",
      "Done for page number:428\n",
      "{\"summary\": \"Post-start hooks are executed immediately after a container's main process starts, allowing additional operations without modifying app source code. They run asynchronously and affect the container's state, killing the main container if they fail or return a non-zero exit code.\"}\n",
      "Done for page number:429\n",
      "{\"summary\": \"A pod's lifecycle includes pre-stop and post-start hooks that can be used to initiate a graceful shutdown, execute arbitrary operations, or run commands before container termination. Pre-stop hooks are executed before a container is terminated, while post-start hooks are executed after the container has started. These hooks can be configured in a pod manifest using similar syntax to adding a hook handler.\"}\n",
      "Done for page number:430\n",
      "{\"summary\": \"Pre-stop hooks in Kubernetes are used to perform actions before a container is terminated. A pre-stop hook can be defined to send an HTTP GET request, but it's not recommended as the container will be terminated regardless of the result. Instead, it's better to configure the container image to handle signals correctly, or use the exec form of ENTRYPOINT/CMD in the Dockerfile.\"}\n",
      "Done for page number:431\n",
      "{\"summary\": \"A pod's lifecycle is managed by the Kubelet, which terminates each container in a pod after setting a deletionTimestamp field when triggered by a DELETE request to the API server. Pre-stop hooks are executed before termination, and containers are given a configurable termination grace period to shut down cleanly. If the container doesn't terminate within this time, it is forcibly killed with SIGKILL.\"}\n",
      "Done for page number:432\n",
      "{\"summary\": \"The termination grace period for pods can be configured in the pod spec with `spec.terminationGracePeriodSeconds`. It defaults to 30 seconds. The grace period can also be overridden when deleting a pod using `$ kubectl delete po mypod --grace-period=5`. Applications should react to a SIGTERM signal by starting their shut-down procedure and terminating when it finishes, or use a pre-stop hook to notify the app to shut down.\"}\n",
      "Done for page number:433\n",
      "{\"summary\": \"When terminating a pod, its data should not be migrated immediately. Instead, a dedicated pod or CronJob can periodically check for orphaned data and migrate it to remaining pods. This approach ensures data is preserved even in node failures or application upgrades.\"}\n",
      "Done for page number:434\n",
      "{\"summary\": \"To ensure all client requests are handled properly when pods start up or shut down, apps need to follow a few rules. A readiness probe should be added to the pod spec to prevent clients from seeing connection refused errors. The probe should return success only when the app is ready to handle incoming requests, and an HTTP GET readiness probe can be used for this purpose.\"}\n",
      "Done for page number:435\n",
      "{\"summary\": \"When a pod is deleted, the API server notifies its watchers (Kubelet and Endpoints controller) which trigger parallel sequences of events (A and B). Sequence A involves stopping containers, notifying kube-proxy, and removing the pod from iptables. Sequence B involves deleting the pod as an endpoint, modifying the endpoint, notifying kube-proxy, and removing the pod from iptables. This ensures client requests are handled properly during pod shut-down.\"}\n",
      "Done for page number:436\n",
      "{\"summary\": \"When a pod is deleted, the Kubelet initiates the shutdown sequence by running the pre-stop hook and sending SIGTERM to the container. This process takes relatively short time due to direct path from API server to Kubelet. Meanwhile, the Endpoints controller removes the pod as an endpoint in all services it's part of, updating iptables rules on worker nodes to prevent new connections. Existing connections remain unaffected.\"}\n",
      "Done for page number:437\n",
      "{\"summary\": \"The API server must notify kube-proxy before modifying iptables rules, causing pods to still receive client requests after termination signal. A readiness probe has no bearing on this process. The proper solution is to wait for a long-enough time to ensure all proxies have done their job, but there's no guarantee on how long that is.\"}\n",
      "Done for page number:438\n",
      "{\"summary\": \"To properly shut down an application, wait for a few seconds before stopping new connections, close inactive keep-alive connections, wait for active requests to finish, and then shut down completely. A pre-stop hook can be added to prevent broken connections by waiting a few seconds.\"}\n",
      "Done for page number:439\n",
      "{\"summary\": \"To make apps easy to run and manage in Kubernetes, they should be built with minimal container images that only include necessary files. This can be achieved by using the FROM scratch directive in Dockerfiles. However, such images are difficult to debug. It's also important to properly tag images and use imagePullPolicy wisely to avoid issues with versioning and rolling back to previous versions.\"}\n",
      "Done for page number:440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"It's recommended to use tags with version designators instead of 'latest', and label resources with multiple dimensions for easier management. Additionally, describe each resource through annotations, providing contact information and metadata such as build and version details. This helps in understanding why a container terminated or is terminating continuously.\"}\n",
      "Done for page number:441\n",
      "{\"summary\": \"In Kubernetes, you can make it easy for ops people to triage issues by including a termination message in your log files. This can be achieved by writing a message to a specific file in the container's filesystem before exiting. The default path is /dev/termination-log, but you can override this by setting the terminationMessagePath field in the container definition. When a pod crashes, kubectl describe will show the reason for the crash without needing to inspect logs.\"}\n",
      "Done for page number:442\n",
      "{\"summary\": \"Kubernetes app development best practices include logging to standard output instead of files for easy log viewing with `kubectl logs`. If a container crashes, use `--previous` with `kubectl logs` to view previous container's logs. Alternatively, display log file using `kubectl exec cat <logfile>`. File copying is also possible with `kubectl cp`, transferring logs and other files between local machine and pod.\"}\n",
      "Done for page number:443\n",
      "{\"summary\": \"This page explains how to make apps easy to run and manage in Kubernetes, specifically using centralized logging. Centralized logging collects all logs from containers and stores them persistently in a central location, allowing historical analysis. The EFK stack (FluentD, ElasticSearch, Kibana) is used for this purpose, where FluentD gathers logs from containers, ElasticSearch stores them, and Kibana visualizes the data in a web browser.\"}\n",
      "Done for page number:444\n",
      "{\"summary\": \"Developing apps in Kubernetes does not require running them in the cluster during development. It's best to develop and run apps on your local machine, replicating the Kubernetes environment if needed. When connecting to backend services, set environment variables manually or use NodePort or LoadBalancer-type Services temporarily.\"}\n",
      "Done for page number:445\n",
      "{\"summary\": \"Best practices for development and testing include accessing the Kubernetes API server from outside the cluster using ServiceAccount's token or an ambassador container. Running inside a container during development can be done without rebuilding the image by mounting local filesystem into the container through Docker volumes. Minikube can also be used to try out apps in Kubernetes, and local files can be mounted into the Minikube VM and containers for testing purposes.\"}\n",
      "Done for page number:446\n",
      "{\"summary\": \"When developing apps with Minikube, it's best to use its Docker daemon by setting DOCKER_HOST environment variable. If this isn't possible, images can be built locally and copied over to the Minikube VM using docker save and load commands. This approach allows for immediate image availability in pods without needing to push the image to a registry or have the Kubelet pull it from an external registry. Additionally, Kubernetes abstracts away underlying infrastructure from apps, enabling smooth transitions between development and production environments.\"}\n",
      "Done for page number:447\n",
      "{\"summary\": \"You can manage running apps by committing changes to the VCS, and use tools like kube-applier or Ksonnet to simplify writing YAML/JSON manifests. Ksonnet allows you to define parameterized JSON fragments, reducing code repetition and making it easier to build Kubernetes resource manifests.\"}\n",
      "Done for page number:448\n",
      "{\"summary\": \"This chapter emphasizes the importance of Ksonnet and Jsonnet in defining consistent manifests, and introduces Continuous Integration and Delivery (CI/CD) pipelines for building application binaries, container images, and resource manifests. It highlights tools like Fabric8 and Google Cloud Platform's online labs to achieve DevOps-style development and deployment on Kubernetes. The chapter aims to deepen the reader's understanding of how resources come together in a typical Kubernetes application and provide insights into handling distributed components and eventual consistency.\"}\n",
      "Done for page number:449\n",
      "{\"summary\": \"This document provides tips for managing apps by keeping image sizes small, adding annotations and labels, and making termination reasons visible. It also teaches developing and running Kubernetes apps locally or in Minikube before deploying to a multi-node cluster, and extends Kubernetes with custom API objects and controllers.\"}\n",
      "Done for page number:450\n",
      "{\"summary\": \"This chapter covers defining custom API objects in Kubernetes, creating controllers for those objects, adding custom API servers, and self-provisioning services with the Service Catalog. It also mentions how others have extended Kubernetes to build PaaS solutions like Red Hat's OpenShift Container Platform and Deis Workflow.\"}\n",
      "Done for page number:451\n",
      "{\"summary\": \"Custom API objects can be defined in Kubernetes by creating a CustomResourceDefinition (CRD) object and posting it to the API server. This allows users to create instances of custom resource types, which can make tangible happenings in the cluster. Each CRD typically has an associated controller that performs actions based on the custom objects.\"}\n",
      "Done for page number:452\n",
      "{\"summary\": \"A custom resource in Kubernetes is created by posting a CustomResourceDefinition to the API server. This definition specifies the metadata and spec for the custom resource, including its name, scope, and required fields such as gitRepo. The custom resource can then be instantiated using a YAML or JSON file, which will result in the creation of a Service and Pod based on the specified requirements.\"}\n",
      "Done for page number:453\n",
      "{\"summary\": \"A custom API object named Website can be created in Kubernetes using a CustomResourceDefinition. The CRD defines a new resource with its own apiVersion, kind, and names. A YAML manifest is used to create an instance of the custom Website resource, specifying the gitRepo URL and other details. The kind is set to Website and the apiVersion is composed of the API group and version number defined in the CRD. The kubectl command is used to create the Website object.\"}\n",
      "Done for page number:454\n",
      "{\"summary\": \"Kubernetes can store, retrieve and delete custom resources through the API server after creating a CustomResourceDefinition object. Custom resources can be listed with kubectl get, described with kubectl describe, or retrieved in YAML format with kubectl get -o yaml. Deleting an instance of a custom resource is done with kubectl delete, deleting only the instance, not the CRD itself.\"}\n",
      "Done for page number:455\n",
      "{\"summary\": \"Custom API objects can be used to store data or trigger actions when created. In this case, creating a Website object results in the spinning up of a web server serving Git repository contents. To automate this process, a custom controller is built and deployed to watch for Website objects and create a Service and web server Pod (or Deployment) for each one, ensuring pod management and node failure resilience.\"}\n",
      "Done for page number:456\n",
      "{\"summary\": \"The website controller starts watching for Website objects by connecting to the kubectl proxy, which forwards requests to the API server. The API server sends watch events for every change, and upon receiving an ADDED event, the controller creates a Deployment and Service object with a template pod containing an nginx server and git-sync process, exposing the web server through a random node port.\"}\n",
      "Done for page number:457\n",
      "{\"summary\": \"A custom API object controller watches for changes to Website resources, deleting related Deployment and Service resources when necessary. The proper way to watch objects is to not only listen for events but also periodically re-list all objects in case any were missed. To run the controller in production, deploy it through a Kubernetes Deployment resource, specifying replicas and template details.\"}\n",
      "Done for page number:458\n",
      "{\"summary\": \"A Kubernetes deployment is created to extend its functionality, deploying a two-container pod with a controller and ambassador container. A service account named website-controller is created, and if RBAC is enabled, a clusterrolebinding is added to allow the controller to watch Website resources and create Deployments or Services. The deployment is then run, and logs are checked to see if it received the watch event.\"}\n",
      "Done for page number:459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"A custom API object is defined with a controller that creates a Service and Deployment upon receiving an ADDED event. The API server responds with a 201 Created response, creating resources such as Deployments, Services, and Pods. Users can deploy static websites in seconds without knowing Kubernetes resources, except the custom Website resource. However, users can create invalid objects due to lack of validation schema, but the controller can validate objects and update error messages.\"}\n",
      "Done for page number:460\n",
      "{\"summary\": \"Kubernetes allows custom object validation by enabling the CustomResourceValidation feature gate and specifying a JSON schema in the CRD, but a better approach is to implement a custom API server for custom objects through API server aggregation, which can validate objects directly and provide a single point of access for clients.\"}\n",
      "Done for page number:461\n",
      "{\"summary\": \"To extend Kubernetes, a custom API server can be added by deploying it as a pod and exposing it through a Service. An APIService resource is created to integrate it into the main API server, forwarding client requests to the custom API server for resources from a specific API group and version. A custom CLI tool can also be built to make deployment of custom objects easier, with dedicated commands for manipulating those objects.\"}\n",
      "Done for page number:462\n",
      "{\n",
      "  \"summary\": \"Kubernetes' Service Catalog allows users to provision services (e.g., databases) without manual deployment. It introduces four generic API resources: ClusterServiceBroker, ClusterServiceClass, ServiceInstance, and ServiceBinding, which enable users to request services and bind them to client pods.\"\n",
      "}\n",
      "Done for page number:463\n",
      "{\"summary\": \"The Kubernetes Service Catalog is a distributed system composed of API Server, etcd as storage, and Controller Manager with controllers running in it. It creates resources such as ServiceInstance, Class, Plan, and Broker through YAML/JSON manifests posted to the API server. External service brokers provision services which are then used by client pods.\"}\n",
      "Done for page number:464\n",
      "{\"summary\": \"A cluster administrator can register external ServiceBrokers in the Service Catalog, implementing the OpenServiceBroker API. The API provides operations for retrieving services, provisioning, updating, binding, unbinding, and deprovisioning service instances. A ClusterServiceBroker resource is registered by posting a manifest to the Service Catalog API, specifying the broker's URL. The Service Catalog then creates ClusterServiceClass resources for each service type, with associated service plans allowing users to choose their service level.\"}\n",
      "Done for page number:465\n",
      "{\"summary\": \"The Kubernetes Service Catalog allows users to retrieve a list of services that can be provisioned in a cluster using kubectl get serviceclasses. ClusterServiceClasses are similar to StorageClasses, but allow selecting the type of service rather than storage. Details of a ClusterServiceClass can be retrieved with its YAML, showing plans such as free and paid options provided by a broker.\"}\n",
      "Done for page number:466\n",
      "{\"summary\": \"To use a service like a database in Kubernetes, create a Service-Instance resource specifying the ClusterServiceClass and chosen plan, along with any required parameters. The Service Catalog will then provision the service by contacting the broker, which may spin up a new instance of the database in a VM or elsewhere. Successful provisioning can be checked by inspecting the status section of the ServiceInstance.\"}\n",
      "Done for page number:467\n",
      "{\"summary\": \"A ServiceBinding resource is created to bind a provisioned ServiceInstance to pods, storing necessary credentials in a Secret. The Service Catalog creates a new Secret with specified data and stores it for use by pods to connect to the database. The Secret can be mounted into pods to read its contents and access the service instance.\"}\n",
      "Done for page number:468\n",
      "{\"summary\": \"The Service Catalog allows service providers to expose services in any Kubernetes cluster by registering a broker in that cluster. A Secret can be used to hold credentials for connecting to a service instance, and can be provisioned or bound using the service catalog. Once no longer needed, a ServiceBinding can be deleted, which deletes the associated Secret and triggers an unbinding operation on the service broker. If the service instance is no longer needed, it should also be deleted, causing a deprovisioning operation on the service broker.\"}\n",
      "Done for page number:469\n",
      "{\"summary\": \"Red Hat OpenShift is a Platform-as-a-Service built on top of Kubernetes, focusing on developer experience. It automates rollouts, scaling, and image building, providing user and group management for secure multi-tenant clusters. Additional resources include users & groups and projects, extending the capabilities beyond standard Kubernetes features.\"}\n",
      "Done for page number:470\n",
      "{\"summary\": \"This chapter covers extending Kubernetes using OpenShift, including user management features that allow for multi-tenant environments and specifying access control. It also introduces application templates, a parameterizable list of resources in JSON or YAML format that can be stored in the API server and instantiated with replacement values, allowing for reusable and customizable deployments.\"}\n",
      "Done for page number:471\n",
      "{\"summary\": \"OpenShift provides features such as template processing, automated builds and deployments using BuildConfigs and DeploymentConfigs respectively. Templates allow users to run complex applications by specifying arguments or none at all. BuildConfigs enable OpenShift to build and deploy container images from source code in a Git repository without manual intervention. Additionally, DeploymentConfigs can automatically deploy newly built images in the cluster by noticing changes in an ImageStream.\"}\n",
      "Done for page number:472\n",
      "{\"summary\": \"OpenShift provides features like Route resource for exposing Services externally, pre- and post-deployment hooks, and ReplicationControllers instead of ReplicaSets. A Router is available out of the box, similar to an Ingress controller. To try OpenShift, use Minishift or OpenShift Online Starter. Deis Workflow and Helm are also PaaS built on top of Kubernetes, providing features like BuildConfigs and DeploymentConfigs.\"}\n",
      "Done for page number:473\n",
      "{\"summary\": \"Deis Workflow is a tool that creates Services and ReplicationControllers in Kubernetes, providing developers with a simple environment. Deploying new versions of an app is triggered by pushing changes with git push deis master. Deis also developed Helm, a package manager for Kubernetes, comprised of a CLI tool and Tiller server component. Helm deploys and manages application packages (Charts) in a Kubernetes cluster, using the helm CLI tool to talk to Tiller, which creates necessary Kubernetes resources defined in the Chart.\"}\n",
      "Done for page number:474\n",
      "{\"summary\": \"Kubernetes can be extended using Helm charts, which are pre-configured templates for applications like PostgreSQL or MySQL that can be easily installed with a one-line command. This simplifies deployment and eliminates the need to manually create manifests. Additional examples include running an OpenVPN server inside the cluster, allowing local machine access to Services as if it were a pod in the cluster.\"}\n",
      "Done for page number:475\n",
      "{\"summary\": \"This chapter shows how Kubernetes can be extended beyond its existing functionalities by registering custom resources, implementing custom controllers, and using API aggregation. Companies like Dies and Red Hat have achieved this through platforms-as-a-service and self-provisioning of external services with the Kubernetes Service Catalog. Additionally, Helm enables easy deployment of existing apps without manual resource manifests.\"}\n",
      "Done for page number:476\n",
      "{\"summary\": \"To switch between Minikube and Google Kubernetes Engine (GKE), run 'minikube start' to reconfigure kubectl for Minikube, or use 'gcloud container clusters get-credentials my-gke-cluster' to configure kubectl for GKE. Switching back to Minikube requires stopping and restarting it.\"}\n",
      "Done for page number:477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"To use kubectl with multiple clusters or namespaces, you can configure the location of the kubeconfig file using the KUBECONFIG environment variable. This allows you to specify multiple config files, which are then used by kubectl. The kubeconfig file itself has four sections: a list of clusters, users, contexts, and the current context. A context defines a kubectl context, while a user's credentials are stored in a separate section.\"}\n",
      "Done for page number:478\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_page_idx,end_page_index):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43menrich_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone for page number:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx))\n",
      "Cell \u001b[0;32mIn[9], line 148\u001b[0m, in \u001b[0;36menrich_page\u001b[0;34m(page_idx)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menrich_page\u001b[39m(page_idx):\n\u001b[1;32m    144\u001b[0m     \n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m#print(\"Page Number\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#print(page_idx)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     page_text\u001b[38;5;241m=\u001b[39m\u001b[43mdocument_dict_deserialized_stage2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpage_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m##Entity Extraction Enrichment\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     page_summary_txt\u001b[38;5;241m=\u001b[39mextract_summary_per_page(page_text)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for idx in range(start_page_idx,end_page_index):\n",
    "   \n",
    "        enrich_page(idx)\n",
    "        print(\"Done for page number:\"+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c44d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The main contents of Kubernets In Action book is contained from page 87 to 565####\n",
    "####Thats why we will only take the sliced portion i.e. from page 87 to 565 of the pdf dictionary list#####\n",
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('./pdf_enriched_output/pdf_enriched_content_dict_stage2_phase1.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized_stage2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5054dc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
