{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640a7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='./PDF_FOLDER/adaptive_pooling.pdf'\n",
    "#url=\"./PDF_FOLDER/shannon_51.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e33cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "output = pymupdf4llm.to_markdown(url, page_chunks=True ,write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e94bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 1},\n",
       "  'toc_items': [],\n",
       "  'tables': [],\n",
       "  'images': [],\n",
       "  'graphics': [],\n",
       "  'text': '# Adaptive Pooling Is All You Need: An Empirical Study on Hyperparameter-insensitive Human Action Recognition Using Wearable Sensors\\n\\n\\nMubarak G. Abdu-Aguye\\n_Department of Computer Engineering_\\n_Ahmadu Bello University_\\nZaria, Nigeria\\nmubarak.abduaguye@ejust.edu.eg\\n\\n\\nWalid Gomaa\\n_Computer Science and Engineering_\\n_Egypt-Japan University of Science and Technology_\\nNew Borg el-Arab, Egypt\\nwalid.gomaa@ejust.edu.eg\\n\\n\\nYasushi Makihara\\n_ISIR_\\n_Osaka University_\\nOsaka, Japan\\nmakihara@am.sanken.osaka-u.ac.jp\\n\\n\\nYasushi Yagi\\n_ISIR_\\n_Osaka University_\\nOsaka, Japan\\nyagi@am.sanken.osaka-u.ac.jp\\n\\n\\n**_Abstract_** **—A plethora of techniques have been proposed in**\\n**human action recognition fields, and particularly deep learningbased methods such as convolutional neural networks (CNNs)**\\n**have achieved impressive results. Usually, there is need to tune**\\n**hyper-parameters in the deep neural network (e.g., filter size,**\\n**stride) to achieve reasonable results. Such hyper-parameter**\\n**tuning is, however, extremely time and resource-intensive even**\\n**for small models. In this paper, we posit that the inclusion of**\\n**an adaptive pooling in CNNs used for human action recognition**\\n**largely eliminates the need for hyper-parameter tuning. Specifically, we demonstrated our idea for human action recognition**\\n**using inertial sensor data (i.e., a temporal sequence) with a onedimensional adaptive pooling. We compared the adaptive pooling**\\n**to conventional CNNs with randomly chosen hyper-parameters**\\n**using a publicly available data set for human action recognition.**\\n**Experimental results showed that the adaptive pooling achieved**\\n**better accuracy than the conventional CNNs.**\\n**—activity recognition, convolutional neural networks, hyperparameters, adaptive pooling** **_Index Terms_**\\n\\nI. I NTRODUCTION\\n\\nHuman action recognition is a challenging task. Traditional\\napproaches to solving the problem include the use of video\\ndata capturing the visual patterns of different actions. More recently however, Inertial Measurement Unit (IMU) sensor data\\nhas been adopted for this same purpose with impressive results\\n\\n[1], [2]. This also comes with the advantages of mobility\\nand improved user privacy. As such, we direct the proceeding\\ndiscussion to IMU sensor-based activity recognition.\\nRegardless of the modality adopted, Human action recognition is beset by different concerns including feature extraction\\nand classification modalities. Traditionally, Feature extraction\\n\\nWalid Gomaa is supported by the Information Technology Industry Development Agency (ITIDA) under the ITAC Program Grant no. PRP2019.R26.1\\n\\n-  A Robust Wearable Activity Recognition System based on IMU Signals.’\\n\\n\\nfor IMU-based sensor data was handled using different statistical and temporal measures, including but not limited to\\nentropy, skewness, kurtosis, etc., with the selection of features\\nusually determined empirically. Although good performance\\nwas obtained, the problem of feature extraction and selection\\nremained pertinent.\\nWith the advent of deep neural networks, specifically convolutional neural networks (CNNs), the need for manual\\nfeature extraction is obviated. This is due to the ability of\\nthe convolutional portions of such networks to automatically\\nlearn the optimal features which aid the downstream task\\n(i.e., classification, regression, etc.). This has led to massive\\nperformance improvements in several domains, such as: WiFi\\nlocalization [3], Radio communications [4] and Video Translation [5].\\nThe prevalence of deep neural networks motivates the\\nquestion of how to obtain the best performance (e.g. predictive\\naccuracy, etc.) from their use. This could be seen to consist\\nof two distinct components. The first of these is the problem\\nof finding the ”best” architecture for the given problem,\\nwhich is formally termed Neural Architecture Search (NAS)\\n\\n[6]. The second component is concerned with finding the\\noptimal configuration for a given network architecture. This\\nconfiguration refers to the _hyperparameters_ of the network,\\nwhich determine the structure of the network. For instance, the\\nnumber of recurrent units to use in Recurrent Neural Networks\\n(RNNs) or the number, size, and stride of convolutional\\nfilters in Convolutional Neural Networks (CNNs). However,\\nnetwork architectures and hyperparameters need to be tuned\\nfor individual problems, and cannot, in general, be inferred\\nautomatically in a theoretically-principled manner.\\nIn the case of NAS, unless particular constraints are placed\\non the network architecture, there are virtually infinite pos-\\n\\n\\n-----\\n\\n'},\n",
       " {'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 2},\n",
       "  'toc_items': [],\n",
       "  'tables': [],\n",
       "  'images': [{'number': 4,\n",
       "    'bbox': (311.9779968261719,\n",
       "     383.435302734375,\n",
       "     567.0969848632812,\n",
       "     557.0350341796875),\n",
       "    'transform': (255.11898803710938,\n",
       "     0.0,\n",
       "     -0.0,\n",
       "     173.5997314453125,\n",
       "     311.9779968261719,\n",
       "     383.435302734375),\n",
       "    'width': 773,\n",
       "    'height': 526,\n",
       "    'colorspace': 3,\n",
       "    'cs-name': 'DeviceRGB',\n",
       "    'xres': 96,\n",
       "    'yres': 96,\n",
       "    'bpc': 8,\n",
       "    'size': 98541}],\n",
       "  'graphics': [],\n",
       "  'text': 'sible architectures that may yield optimal performance on\\nthe given problem. Additionally, it is immediately clear that\\nit is computationally-expensive due to the need to train the\\ncandidate models from scratch each time. Additionally, NAS\\nis, to some extent, still subject to the hyperparameter search\\nproblem i.e. regardless of the architecture chosen, proper\\nhyperparameters must be selected to yield good performance\\non the problem. This also implies that there is no theoretical\\nguarantee that a simpler architecture with proper hyperparameters cannot match or outperform the optimal architecture as\\ndetermined by a NAS procedure.\\nWe believe that the preceding discussion sets the stage for\\nemphasizing the relative benefit of focusing on hyperparameter\\nsearch for deep networks. Given the representational capacity\\nof deep networks in general, it is not unreasonable to instead\\nattempt to tune them for optimal performance rather than\\nfocus solely on their particular architecture. This is because\\nhyperparameter tuning scales with the complexity of the network, and therefore its difficulty can be somewhat contained\\nby simplifying the network architecture as much as possible.\\nAlthough the hyperparameter search problem is, like NAS,\\nbasically infinite, it is much more tractable in practice because\\nit inherently requires discrete and not continuous solutions,\\nand the search space can be limited to regions which contain a\\n(small) range of values. Therefore, we believe that considering\\na fairly simple deep network and tuning that accordingly is a\\nmuch more feasible approach to obtaining good performance\\non a range of different tasks.\\nThere are existing techniques designed to mitigate the\\nhyperparameter search problem. From first principles, Gridsearch [7] is one such approach that may be applied to\\nthe hyperparameter optimization problem, which considers a\\nsubregion of the (discrete) hyperparameter space, and visits\\nevery point in this region with a view to finding the optimal\\nhyperparameter configuration therein. This is something of a\\nnaive approach, necessitating the use of less computationally\\nintensive approaches and methods. Random search [8] is\\nanother approach, which involves the visitation of random\\npoints within some region of the hyperparameter space. This\\napproach, being non-exhaustive, may not yield the optimal\\nresults but rather ‘good-enough’ results depending on the\\ncriteria set. More advanced techniques like Bayesian hyperparameter optimization [9] also exist but are not applicable to all\\nproblems, in that they rely on certain assumptions that cannot\\neasily be guaranteed to be universally valid. Regardless, the\\npreviously-described techniques still remain resource-intensive\\ndue to the need to train and evaluate multiple models, although\\ncountermeasures like parallelization may be adopted to mitigate this. In the presence of these drawbacks, two questions\\nevolve:\\n\\nIs it possible to derive a technique that yields optimal\\n\\n_•_\\nmodel hyperparameters without the associated computational overhead?\\n\\n_•_\\nIs it possible to design models that are robust to hyperparameter settings, such that the exact selection of\\nhyperparameters does not affect the performance of the\\n\\n\\nmodel significantly?\\nTherefore, the work done in this paper is aimed towards\\nproviding an answer to the second question. We propose\\nthe inclusion of _adaptive pooling_ in convolutional neural\\nnetworks applied to activity recognition based on IMU-based\\nsensor data. Adaptive Pooling is a technique to perform multiscale summarization over convolutional feature maps, while\\ncapturing the essential behavior of the feature map itself. It has\\nthe effect of reducing the amount of data needed in tuning the\\ndownstream portion of CNNs, while reducing the convergence\\ntime and increasing generalization performance. To the best of\\nour knowledge, this is also the seminal work in which adaptive\\npooling is adopted for the realization of hyperparameterrobust deep convolutional networks. We design a suitable\\nexperimental methodology to validate our proposition, and\\ncarry out experiments on a publicly-available dataset to this\\nend. Empirical results obtained from our experiments indicate\\nthe efficacy of the proposed method in realizing the stated\\ngoal, consistently providing near-optimal model performance\\nin the worst case.\\nThe rest of this paper is organized as follows. Section II\\nprovides some background on concepts relevant to the work\\ndone in this paper. Section III introduces our experimental\\nmethodology and its particulars. In Section IV, we present the\\nresults obtained and interpret them accordingly. We conclude\\nthe paper and describe points for future consideration in\\nSection V.\\n\\n![adaptive_pooling.pdf-1-0.png](adaptive_pooling.pdf-1-0.png)\\n\\nFig. 1. Spatial pyramid pooling for 1D data (4-2-1 pooling sizes)\\n\\nII. B ACKGROUND\\n_A. Adaptive Pooling_\\nAdaptive pooling is a generalization of another technique\\ncalled spatial pyramid pooling, which was first introduced in\\n\\n[10]. Spatial pyramid pooling was introduced to solve the\\nproblem of varying input sizes in CNNs for image-based tasks,\\nand therefore involves the conversion of convolutional feature\\nmaps of varying sizes into fixed-length summarizations. These\\nfixed-length summarizations are compatible with the fullyconnected portions of such networks which are actually the\\n\\n\\n-----\\n\\n'},\n",
       " {'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 3},\n",
       "  'toc_items': [],\n",
       "  'tables': [{'bbox': (331.1700134277344,\n",
       "     375.5350036621094,\n",
       "     541.0540161132812,\n",
       "     439.4949951171875),\n",
       "    'rows': 3,\n",
       "    'columns': 2}],\n",
       "  'images': [{'number': 0,\n",
       "    'bbox': (72.74500274658203,\n",
       "     50.54020690917969,\n",
       "     539.2589111328125,\n",
       "     305.6650085449219),\n",
       "    'transform': (466.513916015625,\n",
       "     0.0,\n",
       "     -0.0,\n",
       "     255.1248016357422,\n",
       "     72.74500274658203,\n",
       "     50.54020690917969),\n",
       "    'width': 1024,\n",
       "    'height': 560,\n",
       "    'colorspace': 3,\n",
       "    'cs-name': 'DeviceRGB',\n",
       "    'xres': 96,\n",
       "    'yres': 96,\n",
       "    'bpc': 8,\n",
       "    'size': 314445}],\n",
       "  'graphics': [],\n",
       "  'text': '![adaptive_pooling.pdf-2-0.png](adaptive_pooling.pdf-2-0.png)\\n\\nFig. 2. System Architecture.\\n\\n\\nnetwork portions enforcing the requirement for fixed-length\\ninputs.\\nThe summarization itself is performed over multiple scales.\\nThat is, the feature map is broken into a number of fixedsize regions, over which a simple pooling operation (max\\nor average pooling) is carried out, thereby yielding a fixed\\nnumber of features. This is typically done over multiple\\nvarying region sizes, ranging from global (i.e., over the whole\\nimage), to smaller sizes. At each size, the number of features\\nderived is different, but captures details at different levels of\\ngranularity about the input data. By concatenating the features\\nobtained at different region sizes (i.e., scales), a compact yet\\ninformative descriptor is obtained.\\nAlthough the preceding discussion is focused on 2dimensional feature maps, it can be seen that the same\\nprinciple can be applied to multidimensional inputs as well.\\nTherefore, adaptive pooling simply describes the extension of\\nspatial pyramid pooling to arbitrary dimensions, although we\\nfocus on the one-dimensional case in this work due to the\\nnature of the signals involved. In this case, each of the resulting\\n1-D feature maps is divided into regions, over which a pooling\\noperation (i.e., max or average pooling) is performed, similar\\nto the 2-D case. This is illustrated in Figure 1.\\nIn this work we use a simple notation to describe the\\nparameters/configuration of the adaptive pooling operation,\\nwhich effectively determines the (number of) levels/region\\nsizes at which adaptive pooling is performed. For simplicity,\\nwe use hyphens to denote the sizes used i.e., 4-2-1 implies that\\nthe input is broken into 4 regions, each of which is pooled, then\\n2 regions, then a global pooling (i.e., 1 region) is performed.\\nIn general for a configuration x-y-z, the number of features\\n\\n\\nTABLE I\\nH YPERPARAMETER C ONFIGURATIONS C ONSIDERED\\n\\n|Hyperparameter Name|Values|\\n|---|---|\\n|Conv1 Size Conv2 Size Conv2 Stride MaxPool Size MaxPool Stride|2,3,4,5,6,7,8,9,10,11,12,13,14,15 2,3,4,5,6,7,8,9,10,11,12,13,14,15 1,2,3,4,5,6,7,8 2,3,4,5,6,7,8 1,2,3,4,5,6,7,8|\\n|Total Configurations:|87,808|\\n\\n\\n\\nobtained from the use of adaptive pooling is _x_ + _y_ + _z_ .\\n\\nIII. E XPERIMENTAL S ETUP\\n\\n_A. Dataset_\\n\\nFor purposes of clarity, we perform our experiments on the\\nREALDISP dataset ( [11], [12]), which is a dataset consisting\\nof 33 different activities collected from 17 individuals in\\ntotal at a sample rate of 50Hz. REALDISP includes 4 sensor\\nmodalities (i.e., accelerometer, gyroscope, magnetometer and\\nrotation), but we consider only the first two in this work.\\nWe preprocess the dataset into fixed-length windows, each 1\\nsecond long and label each window according to the activity it\\ncorresponds to. This yields a total number of 13,963 samples,\\n75% of which are used for training the models with the\\nremainder used in testing the predictive performance of the\\nmodels. It is also pertinent to state that, during each of the\\n_n_ training and testing steps mentioned previously, a different\\ntraining and testing split is considered in order to obtain a\\nbetter estimate of the model’s true performance. We select\\n_n_ = 5 in this work.\\n\\n\\n-----\\n\\n'},\n",
       " {'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 4},\n",
       "  'toc_items': [],\n",
       "  'tables': [],\n",
       "  'images': [{'number': 14,\n",
       "    'bbox': (311.9779968261719,\n",
       "     187.18064880371094,\n",
       "     567.096923828125,\n",
       "     413.9530029296875),\n",
       "    'transform': (255.118896484375,\n",
       "     0.0,\n",
       "     -0.0,\n",
       "     226.77235412597656,\n",
       "     311.9779968261719,\n",
       "     187.18064880371094),\n",
       "    'width': 540,\n",
       "    'height': 480,\n",
       "    'colorspace': 3,\n",
       "    'cs-name': 'DeviceRGB',\n",
       "    'xres': 96,\n",
       "    'yres': 96,\n",
       "    'bpc': 8,\n",
       "    'size': 63774}],\n",
       "  'graphics': [],\n",
       "  'text': '_B. Protocol_\\n\\nThe core hypotheses governing this work are itemized as\\nfollows:\\n\\nHypothesis 1: Given any arbitrary convolutional neural\\n\\n_•_\\nnetwork architecture, including adaptive pooling in it\\ngives comparable or better results than the vanilla architecture i.e., without adaptive pooling.\\n\\n_•_\\nHypothesis 2: Considering some subregion of the hyperparameter space, the variance in performance of the\\ncorresponding models which include adaptive pooling is\\nsmaller than the corresponding models without it, due\\nto the hyperparameter-agnosticism granted by adaptive\\npooling.\\n\\nHypothesis 3: The performance of the models including\\n\\n_•_\\nadaptive pooling are very close to the best performance\\nobtained by any of the considered models i.e., adaptive\\npooling situates the model performance at or very close\\nto the performance optimum achieved by the best model\\namongst the configurations evaluated.\\n\\nTherefore, in line with the above hypotheses, we design our\\nexperimental protocol as follows:\\n\\nWe design a benchmark convolutional architecture\\n\\n_•_\\n(shown in Figure 2), consisting of a number of convolutional layers and a single max-pooling layer. Since we\\nare not considering neural architecture search, we fix the\\nbase architecture and instead adjust the hyperparameters\\nof the network layers. We adopt this architecture as it has\\nbeen used successfully in solving the activity recognition\\nproblem with generally good results [13], [14], and is\\nsufficiently small to feasibly demonstrate the concept\\nunder discussion.\\n\\nWe define a subregion of the hyperparameter space which\\n\\n_•_\\ncontains reasonable hyperparameter configurations for the\\nproblem at hand. The hyperparameters and the values\\nconsidered for each are given in Table I.\\n\\nGiven the large number of candidate configurations in\\n\\n_•_\\nthis subregion (a total of 87,808), we randomly sample\\na number of configurations to reduce the computational\\nresources required for the experiments while giving an\\nunbiased estimate of the expected behavior of the proposed method. Results from 2,743 models (3.12% of the\\ntotal) are obtained.\\n\\n\\n![adaptive_pooling.pdf-3-0.png](adaptive_pooling.pdf-3-0.png)\\n\\nFor each candidate configuration _c_ , we:\\n\\n_•_\\n\\n**–** Initialize the benchmark architecture’s layers with\\nthe elements of the configuration _c_ , then obtain\\nthe mean value _m_ _c_ of the performance metric (i.e.,\\nprediction accuracy) over a number of runs _n_ , each\\nconsists of a training and a testing step.\\n\\n**–** Using the same configuration, we replace the default\\npooling operator - i.e. traditional max-pooling - in\\nthe architecture with an adaptive pooling layer with a\\nfixed hyperparameter configuration (i.e., 4-2-1, using\\nthe notation described previously), then obtain the\\nmean value _m_ _′_ _c_ of the performance metric using\\n\\n\\nthis modified version of the architecture, also over\\n_n_ training and testing steps.\\n\\n_•_ We then compare _m_ _c_ and _m_ _′_ _c_ and aggregate the statistics\\nover all considered _c_ candidates. Based on the derived\\nstatistics, we then validate each of the foregoing hypothesis accordingly.\\nAll the experiments were implemented using the PyTorch\\n\\n[15] neural network library, version 1.0.\\nWe present the results obtained and their interpretation in\\nthe following section.\\n\\nFig. 3. Distribution of Model Performances With/Without Adaptive Pooling\\n\\nIV. R ESULTS AND D ISCUSSION\\n\\nIn order to streamline the proceeding discourse, we present\\nthe results obtained graphically. We derive a histogram over\\nthe model performances as shown in Figure 3, showing the\\nstatistical distribution thereof between the proposed and comparative (i.e., without adaptive pooling) models. Additionally,\\nwe include a scatter plot of the baseline model performances\\nagainst the proposed models’ performances, shown in Figure 4.\\nIt can be observed that, when the proposed method is used,\\nthe mean of the models’ performances is higher than the\\nmean of the baseline model (94.14% vs. 92.31%). This is in\\nline with the first hypothesis put forth, and may be attributed\\nto the inherent benefits of the adaptive pooling - its ability\\nto effectively summarize the input data, maintaining its key\\ncharacteristics while eliminating much of the details which\\nmay be uninformative (or even misleading to the classifier).\\nThis directly improves generalization as it permits the classification portion of the network focus on the core parts of the\\ninput which are beneficial for the downstream classification,\\nrather than factoring in a large number of features which may\\neventually degrade its performance based on their noisiness.\\nAdditionally, the summarized version of the input also reduces\\n\\n\\n-----\\n\\n'},\n",
       " {'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 5},\n",
       "  'toc_items': [],\n",
       "  'tables': [],\n",
       "  'images': [{'number': 0,\n",
       "    'bbox': (60.32099914550781,\n",
       "     50.54225158691406,\n",
       "     288.66552734375,\n",
       "     277.31201171875),\n",
       "    'transform': (228.34454345703125,\n",
       "     0.0,\n",
       "     -0.0,\n",
       "     226.76976013183594,\n",
       "     60.32099914550781,\n",
       "     50.54225158691406),\n",
       "    'width': 725,\n",
       "    'height': 720,\n",
       "    'colorspace': 3,\n",
       "    'cs-name': 'DeviceRGB',\n",
       "    'xres': 96,\n",
       "    'yres': 96,\n",
       "    'bpc': 8,\n",
       "    'size': 184845}],\n",
       "  'graphics': [],\n",
       "  'text': 'From the preceding discussion, it can be seen that each\\nof the proposed hypotheses have been confirmed, indicating\\nthe validity of the initial proposition. Although the results\\npresented have been aggregated over a fraction of the total\\nhyperparameter space, we ensure that random sampling in\\nsome sense considers the entirety of the desired subregion and\\ntherefore provides a statistically-valid view of the performance\\nof the proposed method.\\n\\nV. C ONCLUSION AND F UTURE W ORK\\n\\nIn this work, we have proposed an empirical adjustment for\\nconvolutional neural networks as applied to wearable sensorbased human activity recognition, which aims to eliminate\\nthe need for hyperparameter tuning. We design an experimental methodology to empirically validate our hypothesis,\\nby considering the results obtained from a baseline CNN\\narchitecture using differently and randomly sampled hyperparameter configurations. Next, we compare them to the results\\nobtained from a version of the same architecture modified\\nbased on the proposed method. We obtained results indicating\\nthat the proposed modification yields generally better and\\nmore stable/consistent results than the baseline architecture,\\nthereby validating the preceding approach and establishing its\\nsuitability for the domain in question.\\nIn the future we intend to perform much more comprehensive evaluations of different models and also investigate the\\npotential of adaptive pooling in achieving the same or similar\\neffects for data types at different dimensionalities e.g. images.\\n\\nR EFERENCES\\n\\n\\n![adaptive_pooling.pdf-4-0.png](adaptive_pooling.pdf-4-0.png)\\n\\nFig. 4. Scatter plot of model performances with/without adaptive pooling.\\nThe Red diagonal line indicates a trade-off between accuracies with/without\\nadaptive pooling, i.e., plots above the line means that accuracies with adaptive\\npooling are better than those without adaptive pooling.\\n\\nthe number of weights utilized in the classification portion of\\nthe network, allowing for faster convergence. Figure 4 also\\nshows the majority of the points distributed above the red\\nline, indicating that for a given configuration, the performance\\nobtained from the proposed method is generally higher than\\nthat obtained using the baseline model.\\nAdditionally, it is also apparent that the spread of the\\nmodels’ performances when using the proposed method is\\nmuch smaller than that without adaptive pooling, which can\\nalso be seen from the spread of the points for the horizontal\\n(i.e., without average pooling) and the vertical (i.e., with\\naverage pooling) directions, in Figure 4. This indicates that\\nthe variance of the models which include the proposed method\\n(0.778% 2 ) is much smaller than the models which do not\\ninclude it (6.09% 2 ). This is in line with the expectations\\nput forth by the second hypothesis. This occurs because\\nunlike the baseline models (i.e., which include max-pooling),\\nthe inclusion of adaptive pooling lends significant robustness\\nagainst the hyperparameters chosen, allowing the models to\\nmaintain their performance even in the presence of disparate\\nhyperparameter settings. This is by virtue of its ability to\\nsummarize the input data across multiple timescales and\\ntherefore preserve the salient information in a robust way.\\nGoing further, it should be noted that the small variance\\nin the performances obtained from the proposed models also\\nindicates that the models are reasonably comparable to the\\nvery best model obtained, although a slight performance\\npenalty may be observed. This validates the third hypothesis,\\nsince their actual performance values lie close (i.e., within 45%) to the maximum possible performance attainable (from\\nusing this architecture).\\n\\n\\n\\n[1] B. Barshan and M. C. Y¨ uksek, “Recognizing daily and sports activities\\nin two open source machine learning environments using body-worn\\nsensor units,” _The Computer Journal_ , vol. 57, no. 11, pp. 1649–1667,\\n2014.\\n\\n[2] S. Ashry, R. Elbasiony, and W. Gomaa, “An lstm-based descriptor for\\nhuman activities recognition using imu sensors,” in _Proceedings of the_\\n_15th International Conference on Informatics in Control, Automation_\\n_and Robotics, ICINCO_ , vol. 1, 2018, pp. 494–501.\\n\\n[3] H. Rizk, M. Torki, and M. Youssef, “Cellindeep: Robust and accurate\\ncellular-based indoor localization via deep learning,” _IEEE Sensors_\\n_Journal_ , vol. 19, no. 6, pp. 2305–2312, 2018.\\n\\n[4] C.-K. Wen, W.-T. Shih, and S. Jin, “Deep learning for massive mimo\\ncsi feedback,” _IEEE Wireless Communications Letters_ , vol. 7, no. 5, pp.\\n748–751, 2018.\\n\\n[5] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,\\n“Evaluating surgical skills from kinematic data using convolutional neural networks,” in _International Conference on Medical Image Computing_\\n_and Computer-Assisted Intervention_ . Springer, 2018, pp. 214–221.\\n\\n\\n\\n[6] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\\nsurvey.” _Journal of Machine Learning Research_ , vol. 20, no. 55, pp.\\n1–21, 2019.\\n\\n\\n\\n[7] D. C. Montgomery, _Design and analysis of experiments_ . John wiley\\n& sons, 2017.\\n\\n\\n\\n[8] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” _Journal of Machine Learning Research_ , vol. 13, no. Feb, pp.\\n281–305, 2012.\\n\\n\\n\\n[9] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimization of machine learning algorithms,” in _Advances in neural information_\\n_processing systems_ , 2012, pp. 2951–2959.\\n\\n[10] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” _IEEE transactions on_\\n_pattern analysis and machine intelligence_ , vol. 37, no. 9, pp. 1904–\\n1916, 2015.\\n\\n\\n-----\\n\\n'},\n",
       " {'metadata': {'format': 'PDF 1.5',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'creator': \"'Certified by IEEE PDFeXpress at 04/04/2020 6:15:10 AM'\",\n",
       "   'producer': 'pdfTeX-1.40.19',\n",
       "   'creationDate': 'D:20200404131156Z',\n",
       "   'modDate': \"D:20200404061510-07'00'\",\n",
       "   'trapped': '',\n",
       "   'encryption': None,\n",
       "   'file_path': './PDF_FOLDER/adaptive_pooling.pdf',\n",
       "   'page_count': 6,\n",
       "   'page': 6},\n",
       "  'toc_items': [],\n",
       "  'tables': [],\n",
       "  'images': [],\n",
       "  'graphics': [],\n",
       "  'text': \"[11] O. Banos, M. Damas, H. Pomares, I. Rojas, M. A. Toth, and\\nO. Amft, “A benchmark dataset to evaluate sensor displacement\\nin activity recognition,” in _Proceedings_ _of_ _the_ _2012_ _ACM_\\n_Conference on Ubiquitous Computing_ , ser. UbiComp ’12. New\\nYork, NY, USA: ACM, 2012, pp. 1026–1035. [Online]. Available:\\nhttp://doi.acm.org/10.1145/2370216.2370437\\n\\n[12] O. Banos, M. A. Toth, M. Damas, H. Pomares, and I. Rojas,\\n“Dealing with the effects of sensor displacement in wearable activity\\nrecognition,” _Sensors_ , vol. 14, no. 6, pp. 9995–10 023, 2014. [Online].\\nAvailable: http://www.mdpi.com/1424-8220/14/6/9995\\n\\n[13] M. G. Abdu-Aguye and W. Gomaa, “Competitive feature extraction for\\nactivity recognition based on wavelet transforms and adaptive pooling,”\\nin _2019 International Joint Conference on Neural Networks (IJCNN)_ ,\\nJuly 2019, pp. 1–8.\\n\\n\\n\\n[14] M. G. Abdu-Aguye and W. Gomaa, “Versatl: versatile transfer learning\\nfor imu-based activity recognition using convolutional neural networks,”\\nin _The 16th International Conference on Informatics in Control, Automation and Robotics (ICINCO)_ , 2019.\\n\\n[15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\\nB. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An\\nimperative style, high-performance deep learning library,” in\\n_Advances in Neural Information Processing Systems 32_ , H. Wallach,\\nH. Larochelle, A. Beygelzimer, F. d'Alch´ e-Buc, E. Fox, and\\nR. Garnett, Eds. Curran Associates, Inc., 2019, pp. 8024–\\n8035. [Online]. Available: http://papers.neurips.cc/paper/9015-pytorchan-imperative-style-high-performance-deep-learning-library.pdf\\n\\n\\n-----\\n\\n\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95cc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf # imports the pymupdf library\n",
    "import pandas as pd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "610c1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(url) # open a document\n",
    "num_pages=len(doc)\n",
    "\n",
    "def get_text_from_pdf(doc_path):\n",
    "    doc = pymupdf.open(doc_path) # open a document\n",
    "    num_pages=len(doc)\n",
    "    page_text_lst=[]\n",
    "    \n",
    "    for page in doc: # iterate the document pages\n",
    "        text = page.get_text()\n",
    "        page_text_lst.append(text)\n",
    "    return page_text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8b018633",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text_lst_final=get_text_from_pdf(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2d39484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fig. 2. System Architecture.\\nnetwork portions enforcing the requirement for ﬁxed-length\\ninputs.\\nThe summarization itself is performed over multiple scales.\\nThat is, the feature map is broken into a number of ﬁxed-\\nsize regions, over which a simple pooling operation (max\\nor average pooling) is carried out, thereby yielding a ﬁxed\\nnumber of features. This is typically done over multiple\\nvarying region sizes, ranging from global (i.e., over the whole\\nimage), to smaller sizes. At each size, the number of features\\nderived is different, but captures details at different levels of\\ngranularity about the input data. By concatenating the features\\nobtained at different region sizes (i.e., scales), a compact yet\\ninformative descriptor is obtained.\\nAlthough the preceding discussion is focused on 2-\\ndimensional feature maps, it can be seen that the same\\nprinciple can be applied to multidimensional inputs as well.\\nTherefore, adaptive pooling simply describes the extension of\\nspatial pyramid pooling to arbitrary dimensions, although we\\nfocus on the one-dimensional case in this work due to the\\nnature of the signals involved. In this case, each of the resulting\\n1-D feature maps is divided into regions, over which a pooling\\noperation (i.e., max or average pooling) is performed, similar\\nto the 2-D case. This is illustrated in Figure 1.\\nIn this work we use a simple notation to describe the\\nparameters/conﬁguration of the adaptive pooling operation,\\nwhich effectively determines the (number of) levels/region\\nsizes at which adaptive pooling is performed. For simplicity,\\nwe use hyphens to denote the sizes used i.e., 4-2-1 implies that\\nthe input is broken into 4 regions, each of which is pooled, then\\n2 regions, then a global pooling (i.e., 1 region) is performed.\\nIn general for a conﬁguration x-y-z, the number of features\\nTABLE I\\nHYPERPARAMETER CONFIGURATIONS CONSIDERED\\nHyperparameter Name\\nValues\\nConv1 Size\\n2,3,4,5,6,7,8,9,10,11,12,13,14,15\\nConv2 Size\\n2,3,4,5,6,7,8,9,10,11,12,13,14,15\\nConv2 Stride\\n1,2,3,4,5,6,7,8\\nMaxPool Size\\n2,3,4,5,6,7,8\\nMaxPool Stride\\n1,2,3,4,5,6,7,8\\nTotal Conﬁgurations:\\n87,808\\nobtained from the use of adaptive pooling is x + y + z.\\nIII. EXPERIMENTAL SETUP\\nA. Dataset\\nFor purposes of clarity, we perform our experiments on the\\nREALDISP dataset ( [11], [12]), which is a dataset consisting\\nof 33 different activities collected from 17 individuals in\\ntotal at a sample rate of 50Hz. REALDISP includes 4 sensor\\nmodalities (i.e., accelerometer, gyroscope, magnetometer and\\nrotation), but we consider only the ﬁrst two in this work.\\nWe preprocess the dataset into ﬁxed-length windows, each 1\\nsecond long and label each window according to the activity it\\ncorresponds to. This yields a total number of 13,963 samples,\\n75% of which are used for training the models with the\\nremainder used in testing the predictive performance of the\\nmodels. It is also pertinent to state that, during each of the\\nn training and testing steps mentioned previously, a different\\ntraining and testing split is considered in order to obtain a\\nbetter estimate of the model’s true performance. We select\\nn = 5 in this work.\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_text_lst_final[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e237ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_from_doc(doc_path):\n",
    "    doc = pymupdf.open(doc_path)\n",
    "    num_pages=len(doc)\n",
    "    print(num_pages)\n",
    "    \n",
    "    for page_index in range(num_pages): # iterate over pdf pages\n",
    "        page = doc[page_index] # get the page\n",
    "        image_list = page.get_images()\n",
    "        print(image_list)\n",
    "\n",
    "        # print the number of images found on the page\n",
    "        if image_list:\n",
    "            print(f\"Found {len(image_list)} images on page {page_index}\")\n",
    "        else:\n",
    "            print(\"No images found on page\", page_index)\n",
    "\n",
    "        for image_index, img in enumerate(image_list, start=1): # enumerate the image list\n",
    "            xref = img[0] # get the XREF of the image\n",
    "            pix = pymupdf.Pixmap(doc, xref) # create a Pixmap\n",
    "\n",
    "            if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
    "                pix = pymupdf.Pixmap(pymupdf.csRGB, pix)\n",
    "\n",
    "            pix.save(\"page_%s-image_%s.png\" % (page_index, image_index)) # save the image as png\n",
    "            pix = None\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "694f4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[]\n",
      "No images found on page 0\n",
      "[(2, 4, 773, 526, 8, 'DeviceRGB', '', 'Im1', 'FlateDecode')]\n",
      "Found 1 images on page 1\n",
      "[(6, 0, 1024, 560, 8, 'DeviceRGB', '', 'Im2', 'FlateDecode')]\n",
      "Found 1 images on page 2\n",
      "[(10, 12, 540, 480, 8, 'DeviceRGB', '', 'Im3', 'FlateDecode')]\n",
      "Found 1 images on page 3\n",
      "[(15, 0, 725, 720, 8, 'DeviceRGB', '', 'Im4', 'FlateDecode')]\n",
      "Found 1 images on page 4\n",
      "[]\n",
      "No images found on page 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_images_from_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59cd9166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 1, 'img_lst': [], 'img_cnt': 0}\n",
      "{'page': 2, 'img_lst': [(2, 4, 773, 526, 8, 'DeviceRGB', '', 'Im1', 'FlateDecode')], 'img_cnt': 1}\n",
      "{'page': 3, 'img_lst': [(6, 0, 1024, 560, 8, 'DeviceRGB', '', 'Im2', 'FlateDecode')], 'img_cnt': 1}\n",
      "{'page': 4, 'img_lst': [(10, 12, 540, 480, 8, 'DeviceRGB', '', 'Im3', 'FlateDecode')], 'img_cnt': 1}\n",
      "{'page': 5, 'img_lst': [(15, 0, 725, 720, 8, 'DeviceRGB', '', 'Im4', 'FlateDecode')], 'img_cnt': 1}\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 1, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 2,\n",
       "  'img_lst': [(2, 4, 773, 526, 8, 'DeviceRGB', '', 'Im1', 'FlateDecode')],\n",
       "  'img_cnt': 1},\n",
       " {'page': 3,\n",
       "  'img_lst': [(6, 0, 1024, 560, 8, 'DeviceRGB', '', 'Im2', 'FlateDecode')],\n",
       "  'img_cnt': 1},\n",
       " {'page': 4,\n",
       "  'img_lst': [(10, 12, 540, 480, 8, 'DeviceRGB', '', 'Im3', 'FlateDecode')],\n",
       "  'img_cnt': 1},\n",
       " {'page': 5,\n",
       "  'img_lst': [(15, 0, 725, 720, 8, 'DeviceRGB', '', 'Im4', 'FlateDecode')],\n",
       "  'img_cnt': 1},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Extract Images from individual pages.\n",
    "\n",
    "doc = pymupdf.open(url)\n",
    "num_pages=len(doc)\n",
    "#print(num_pages)\n",
    "\n",
    "\n",
    "\n",
    "for page_index in range(num_pages): # iterate over pdf pages\n",
    "    \n",
    "    page_number=page_index+1\n",
    "    page = doc[page_index] # get the page by index\n",
    "    \n",
    "    image_list = page.get_images()\n",
    "    page_image_dict={}\n",
    "    page_image_dict['page']=page_number\n",
    "    page_image_dict['img_lst']=image_list\n",
    "    page_image_dict['img_cnt']=len(image_list)\n",
    "    print(page_image_dict)\n",
    "    #print(doc_img_dict_lst)\n",
    "    doc_img_dict_lst1.append(page_image_dict)\n",
    "    \n",
    "    #print(doc_img_dict_lst)\n",
    "doc_img_dict_lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2789ae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0},\n",
       " {'page': 6, 'img_lst': [], 'img_cnt': 0}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_img_dict_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81250743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for showing images.\n",
    "\n",
    "Intended to be imported in Jupyter notebooks to display pixmap images.\n",
    "\n",
    "Invocation: \"show_image(item, title)\", where item is a PyMuPDF object\n",
    "which has a \"get_pixmap\" method, and title is an optional string.\n",
    "\n",
    "The function executes \"item.get_pixmap(dpi=150)\" and show the resulting\n",
    "image.\n",
    "\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "numpy, matplotlib, pymupdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_image(item, title=\"\"):\n",
    "    \"\"\"Display a pixmap.\n",
    "\n",
    "    Just to display Pixmap image of \"item\" - ignore the man behind the curtain.\n",
    "\n",
    "    Args:\n",
    "        item: any PyMuPDF object having a \"get_pixmap\" method.\n",
    "        title: a string to be used as image title\n",
    "\n",
    "    Generates an RGB Pixmap from item using a constant DPI and using matplotlib\n",
    "    to show it inline of the notebook.\n",
    "    \"\"\"\n",
    "    DPI = 150  # use this resolution\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # %matplotlib inline\n",
    "    pix = item.get_pixmap(dpi=DPI)\n",
    "    img = np.ndarray([pix.h, pix.w, 3], dtype=np.uint8, buffer=pix.samples_mv)\n",
    "    plt.figure(dpi=DPI)  # set the figure's DPI\n",
    "    plt.title(title)  # set title of image\n",
    "    _ = plt.imshow(img, extent=(0, pix.w * 72 / DPI, pix.h * 72 / DPI, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fb90129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_per_page(page_index):\n",
    "    page_image_dict={}\n",
    "    page_number=page_index+1\n",
    "    page = doc[page_index] # get the page by index\n",
    "    image_list = page.get_images()\n",
    "    img_cnt=len(image_list)\n",
    "    \n",
    "    page_image_dict['page']=page_number\n",
    "    page_image_dict['img_lst']=image_list\n",
    "    page_image_dict['img_cnt']=len(image_list)\n",
    "    \n",
    "    return page_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "450eba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_img_dict_lst=[]\n",
    "for page_index in range(num_pages): # iterate over pdf pages\n",
    "    \n",
    "    page_image_dict=extract_images_per_page(page_index)\n",
    "    #print(page_image_dict)\n",
    "    #doc_img_dict_lst.insert(page_index, page_image_dict)\n",
    "    doc_img_dict_lst.append(page_image_dict)\n",
    "    #doc_img_dict_lst.append(page_image_dict)\n",
    "    #print(page_image_dict)\n",
    "    \n",
    "#doc_img_dict_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad9c6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "doc_per_page_tabs_lst=[]\n",
    "def extract_text_tables(doc_path):\n",
    "    doc = fitz.open(doc_path)\n",
    "    num_pages=len(doc)\n",
    "    \n",
    "    \n",
    "    for i in range(num_pages):\n",
    "        tab_df_lst=extract_text_tables_per_page(i)\n",
    "        #print(\"tab_df_lst\")\n",
    "        #print(tab_df_lst)\n",
    "        \n",
    "        if len(tab_df_lst) == 0:\n",
    "            print(\"Do Nothing Here\")\n",
    "        else: \n",
    "            doc_per_page_tabs_lst.append(tab_df_lst)\n",
    "        \n",
    "    return doc_per_page_tabs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "463ca075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_tables_per_page(index):\n",
    "    page_dict={}\n",
    "    page_image_dict={}\n",
    "    tab_df_lst=[]\n",
    "    page = doc[index]\n",
    "    tabs = page.find_tables()  # detect the tables\n",
    "    \n",
    "    #Extract Text From each page\n",
    "    text = page.get_text()\n",
    "    page_dict['text']=text\n",
    "    \n",
    "    \n",
    "    ##Extract Images From Pages############\n",
    "    \n",
    "    page_image_dict=extract_images_per_page(page_index)\n",
    "    \n",
    "    print(\"page_image_dict\")\n",
    "    print(page_image_dict)\n",
    "    \n",
    "    \n",
    "    page_dict['page']=page_image_dict['page']\n",
    "    page_dict['img_lst']=page_image_dict['img_lst']\n",
    "    page_dict['img_cnt']=page_image_dict['img_cnt']\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "        for cell in tab.header.cells:\n",
    "            page.draw_rect(cell,color=fitz.pdfcolor[\"red\"],width=0.3)\n",
    "        page.draw_rect(tab.bbox,color=fitz.pdfcolor[\"green\"])\n",
    "        print(f\"Table {i} column names: {tab.header.names}, external: {tab.header.external}\")\n",
    "    \n",
    "    #show_image(page, f\"Table & Header BBoxes\")\n",
    "   \n",
    "    # choose the second table for conversion to a DataFrame\n",
    "    #tab = tabs[0]\n",
    "    #print(tabs)\n",
    "    \n",
    "    if tabs.tables == []:\n",
    "        print('Do Nothing')\n",
    "    else:\n",
    "        for tab in tabs:\n",
    "            df=pd.DataFrame()\n",
    "            df = tab.to_pandas()\n",
    "            tab_df_lst.append(df)\n",
    "    \n",
    "    page_dict['tables']=tab_df_lst\n",
    "    #print(tab_df_lst)\n",
    "    #df = tab.to_pandas()\n",
    "    # show the DataFrame\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b970a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Do Nothing\n",
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Do Nothing\n",
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Table 0 column names: ['Hyperparameter Name', 'Values'], external: False\n",
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Do Nothing\n",
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Do Nothing\n",
      "page_image_dict\n",
      "{'page': 6, 'img_lst': [], 'img_cnt': 0}\n",
      "Do Nothing\n"
     ]
    }
   ],
   "source": [
    "doc_dict=extract_text_tables(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ee6d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Adaptive Pooling Is All You Need: An Empirical\\nStudy on Hyperparameter-insensitive Human Action\\nRecognition Using Wearable Sensors\\nMubarak G. Abdu-Aguye\\nDepartment of Computer Engineering\\nAhmadu Bello University\\nZaria, Nigeria\\nmubarak.abduaguye@ejust.edu.eg\\nWalid Gomaa\\nComputer Science and Engineering\\nEgypt-Japan University of Science and Technology\\nNew Borg el-Arab, Egypt\\nwalid.gomaa@ejust.edu.eg\\nYasushi Makihara\\nISIR\\nOsaka University\\nOsaka, Japan\\nmakihara@am.sanken.osaka-u.ac.jp\\nYasushi Yagi\\nISIR\\nOsaka University\\nOsaka, Japan\\nyagi@am.sanken.osaka-u.ac.jp\\nAbstract—A plethora of techniques have been proposed in\\nhuman action recognition ﬁelds, and particularly deep learning-\\nbased methods such as convolutional neural networks (CNNs)\\nhave achieved impressive results. Usually, there is need to tune\\nhyper-parameters in the deep neural network (e.g., ﬁlter size,\\nstride) to achieve reasonable results. Such hyper-parameter\\ntuning is, however, extremely time and resource-intensive even\\nfor small models. In this paper, we posit that the inclusion of\\nan adaptive pooling in CNNs used for human action recognition\\nlargely eliminates the need for hyper-parameter tuning. Specif-\\nically, we demonstrated our idea for human action recognition\\nusing inertial sensor data (i.e., a temporal sequence) with a one-\\ndimensional adaptive pooling. We compared the adaptive pooling\\nto conventional CNNs with randomly chosen hyper-parameters\\nusing a publicly available data set for human action recognition.\\nExperimental results showed that the adaptive pooling achieved\\nbetter accuracy than the conventional CNNs.\\nIndex Terms—activity recognition, convolutional neural net-\\nworks, hyperparameters, adaptive pooling\\nI. INTRODUCTION\\nHuman action recognition is a challenging task. Traditional\\napproaches to solving the problem include the use of video\\ndata capturing the visual patterns of different actions. More re-\\ncently however, Inertial Measurement Unit (IMU) sensor data\\nhas been adopted for this same purpose with impressive results\\n[1], [2]. This also comes with the advantages of mobility\\nand improved user privacy. As such, we direct the proceeding\\ndiscussion to IMU sensor-based activity recognition.\\nRegardless of the modality adopted, Human action recogni-\\ntion is beset by different concerns including feature extraction\\nand classiﬁcation modalities. Traditionally, Feature extraction\\nWalid Gomaa is supported by the Information Technology Industry Devel-\\nopment Agency (ITIDA) under the ITAC Program Grant no. PRP2019.R26.1\\n- A Robust Wearable Activity Recognition System based on IMU Signals.’\\nfor IMU-based sensor data was handled using different sta-\\ntistical and temporal measures, including but not limited to\\nentropy, skewness, kurtosis, etc., with the selection of features\\nusually determined empirically. Although good performance\\nwas obtained, the problem of feature extraction and selection\\nremained pertinent.\\nWith the advent of deep neural networks, speciﬁcally con-\\nvolutional neural networks (CNNs), the need for manual\\nfeature extraction is obviated. This is due to the ability of\\nthe convolutional portions of such networks to automatically\\nlearn the optimal features which aid the downstream task\\n(i.e., classiﬁcation, regression, etc.). This has led to massive\\nperformance improvements in several domains, such as: WiFi\\nlocalization [3], Radio communications [4] and Video Trans-\\nlation [5].\\nThe prevalence of deep neural networks motivates the\\nquestion of how to obtain the best performance (e.g. predictive\\naccuracy, etc.) from their use. This could be seen to consist\\nof two distinct components. The ﬁrst of these is the problem\\nof ﬁnding the ”best” architecture for the given problem,\\nwhich is formally termed Neural Architecture Search (NAS)\\n[6]. The second component is concerned with ﬁnding the\\noptimal conﬁguration for a given network architecture. This\\nconﬁguration refers to the hyperparameters of the network,\\nwhich determine the structure of the network. For instance, the\\nnumber of recurrent units to use in Recurrent Neural Networks\\n(RNNs) or the number, size, and stride of convolutional\\nﬁlters in Convolutional Neural Networks (CNNs). However,\\nnetwork architectures and hyperparameters need to be tuned\\nfor individual problems, and cannot, in general, be inferred\\nautomatically in a theoretically-principled manner.\\nIn the case of NAS, unless particular constraints are placed\\non the network architecture, there are virtually inﬁnite pos-\\n',\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': []},\n",
       " {'text': 'sible architectures that may yield optimal performance on\\nthe given problem. Additionally, it is immediately clear that\\nit is computationally-expensive due to the need to train the\\ncandidate models from scratch each time. Additionally, NAS\\nis, to some extent, still subject to the hyperparameter search\\nproblem i.e. regardless of the architecture chosen, proper\\nhyperparameters must be selected to yield good performance\\non the problem. This also implies that there is no theoretical\\nguarantee that a simpler architecture with proper hyperparam-\\neters cannot match or outperform the optimal architecture as\\ndetermined by a NAS procedure.\\nWe believe that the preceding discussion sets the stage for\\nemphasizing the relative beneﬁt of focusing on hyperparameter\\nsearch for deep networks. Given the representational capacity\\nof deep networks in general, it is not unreasonable to instead\\nattempt to tune them for optimal performance rather than\\nfocus solely on their particular architecture. This is because\\nhyperparameter tuning scales with the complexity of the net-\\nwork, and therefore its difﬁculty can be somewhat contained\\nby simplifying the network architecture as much as possible.\\nAlthough the hyperparameter search problem is, like NAS,\\nbasically inﬁnite, it is much more tractable in practice because\\nit inherently requires discrete and not continuous solutions,\\nand the search space can be limited to regions which contain a\\n(small) range of values. Therefore, we believe that considering\\na fairly simple deep network and tuning that accordingly is a\\nmuch more feasible approach to obtaining good performance\\non a range of different tasks.\\nThere are existing techniques designed to mitigate the\\nhyperparameter search problem. From ﬁrst principles, Grid-\\nsearch [7] is one such approach that may be applied to\\nthe hyperparameter optimization problem, which considers a\\nsubregion of the (discrete) hyperparameter space, and visits\\nevery point in this region with a view to ﬁnding the optimal\\nhyperparameter conﬁguration therein. This is something of a\\nnaive approach, necessitating the use of less computationally\\nintensive approaches and methods. Random search [8] is\\nanother approach, which involves the visitation of random\\npoints within some region of the hyperparameter space. This\\napproach, being non-exhaustive, may not yield the optimal\\nresults but rather ‘good-enough’ results depending on the\\ncriteria set. More advanced techniques like Bayesian hyperpa-\\nrameter optimization [9] also exist but are not applicable to all\\nproblems, in that they rely on certain assumptions that cannot\\neasily be guaranteed to be universally valid. Regardless, the\\npreviously-described techniques still remain resource-intensive\\ndue to the need to train and evaluate multiple models, although\\ncountermeasures like parallelization may be adopted to miti-\\ngate this. In the presence of these drawbacks, two questions\\nevolve:\\n• Is it possible to derive a technique that yields optimal\\nmodel hyperparameters without the associated computa-\\ntional overhead?\\n• Is it possible to design models that are robust to hy-\\nperparameter settings, such that the exact selection of\\nhyperparameters does not affect the performance of the\\nmodel signiﬁcantly?\\nTherefore, the work done in this paper is aimed towards\\nproviding an answer to the second question. We propose\\nthe inclusion of adaptive pooling in convolutional neural\\nnetworks applied to activity recognition based on IMU-based\\nsensor data. Adaptive Pooling is a technique to perform multi-\\nscale summarization over convolutional feature maps, while\\ncapturing the essential behavior of the feature map itself. It has\\nthe effect of reducing the amount of data needed in tuning the\\ndownstream portion of CNNs, while reducing the convergence\\ntime and increasing generalization performance. To the best of\\nour knowledge, this is also the seminal work in which adaptive\\npooling is adopted for the realization of hyperparameter-\\nrobust deep convolutional networks. We design a suitable\\nexperimental methodology to validate our proposition, and\\ncarry out experiments on a publicly-available dataset to this\\nend. Empirical results obtained from our experiments indicate\\nthe efﬁcacy of the proposed method in realizing the stated\\ngoal, consistently providing near-optimal model performance\\nin the worst case.\\nThe rest of this paper is organized as follows. Section II\\nprovides some background on concepts relevant to the work\\ndone in this paper. Section III introduces our experimental\\nmethodology and its particulars. In Section IV, we present the\\nresults obtained and interpret them accordingly. We conclude\\nthe paper and describe points for future consideration in\\nSection V.\\nFig. 1. Spatial pyramid pooling for 1D data (4-2-1 pooling sizes)\\nII. BACKGROUND\\nA. Adaptive Pooling\\nAdaptive pooling is a generalization of another technique\\ncalled spatial pyramid pooling, which was ﬁrst introduced in\\n[10]. Spatial pyramid pooling was introduced to solve the\\nproblem of varying input sizes in CNNs for image-based tasks,\\nand therefore involves the conversion of convolutional feature\\nmaps of varying sizes into ﬁxed-length summarizations. These\\nﬁxed-length summarizations are compatible with the fully-\\nconnected portions of such networks which are actually the\\n',\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': []},\n",
       " {'text': 'Fig. 2. System Architecture.\\nnetwork portions enforcing the requirement for ﬁxed-length\\ninputs.\\nThe summarization itself is performed over multiple scales.\\nThat is, the feature map is broken into a number of ﬁxed-\\nsize regions, over which a simple pooling operation (max\\nor average pooling) is carried out, thereby yielding a ﬁxed\\nnumber of features. This is typically done over multiple\\nvarying region sizes, ranging from global (i.e., over the whole\\nimage), to smaller sizes. At each size, the number of features\\nderived is different, but captures details at different levels of\\ngranularity about the input data. By concatenating the features\\nobtained at different region sizes (i.e., scales), a compact yet\\ninformative descriptor is obtained.\\nAlthough the preceding discussion is focused on 2-\\ndimensional feature maps, it can be seen that the same\\nprinciple can be applied to multidimensional inputs as well.\\nTherefore, adaptive pooling simply describes the extension of\\nspatial pyramid pooling to arbitrary dimensions, although we\\nfocus on the one-dimensional case in this work due to the\\nnature of the signals involved. In this case, each of the resulting\\n1-D feature maps is divided into regions, over which a pooling\\noperation (i.e., max or average pooling) is performed, similar\\nto the 2-D case. This is illustrated in Figure 1.\\nIn this work we use a simple notation to describe the\\nparameters/conﬁguration of the adaptive pooling operation,\\nwhich effectively determines the (number of) levels/region\\nsizes at which adaptive pooling is performed. For simplicity,\\nwe use hyphens to denote the sizes used i.e., 4-2-1 implies that\\nthe input is broken into 4 regions, each of which is pooled, then\\n2 regions, then a global pooling (i.e., 1 region) is performed.\\nIn general for a conﬁguration x-y-z, the number of features\\nTABLE I\\nHYPERPARAMETER CONFIGURATIONS CONSIDERED\\nHyperparameter Name\\nValues\\nConv1 Size\\n2,3,4,5,6,7,8,9,10,11,12,13,14,15\\nConv2 Size\\n2,3,4,5,6,7,8,9,10,11,12,13,14,15\\nConv2 Stride\\n1,2,3,4,5,6,7,8\\nMaxPool Size\\n2,3,4,5,6,7,8\\nMaxPool Stride\\n1,2,3,4,5,6,7,8\\nTotal Conﬁgurations:\\n87,808\\nobtained from the use of adaptive pooling is x + y + z.\\nIII. EXPERIMENTAL SETUP\\nA. Dataset\\nFor purposes of clarity, we perform our experiments on the\\nREALDISP dataset ( [11], [12]), which is a dataset consisting\\nof 33 different activities collected from 17 individuals in\\ntotal at a sample rate of 50Hz. REALDISP includes 4 sensor\\nmodalities (i.e., accelerometer, gyroscope, magnetometer and\\nrotation), but we consider only the ﬁrst two in this work.\\nWe preprocess the dataset into ﬁxed-length windows, each 1\\nsecond long and label each window according to the activity it\\ncorresponds to. This yields a total number of 13,963 samples,\\n75% of which are used for training the models with the\\nremainder used in testing the predictive performance of the\\nmodels. It is also pertinent to state that, during each of the\\nn training and testing steps mentioned previously, a different\\ntraining and testing split is considered in order to obtain a\\nbetter estimate of the model’s true performance. We select\\nn = 5 in this work.\\n',\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': [                                 Hyperparameter Name  \\\n",
       "   0  Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...   \n",
       "   1                              Total Configurations:   \n",
       "   \n",
       "                                                 Values  \n",
       "   0  2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...  \n",
       "   1                                             87,808  ]},\n",
       " {'text': 'B. Protocol\\nThe core hypotheses governing this work are itemized as\\nfollows:\\n• Hypothesis 1: Given any arbitrary convolutional neural\\nnetwork architecture, including adaptive pooling in it\\ngives comparable or better results than the vanilla archi-\\ntecture i.e., without adaptive pooling.\\n• Hypothesis 2: Considering some subregion of the hy-\\nperparameter space, the variance in performance of the\\ncorresponding models which include adaptive pooling is\\nsmaller than the corresponding models without it, due\\nto the hyperparameter-agnosticism granted by adaptive\\npooling.\\n• Hypothesis 3: The performance of the models including\\nadaptive pooling are very close to the best performance\\nobtained by any of the considered models i.e., adaptive\\npooling situates the model performance at or very close\\nto the performance optimum achieved by the best model\\namongst the conﬁgurations evaluated.\\nTherefore, in line with the above hypotheses, we design our\\nexperimental protocol as follows:\\n• We\\ndesign\\na\\nbenchmark\\nconvolutional\\narchitecture\\n(shown in Figure 2), consisting of a number of convo-\\nlutional layers and a single max-pooling layer. Since we\\nare not considering neural architecture search, we ﬁx the\\nbase architecture and instead adjust the hyperparameters\\nof the network layers. We adopt this architecture as it has\\nbeen used successfully in solving the activity recognition\\nproblem with generally good results [13], [14], and is\\nsufﬁciently small to feasibly demonstrate the concept\\nunder discussion.\\n• We deﬁne a subregion of the hyperparameter space which\\ncontains reasonable hyperparameter conﬁgurations for the\\nproblem at hand. The hyperparameters and the values\\nconsidered for each are given in Table I.\\n• Given the large number of candidate conﬁgurations in\\nthis subregion (a total of 87,808), we randomly sample\\na number of conﬁgurations to reduce the computational\\nresources required for the experiments while giving an\\nunbiased estimate of the expected behavior of the pro-\\nposed method. Results from 2,743 models (3.12% of the\\ntotal) are obtained.\\n• For each candidate conﬁguration c, we:\\n– Initialize the benchmark architecture’s layers with\\nthe elements of the conﬁguration c, then obtain\\nthe mean value mc of the performance metric (i.e.,\\nprediction accuracy) over a number of runs n, each\\nconsists of a training and a testing step.\\n– Using the same conﬁguration, we replace the default\\npooling operator - i.e. traditional max-pooling - in\\nthe architecture with an adaptive pooling layer with a\\nﬁxed hyperparameter conﬁguration (i.e., 4-2-1, using\\nthe notation described previously), then obtain the\\nmean value m′\\nc of the performance metric using\\nthis modiﬁed version of the architecture, also over\\nn training and testing steps.\\n• We then compare mc and m′\\nc and aggregate the statistics\\nover all considered c candidates. Based on the derived\\nstatistics, we then validate each of the foregoing hypoth-\\nesis accordingly.\\nAll the experiments were implemented using the PyTorch\\n[15] neural network library, version 1.0.\\nWe present the results obtained and their interpretation in\\nthe following section.\\nFig. 3. Distribution of Model Performances With/Without Adaptive Pooling\\nIV. RESULTS AND DISCUSSION\\nIn order to streamline the proceeding discourse, we present\\nthe results obtained graphically. We derive a histogram over\\nthe model performances as shown in Figure 3, showing the\\nstatistical distribution thereof between the proposed and com-\\nparative (i.e., without adaptive pooling) models. Additionally,\\nwe include a scatter plot of the baseline model performances\\nagainst the proposed models’ performances, shown in Figure 4.\\nIt can be observed that, when the proposed method is used,\\nthe mean of the models’ performances is higher than the\\nmean of the baseline model (94.14% vs. 92.31%). This is in\\nline with the ﬁrst hypothesis put forth, and may be attributed\\nto the inherent beneﬁts of the adaptive pooling - its ability\\nto effectively summarize the input data, maintaining its key\\ncharacteristics while eliminating much of the details which\\nmay be uninformative (or even misleading to the classiﬁer).\\nThis directly improves generalization as it permits the classi-\\nﬁcation portion of the network focus on the core parts of the\\ninput which are beneﬁcial for the downstream classiﬁcation,\\nrather than factoring in a large number of features which may\\neventually degrade its performance based on their noisiness.\\nAdditionally, the summarized version of the input also reduces\\n',\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': []},\n",
       " {'text': 'Fig. 4.\\nScatter plot of model performances with/without adaptive pooling.\\nThe Red diagonal line indicates a trade-off between accuracies with/without\\nadaptive pooling, i.e., plots above the line means that accuracies with adaptive\\npooling are better than those without adaptive pooling.\\nthe number of weights utilized in the classiﬁcation portion of\\nthe network, allowing for faster convergence. Figure 4 also\\nshows the majority of the points distributed above the red\\nline, indicating that for a given conﬁguration, the performance\\nobtained from the proposed method is generally higher than\\nthat obtained using the baseline model.\\nAdditionally, it is also apparent that the spread of the\\nmodels’ performances when using the proposed method is\\nmuch smaller than that without adaptive pooling, which can\\nalso be seen from the spread of the points for the horizontal\\n(i.e., without average pooling) and the vertical (i.e., with\\naverage pooling) directions, in Figure 4. This indicates that\\nthe variance of the models which include the proposed method\\n(0.778%2) is much smaller than the models which do not\\ninclude it (6.09%2). This is in line with the expectations\\nput forth by the second hypothesis. This occurs because\\nunlike the baseline models (i.e., which include max-pooling),\\nthe inclusion of adaptive pooling lends signiﬁcant robustness\\nagainst the hyperparameters chosen, allowing the models to\\nmaintain their performance even in the presence of disparate\\nhyperparameter settings. This is by virtue of its ability to\\nsummarize the input data across multiple timescales and\\ntherefore preserve the salient information in a robust way.\\nGoing further, it should be noted that the small variance\\nin the performances obtained from the proposed models also\\nindicates that the models are reasonably comparable to the\\nvery best model obtained, although a slight performance\\npenalty may be observed. This validates the third hypothesis,\\nsince their actual performance values lie close (i.e., within 4-\\n5%) to the maximum possible performance attainable (from\\nusing this architecture).\\nFrom the preceding discussion, it can be seen that each\\nof the proposed hypotheses have been conﬁrmed, indicating\\nthe validity of the initial proposition. Although the results\\npresented have been aggregated over a fraction of the total\\nhyperparameter space, we ensure that random sampling in\\nsome sense considers the entirety of the desired subregion and\\ntherefore provides a statistically-valid view of the performance\\nof the proposed method.\\nV. CONCLUSION AND FUTURE WORK\\nIn this work, we have proposed an empirical adjustment for\\nconvolutional neural networks as applied to wearable sensor-\\nbased human activity recognition, which aims to eliminate\\nthe need for hyperparameter tuning. We design an experi-\\nmental methodology to empirically validate our hypothesis,\\nby considering the results obtained from a baseline CNN\\narchitecture using differently and randomly sampled hyperpa-\\nrameter conﬁgurations. Next, we compare them to the results\\nobtained from a version of the same architecture modiﬁed\\nbased on the proposed method. We obtained results indicating\\nthat the proposed modiﬁcation yields generally better and\\nmore stable/consistent results than the baseline architecture,\\nthereby validating the preceding approach and establishing its\\nsuitability for the domain in question.\\nIn the future we intend to perform much more comprehen-\\nsive evaluations of different models and also investigate the\\npotential of adaptive pooling in achieving the same or similar\\neffects for data types at different dimensionalities e.g. images.\\nREFERENCES\\n[1] B. Barshan and M. C. Y¨\\nuksek, “Recognizing daily and sports activities\\nin two open source machine learning environments using body-worn\\nsensor units,” The Computer Journal, vol. 57, no. 11, pp. 1649–1667,\\n2014.\\n[2] S. Ashry, R. Elbasiony, and W. Gomaa, “An lstm-based descriptor for\\nhuman activities recognition using imu sensors,” in Proceedings of the\\n15th International Conference on Informatics in Control, Automation\\nand Robotics, ICINCO, vol. 1, 2018, pp. 494–501.\\n[3] H. Rizk, M. Torki, and M. Youssef, “Cellindeep: Robust and accurate\\ncellular-based indoor localization via deep learning,” IEEE Sensors\\nJournal, vol. 19, no. 6, pp. 2305–2312, 2018.\\n[4] C.-K. Wen, W.-T. Shih, and S. Jin, “Deep learning for massive mimo\\ncsi feedback,” IEEE Wireless Communications Letters, vol. 7, no. 5, pp.\\n748–751, 2018.\\n[5] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,\\n“Evaluating surgical skills from kinematic data using convolutional neu-\\nral networks,” in International Conference on Medical Image Computing\\nand Computer-Assisted Intervention.\\nSpringer, 2018, pp. 214–221.\\n[6] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\\nsurvey.” Journal of Machine Learning Research, vol. 20, no. 55, pp.\\n1–21, 2019.\\n[7] D. C. Montgomery, Design and analysis of experiments.\\nJohn wiley\\n& sons, 2017.\\n[8] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-\\nmization,” Journal of Machine Learning Research, vol. 13, no. Feb, pp.\\n281–305, 2012.\\n[9] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-\\ntion of machine learning algorithms,” in Advances in neural information\\nprocessing systems, 2012, pp. 2951–2959.\\n[10] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” IEEE transactions on\\npattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–\\n1916, 2015.\\n',\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': []},\n",
       " {'text': \"[11] O. Banos, M. Damas, H. Pomares, I. Rojas, M. A. Toth, and\\nO. Amft, “A benchmark dataset to evaluate sensor displacement\\nin\\nactivity\\nrecognition,”\\nin\\nProceedings\\nof\\nthe\\n2012\\nACM\\nConference on Ubiquitous Computing, ser. UbiComp ’12.\\nNew\\nYork, NY, USA: ACM, 2012, pp. 1026–1035. [Online]. Available:\\nhttp://doi.acm.org/10.1145/2370216.2370437\\n[12] O. Banos, M. A. Toth, M. Damas, H. Pomares, and I. Rojas,\\n“Dealing with the effects of sensor displacement in wearable activity\\nrecognition,” Sensors, vol. 14, no. 6, pp. 9995–10 023, 2014. [Online].\\nAvailable: http://www.mdpi.com/1424-8220/14/6/9995\\n[13] M. G. Abdu-Aguye and W. Gomaa, “Competitive feature extraction for\\nactivity recognition based on wavelet transforms and adaptive pooling,”\\nin 2019 International Joint Conference on Neural Networks (IJCNN),\\nJuly 2019, pp. 1–8.\\n[14] M. G. Abdu-Aguye and W. Gomaa, “Versatl: versatile transfer learning\\nfor imu-based activity recognition using convolutional neural networks,”\\nin The 16th International Conference on Informatics in Control, Automa-\\ntion and Robotics (ICINCO), 2019.\\n[15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT.\\nKilleen,\\nZ.\\nLin,\\nN.\\nGimelshein,\\nL.\\nAntiga,\\nA.\\nDesmaison,\\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\\nB.\\nSteiner,\\nL.\\nFang,\\nJ.\\nBai,\\nand\\nS.\\nChintala,\\n“Pytorch:\\nAn\\nimperative\\nstyle,\\nhigh-performance\\ndeep\\nlearning\\nlibrary,”\\nin\\nAdvances in Neural Information Processing Systems 32, H. Wallach,\\nH.\\nLarochelle,\\nA.\\nBeygelzimer,\\nF.\\nd'Alch´\\ne-Buc,\\nE.\\nFox,\\nand\\nR.\\nGarnett,\\nEds.\\nCurran\\nAssociates,\\nInc.,\\n2019,\\npp.\\n8024–\\n8035. [Online]. Available: http://papers.neurips.cc/paper/9015-pytorch-\\nan-imperative-style-high-performance-deep-learning-library.pdf\\n\",\n",
       "  'page': 6,\n",
       "  'img_lst': [],\n",
       "  'img_cnt': 0,\n",
       "  'tables': []}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ea967967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_per_page_tabs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "81e3d94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                 Hyperparameter Name  \\\n",
       " 0  Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...   \n",
       " 1                              Total Configurations:   \n",
       " \n",
       "                                               Values  \n",
       " 0  2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...  \n",
       " 1                                             87,808  ]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in doc_per_page_tabs_lst:\n",
    "    df2=i\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8d8673a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.append(df2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "034236c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameter Name</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...</td>\n",
       "      <td>2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Configurations:</td>\n",
       "      <td>87,808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Hyperparameter Name  \\\n",
       "0  Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...   \n",
       "1                              Total Configurations:   \n",
       "\n",
       "                                              Values  \n",
       "0  2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...  \n",
       "1                                             87,808  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0539ed52",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'TableFinder' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m page \u001b[38;5;241m=\u001b[39m doc[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m tabs \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mfind_tables()  \u001b[38;5;66;03m# detect the tables\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtabs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,tab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tabs):  \u001b[38;5;66;03m# iterate over all tables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m tab\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mcells:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'TableFinder' has no len()"
     ]
    }
   ],
   "source": [
    "page = doc[1]\n",
    "tabs = page.find_tables()  # detect the tables\n",
    "\n",
    "\n",
    "for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "    for cell in tab.header.cells:\n",
    "        page.draw_rect(cell,color=fitz.pdfcolor[\"red\"],width=0.3)\n",
    "    page.draw_rect(tab.bbox,color=fitz.pdfcolor[\"green\"])\n",
    "    print(f\"Table {i} column names: {tab.header.names}, external: {tab.header.external}\")\n",
    "    \n",
    "show_image(page, f\"Table & Header BBoxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6506da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = doc[1]\n",
    "tabs = page.find_tables()  # detect the tables\n",
    "tabs.tables == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24d35e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameter Name</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...</td>\n",
       "      <td>2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Configurations:</td>\n",
       "      <td>87,808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Hyperparameter Name  \\\n",
       "0  Conv1 Size\\nConv2 Size\\nConv2 Stride\\nMaxPool ...   \n",
       "1                              Total Configurations:   \n",
       "\n",
       "                                              Values  \n",
       "0  2,3,4,5,6,7,8,9,10,11,12,13,14,15\\n2,3,4,5,6,7...  \n",
       "1                                             87,808  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the second table for conversion to a DataFrame\n",
    "tab = tabs[0]\n",
    "df = tab.to_pandas()\n",
    "# show the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331629d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
